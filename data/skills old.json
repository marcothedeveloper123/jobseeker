{"0":" [\n  { \"skill\": \"5+ years of network engineering, offensive\/defensive security\", \"reference\": \"HODL 5+ years of network engineering background including offensive\/defensive security\" },\n  { \"skill\": \"Public\/private\/hybrid cloud infrastructure experience\", \"reference\": \"Deep experience in public (AWS or Azure or Google), private and\/or hybrid cloud infrastructure\" },\n  { \"skill\": \"Network security management tools, familiarity with testing tools\", \"reference\": \"Experience with network security management tools and techniques, Familiarity with security testing tools (performance and threat-based)\" }\n]"}
{"0":" [\n  { \"skill\": \"Data Pipeline Development\", \"reference\": \"Data pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation.\" },\n  { \"skill\": \"Data Modeling\", \"reference\": \"Data modeling: Create and maintain data models ensuring data quality, scalability, and efficiency\" },\n  { \"skill\": \"Data Integration\", \"reference\": \"Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently.\" }\n]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"The Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools.\"},\n { \"skill\": \"Data Analytics Consumers\", \"reference\": \"The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business\/data analysts, data scientists or other stakeholders.\"},\n { \"skill\": \"Data Governance & Security\", \"reference\": \"Ensure compliance with data governance and data security requirements while creating, improving and operationalizing integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives.\"},\n { \"skill\": \"Collaboration\", \"reference\": \"The Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms toward optimization for data quality, security and governance to put them into production leading to potentially large productivity gains.\"},\n { \"skill\": \"Azure SQL Server & SSIS\", \"reference\": \"Complete analysis, design and development of BI solutions using Azure SQL Server; Experience coding ETL processes in Data Factory and SQL.\"},\n { \"skill\": \"Power BI & SSRS\", \"reference\": \"Generate ad hoc reports using Power BI or SSRS.\"},\n { \"skill\": \"Data Warehouse Concepts\", \"reference\": \"Familiarity with Data Warehouse concepts.\"},\n { \"skill\": \"Database Development\", \"reference\": \"Experience in database development, report writing and\/or statistics preferred.\"},\n { \"skill\": \"ETL Processes\", \"reference\": \"The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business\/data analysts, data scientists or other stakeholders.\"},\n { \"skill\": \"Data Science Initiatives\", \"reference\": \"Assist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives.\"},\n { \"skill\": \"Performance Tuning\", \"reference\": \"Troubleshoot Azure tools, systems, and software; Review queries for performance issues, making changes as needed; Collaborate with team to performance-tune Azure application as necessary.\"}]"}
{"0":" [\n    { \"skill\": \"Data Engineering\", \"reference\": \"The Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert.\" },\n    { \"skill\": \"Data Pipeline Architecture\", \"reference\": \"You will be responsible for building a data platform to facilitate the data science team. Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional \/ non-functional business requirements.\" },\n    { \"skill\": \"Cross Functional Team Collaboration\", \"reference\": \"Experience supporting and working with cross-functional teams in a dynamic environment. Ability to mentor\/guide\/collaborate with other team members.\" }\n]"}
{"0":" [{ \"skill\": \"Bachelors\/Masters degree, highly motivated, self-learner, technically inquisitive\", \"reference\": \"Required skills for Java\/Software Programmers and Data Science\/Machine Learning\" },\n{ \"skill\": \"Programming language Java, understanding of software development life cycle, knowledge of Core Java, JavaScript, C++, or software programming experience\", \"reference\": \"Required skills for Java\/Software Programmers\" },\n{ \"skill\": \"Spring boot, Microservices and REST API's experience\", \"reference\": \"Required skills for Java\/Software Programmers\" },\n{ \"skill\": \"Statistics, Python, data visualization tools, NLP, Text mining, Tableau, Time series analysis\", \"reference\": \"Preferred skills: Data Science\/Machine Learning\" }]"}
{"0":" [\n    { \"skill\": \"ETL pipelines, scalable platforms, robust data management\", \"reference\": \"Design, build, and maintain robust ETL pipelines.\" },\n    { \"skill\": \"Data platform excellence, collaboration, industry knowledge\", \"reference\": \"Contribute to making our data platform world-class.\" },\n    { \"skill\": \"Code reviews, quality control, software development expertise\", \"reference\": \"Actively participate in code and design reviews.\" }\n]"}
{"0":" [{ \"skill\": \"Expert Scala with ZIO\", \"reference\": \"About you: Expert using Scala within ZIO\" }, { \"skill\": \"12-month contract to hire\", \"reference\": \"Lead Scala Developers - ZIO 12 month Contract to Hire\" }]"}
{"0":" [{ \"skill\": \"Staff Front-end Software Engineer\", \"reference\": \"We are seeking an experienced Staff Front-end Software Engineer with React, TypeScript, and related technologies expertise to play a pivotal role in the future growth of our company.\"},\n { \"skill\": \"Agile Teamwork\", \"reference\": \"We believe that accomplishing our ambitious goals cannot be done by lone heroes. Lasting change requires work by synergistic teams, and we look for Engineers who are highly skilled at collaboration.\"},\n { \"skill\": \"Front-end Guidance\", \"reference\": \"You will be responsible for providing front-end guidance for an agile team of mission-driven and talented front-end and back-end engineers.\"}]"}
{"0":" [{ \"skill\": [\"R, Python for data analysis and modeling\", \"Statistical programs like R, SAS, MATLAB, or Python\"], \"reference\": \"Skills and Abilities: Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python.\"},\n{ \"skill\": [\"Proficiency in spreadsheets (VBA), database applications\", \"Familiarity with SQL, Javascript, XML, JSON, HTML\"], \"reference\": \"Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). Basic understanding of SQL, Javascript, XML, JSON, and HTML.\"},\n{ \"skill\": [\"Quick learning abilities\", \"Working under deadlines\", \"Teamwork and communication skills\", \"Strong analytical problem-solving\"], \"reference\": \"Ability to learn new methods quickly and work under deadlines. Excellent teamwork and communication skills. Strong analytical and problem-solving abilities.\"}]"}
{"0":" [\n    { \"skill\": \"Data modeling\", \"reference\": \"Experience working with large-scale data model refactoring for better performance, interpretability, and maintainability\" },\n    { \"skill\": \"ETL expertise\", \"reference\": \"Experience designing and implementing data models and analytics reports in a cloud environment\" },\n    { \"skill\": \"Data engineering best practices\", \"reference\": \"Proven track record of implementing data engineering best practices in all aspects of the data pipeline, i.e. ETL, data integrity, and monitoring\" },\n    { \"skill\": \"Python, SQL, DBT proficiency\", \"reference\": \"Demonstrable proficiency in Python, SQL, and DBT. Experience with Looker is a plus\" },\n    { \"skill\": \"Version control tools\", \"reference\": \"Experience with version control tools (GitHub, GitLab) and Agile methodologies.\" },\n    { \"skill\": \"Bachelor's degree in quantitative field or equivalent experience\", \"reference\": \"Bachelor's Degree in a quantitative field such as Physics, Engineering, Computer Science, or demonstrated equivalent quantitative experience\" },\n    { \"skill\": \"Collaboration\", \"reference\": \"Excellent communication skills to effectively collaborate with different teams within the data org\" }\n]"}
{"0":" [{ \"skill\": [\"LinkedIn profile\", \"6+ months CTH\"], \"reference\": \"About the job\"},\n { \"skill\": [\"3-6 years of experience\", \"Python, SQL, DBT\"], \"reference\": \"Required Skills\"},\n { \"skill\": [\"Start up environment experience\"], \"reference\": \"Experience\"} ]"}
{"0":" [\n    { \"skill\": [\"Bachelor's degree or Master's degree in Computer Science, Engineering, Mathematics\/Statistics\", \"Highly motivated, self-learner, and technically inquisitive\"], \"reference\": \"Required Skills For Java \/Full Stack\/Software Programmer Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\" },\n    { \"skill\": [\"Bachelor's degree or Master's degree in Computer Science, Engineering, Mathematics\/Statistics\", \"Highly motivated, self-learner, and technically inquisitive\"], \"reference\": \"Required Skills For Data Science\/Machine Learning Positions Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\" },\n    { \"skill\": [\"Bachelor's degree or Master's degree in any field (SynergisticIT offers skill enhancement programs)\", \"Highly motivated, self-learner, and technically inquisitive\"], \"reference\": \"Who Should Apply Recent Computer science\/Engineering \/Mathematics\/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry\" },\n    { \"skill\": [\"Programming language Java, understanding of the software development life cycle\", \"Project work on skills needed\"], \"reference\": \"Required Skills For Java \/Full Stack\/Software Programmer Experience in programming language Java and understanding of the software development life cycle Project work on the skills\" },\n    { \"skill\": [\"Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools\", \"Experience in Machine Learning\"], \"reference\": \"Required Skills For Data Science\/Machine Learning Positions Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools Experience in Machine Learning\" },\n    { \"skill\": [\"NLP, Text mining, Tableau, PowerBI, TensorFlow\"], \"reference\": \"Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorFlow\" }\n]"}
{"0":" [{ \"skill\": [\"Data Engineering\", \"Database Management\"], \"reference\": \"About The RoleAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data.\"},\n { \"skill\": [\"ETL Technologies\", \"Analytics\"], \"reference\": \"Experience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. Experience working with ETL technologies , such as Databricks, Fivetran , or dbt Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake\"},\n { \"skill\": [\"Data Pipelines\", \"Monitoring\"], \"reference\": \"Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems Ingesting financial datasets from external customers, then updating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data Monitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct\"},\n { \"skill\": [\"API Integration\", \"3rd Party APIs\"], \"reference\": \"Developing APIs and integrating with 3rd party APIs Bonus: experience in fintech \/ SaaS \/ credit analysis\"},\n { \"skill\": [\"CI\/CD Automation\", \"Communication\"], \"reference\": \"Some experience with CI\/CD automation such as GitHub Actions Passionate about creating great digital experiences for users Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same\"},\n { \"skill\": [\"Problem Solving\", \"Status Quo\"], \"reference\": \"Passionate problem solvers who are not satisfied with the status quo\"},\n { \"skill\": [\"Flexibility\", \"Remote Work\"], \"reference\": \"Self-starters, who take the initiative to tackle challenges in a remote work environment\"}]"}
{"0":" [\n  { \"skill\": \"Data Engineer\", \"reference\": \"Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\" },\n  { \"skill\": \"Database Management Systems\", \"reference\": \"Designs, develops, builds, analyzes, evaluates, and installs database management systems\" },\n  { \"skill\": \"Data Architecture\", \"reference\": \"Defines and oversees database organizations, standards, controls, procedures, and documentation.\" },\n  { \"skill\": \"Database Consulting\", \"reference\": \"Provides technical consulting in the definition, design, and creation of a database environment\" },\n  { \"skill\": \"Database Implementation\", \"reference\": \"Designs and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\" },\n  { \"skill\": \"Data Storage\", \"reference\": \"Determines data storage and optimum storage requirements.\" },\n  { \"skill\": \"Data Mining & Analysis\", \"reference\": \"Uses data mapping, data mining, and data transformational analysis tools to design and develop databases.\" },\n  { \"skill\": \"Applications Development Experience\", \"reference\": \"Applications development experience with data-based solutions to business problems\" },\n  { \"skill\": \"Excellent Communication Skills\", \"reference\": \"Excellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\" }\n]"}
{"0":" [\n  { \"skill\": \"Data Backend Engineering\", \"reference\": \"As a Data Backend Engineer, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp.\" },\n  { \"skill\": \"Efficient Data Modeling\", \"reference\": \"Build systems that can effectively store and crunch terabytes of data. Design and develop data models for efficient data storage, retrieval, and reporting.\" },\n  { \"skill\": \"Cross-Functional Collaboration\", \"reference\": \"Collaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists to understand data requirements and translate them into effective data models.\" }\n]"}
{"0":" [\n  { \"skill\": [\"Quantexa Data engineer\", \"Remote\", \"Long Term Contract\"], \"reference\": \"Role: Quantexa Data Engineer\" },\n  { \"skill\": [\"End to End usage of Quantexa\", \"Guiding clients on usage\", \"Data Ingestion\"], \"reference\": \"Job Description: 1.\" },\n  { \"skill\": [\"Entity issues resolution\", \"Data\", \"Issues with data\"], \"reference\": \"2. Resolve Entity issues with data\" },\n  { \"skill\": [\"Knowledge of Scala and Hive\", \"Coding experience using parquet\"], \"reference\": \"3.\" },\n  { \"skill\": [\"Advanced SQL knowledge\", \"Writing SQL queries\"], \"reference\": \"4.\" },\n  { \"skill\": [\"Experience with GCP data lake\"], \"reference\": \"5. Experience having worked with GCP data lake\" },\n  { \"skill\": [\"Quantexa certification required\"], \"reference\": \"Reason they are asking for Certification 2.\" }\n]"}
{"0":" [{ \"skill\": [\"Data Engineering\", \"SQL Databases\", \"ETL Processes\", \"Distributed Computing\"], \"reference\": \"For over 17 years, Catalist has been a leader in civic data and data science innovation. As a Data Engineer at Catalist...\"},\n        { \"skill\": [\"Problem Solving\", \"Creativity\", \"Teamwork\", \"Communication\"], \"reference\": \"The ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving.\"},\n        { \"skill\": [\"Project Management\", \"Cross-Departmental Collaboration\", \"Stakeholder Engagement\"], \"reference\": \"This position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist...\"}]"}
{"0":" [{ \"skill\": \"Data solutions\", \"reference\": \"Can UDig It? UDig designs, builds, and implements technology solutions that deliver on business objectives.\" },\n{ \"skill\": \"Agile project delivery\", \"reference\": \"Part of the Breaking Ground training program is covering UDig's delivery approach within a project team, focusing on agile methodologies.\" },\n{ \"skill\": \"Data pipeline development\", \"reference\": \"A typical day might entail: Design and development of data pipelines, BI reports, and more.\" }]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"Follow us on Linkedin: https:\/\/www.linkedin.com\/company\/pulivarthigroup\/Pulivarthi Group LLC is a Global Staffing & IT Technology Solutions company\" },\n { \"skill\": \"Azure CI\/CD\", \"reference\": \"Our customer is requesting a resource who knows Azure CI\/CD (Continuous Integration \/ Continuous Deployment) to build and integrate their DevOps pipeline.\" },\n { \"skill\": \"Data Lake Connection\", \"reference\": \"Once the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\" },\n { \"skill\": \"Central Time Zone Collaboration\", \"reference\": \"The Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration.\" }]"}
{"0":" [{ \"skill\": [\"Designing UI changes\", \"Reviewing requirements\", \"React concepts\"], \"reference\": \"Responsibilities Designing user interface changes for web-based DB applications. Reviewing application requirements and interface designs. Developing and implementing highly responsive user interface components using react concepts.\"},\n { \"skill\": [\"JavaScript\", \"React.js\", \"Front-end architecture\", \"Performance monitoring\"], \"reference\": \"Troubleshooting interface software and debugging application codes. Developing and implementing front-end architecture to support user interface concepts. Monitoring and improving front-end performance.\"},\n { \"skill\": [\"Bachelor's degree\", \"React.js experience\", \"Front-end languages\"], \"reference\": \"Qualifications And Experience Bachelor\u2019s degree in computer science, information technology, or a similar field. Previous experience working as a react.js developer. In-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages.\"}]"}
{"0":" [\n    { \"skill\": \"Data Engineering \/ Analytics\", \"reference\": \"As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline\" },\n    { \"skill\": \"SQL & Relational Databases\", \"reference\": \"Advance working knowledge and experience in SQL and relational databases\" },\n    { \"skill\": \"Data Management Abilities\", \"reference\": \"Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data\" }\n]"}
{"0":" [{ \"skill\": [\"e-commerce (retail) experience\", \"Python\"], \"reference\": \"MUST HAVE: e-commerce(retail) experience 5-7 years experience with Python\" },\n { \"skill\": [\"Google BigQuery and SQL\"], \"reference\": \"Experience working with Google BigQuery and SQL\" }]"}
{"0":" [{ \"skill\": [\"tech savvy\", \"Data Engineer\"], \"reference\": \"We are looking for a tech savvy Data Engineer to contribute\" },\n { \"skill\": [\"data pipeline builder\", \"data wrangler\"], \"reference\": \"The ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems\" },\n { \"skill\": [\"optimize data systems\", \"build from scratch\"], \"reference\": \"Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems.\" },\n { \"skill\": [\"process improvement\", \"automation\"], \"reference\": \"The right-fit person will be excited by the prospect of optimizing or even re-designing data processes\" },\n { \"skill\": [\"SQL\", \"Data Management\", \"cloud based 'big data' technologies\"], \"reference\": \"Build the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\" },\n { \"skill\": [\"analytics tools\", \"provide actionable insights\"], \"reference\": \"Build analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\" },\n { \"skill\": [\"collaborate with client teams\", \"local requirements\", \"standard global business model\"], \"reference\": \"Collaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\" },\n { \"skill\": [\"meetings\", \"discussions\", \"strategy sessions\"], \"reference\": \"Participates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved.\" },\n { \"skill\": [\"progressive experiences\", \"Bachelor\u2019s degree or higher\"], \"reference\": \"The exceptional team member will have or demonstrate progressive experiences in...\" },\n { \"skill\": [\"BI and data warehousing tools\", \"enterprise data security & governance\"], \"reference\": \"Experience working with BI and data warehousing tools, Experience working with enterprise data, where security is paramount and data governance is critical.\" },\n { \"skill\": [\"independent work\", \"team environment\"], \"reference\": \"Ability to work independently and in a team environment, and effectively engage all levels of the organization.\" },\n { \"skill\": [\"data manipulation\", \"extract value from large datasets\"], \"reference\": \"Understanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\" },\n { \"skill\": [\"effective communication\", \"listening, presenting, questioning\"], \"reference\": \"Ability to communicate effectively (listening, presenting, and questioning) Strong organizational, written, and communication skills.\" }]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"We're hiring a Data Engineer. About Our Team And What We'll Build Together\" }, { \"skill\": \"Data Warehouse and Infrastructure Management\", \"reference\": \"Our business is run on detailed analysis of how our products perform with our members, which features they love and which can be improved, and which product and marketing campaigns are bringing in the best quality users. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature\" }, { \"skill\": \"Collaboration\", \"reference\": \"You're ready for this! Here's a bit more about what we're looking for 3+ years of industry experience measuring product performance and user behavior, Experience working with a variety of data management technologies, including RedShift, Kinesis\/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others. Experience implementing BI reporting tools such as Looker, Experience interfacing with engineers, product managers and analysts to understand data needs\" }, { \"skill\": \"Leadership\", \"reference\": \"The expected annual base salary for this role is $125,000-$135,000. We'll consider a variety of factors when determining the offered base salary including an evaluation of a candidate's skills, abilities, experience, location, market demands, and internal parity\" }, { \"skill\": \"Problem-Solving\", \"reference\": \"Ability to thrive in a dynamic, fast-paced, collaborative, and high-growth environment, Facility in presenting and discussing the trade-offs in employing different engineering solutions to a problem, valuing pragmatism over idealism, An empathetic leadership style that encourages open communication and trust\" }]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"Implement and deploy data analysis pipelines at various clients of Lynx Analytics.\" },\n { \"skill\": \"Programming Skills\", \"reference\": \"Strong programming skills\" },\n { \"skill\": \"Big Data, GCP, Airflow & Spark Experience\", \"reference\": \"Experience in Big Data, A minimum of 3 years of experience in Data Science or Analytics, Industry experience in working for a big enterprise (like our clients), Experience with GCP, Airflow and Spark\" }]"}
{"0":" [{ \"skill\": \"Bachelor's or Master's degree in related fields\", \"reference\": \"Entry-level software programmers, Java Full stack developers, Python\/Java developers, Data analysts\/Data Scientists, Machine Learning engineers for full time positions with clients.\"},\n { \"skill\": \"Highly motivated self-learners and technically inquisitive\", \"reference\": \"Highly motivated, self-learner, and technically inquisitive\" },\n { \"skill\": \"Proficiency in programming languages (e.g., Java, JavaScript, C++)\", \"reference\": \"Experience in programming language Java and understanding of the software development life cycle\" }]"}
{"0":" [\n  { \"skill\": \"General Purpose Python Programming\", \"reference\": \"Preference will be shown to candidates who can provide a link to their open-source code portfolio (a link to your profile on github.com, bitbucket.com, gitlab.com, or another public VCS is sufficient)\" },\n  { \"skill\": \"Database Design and SQL\", \"reference\": \"You are proficient in authoring readable, well-structured, SQL SELECT statements using ISO\/ANSI-standard SQL\" },\n  { \"skill\": \"Version Control and CI\/CD\", \"reference\": \"You have experience with trunk-based development (feature branching) using git for version control, with fully automated deployments (CI\/CD)\" }\n]"}
{"0":" [{ \"skill\": [\"Microsoft\", \"Azure Fabric\"], \"reference\": \"The client is placing heavy emphasis on Microsoft and Azure Fabric experience.\" },\n{ \"skill\": [\"Information Management Engineer\"], \"reference\": \"\"Information Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\" },\n{ \"skill\": [\"Data Migration\", \"Client Data Lake\"], \"reference\": \"Take the identified data sources and bring them into our Data Lake, Azure Fabric.\" },\n{ \"skill\": [\"Data Transformation\", \"Cleaning\"], \"reference\": \"Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data).\" },\n{ \"skill\": [\"Medallion Data Model\", \"Data Architecture\"], \"reference\": \"We are focused on using a medallion data model for data architecture.\" },\n{ \"skill\": [\"Delta Parquet Files\", \"Python\", \"Spark\", \"Notebooks\", \"T-SQL\"], \"reference\": \"\"The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL,\" },\n{ \"skill\": [\"Fabric Lakehouse\/Warehouse\", \"Dataflows Gen2\", \"Data Pipelines\", \"Direct Lake\"], \"reference\": \"We are using Fabric lakehouse\/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake.\" },\n{ \"skill\": [\"Shortcuts\", \"ADF\"], \"reference\": \"shortcuts, ADF.\" }]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"As a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights\" },\n { \"skill\": \"Data Analysis and Predictive Modeling\", \"reference\": \"Conduct exploratory data analysis to identify patterns, trends, and anomalies. Perform statistical analysis and hypothesis testing to derive meaningful insights; Develop predictive models to forecast manufacturing performance, quality, and operational outcomes\" },\n { \"skill\": \"Data Visualization and Communication\", \"reference\": \"Create informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries; Generate regular reports to communicate performance trends to stakeholders; Communicate complex findings and insights effectively to both technical and non-technical audiences\" },\n { \"skill\": \"Cross-functional Collaboration\", \"reference\": \"Collaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support\" },\n { \"skill\": \"Continuous Improvement\", \"reference\": \"Suggest process improvements and optimizations based on data analysis\" },\n { \"skill\": \"Data Security and Compliance\", \"reference\": \"Ensure data security and compliance with relevant regulations, such as GDPR or industry-specific standards\" },\n { \"skill\": \"Infrastructure and Development\", \"reference\": \"Build and maintain the infrastructure for collecting, storing, and processing data; Design, develop, and manage data pipelines, ETL processes, and data warehouses\" }]"}
{"0":" [{ \"skill\": [\"Data Engineering\", \"Visualization\", \"Maintaining Technical Foundation\"], \"reference\": \"About the job, We are looking for a talented individual to join our team as a Data Engineer. You'll play a crucial part in supporting our Data and Process Governance Policies and Standards while optimizing workflows and data processing.\"},\n         { \"skill\": [\"Data Governance\", \"Process Optimization\"], \"reference\": \"Responsibilities include collaborating with cross-functional teams to implement and support Data and Process Governance Policies and Standards, maintaining the technical foundation of data visualizations, and optimizing workflows through database theory knowledge.\"},\n         { \"skill\": [\"Data Visualization\", \"Maintaining Data Integrity\"], \"reference\": \"Develop and implement data visualizations and reporting solutions based on user needs, collaborate with engineers and developers to create insightful reports for business, and partner with Supply Chain to define, document, implement, and maintain data workflows.\"},\n         { \"skill\": [\"Project Management\", \"Cross-Functional Collaboration\"], \"reference\": \"Proven experience managing complex projects and collaborating with cross-functional teams is required as a part of this role.\"},\n         { \"skill\": [\"Data Access Management\", \"Change Management\"], \"reference\": \"Strong knowledge of data governance concepts, expertise in data access management, change management, and data historian\/time series data are crucial for this position.\"},\n         { \"skill\": [\"SQL\", \"MSSQL\"], \"reference\": \"Proficiency in SQL, MSSQL, TSQL, and data modeling for maintaining data integrity between multiple schemas is essential.\"},\n         { \"skill\": [\"ETL Processes\", \"Denodo\/Data Virtualization\"], \"reference\": \"Experience with ETL processes, with knowledge of Denodo\/data virtualization being preferred for this role.\"},\n         { \"skill\": [\"Python\", \"Query Optimization\"], \"reference\": \"Proficiency in Python and query optimization is required for the successful completion of tasks within this role.\"},\n         { \"skill\": [\"Big Data Sets\", \"Tableau Server\/Desktop\/Prep Setup\"], \"reference\": \"Hands-on experience working with big data sets, familiarity with Tableau Server\/Desktop\/Prep setup and dashboard building are also necessary skills for this position.\"},\n         { \"skill\": [\"Data Cleaning and Wrangling\", \"Self-Motivation\"], \"reference\": \"Strong data cleaning and wrangling skills, initiative-driven, self-motivated, results-oriented, and able to work independently are also important characteristics for this role.\"},\n         { \"skill\": [\"Effective Communication\"], \"reference\": \"Effective communication skills, both written and verbal, as this is a client-facing role are required for the Data Engineer position.\"}]"}
{"0":" [{ \"skill\": [\"Data Engineering\", \"Python\", \"SQL\"], \"reference\": \"Ideal candidate for the Data Engineer role will have strong technical skills including Python, SQL, and AWS.\"},\n { \"skill\": [\"Data Pipeline\", \"Collaboration\"], \"reference\": \"Main Responsibilities: Build the data pipelines that power our business, Collaborate across disciplines to high-quality datasets.\"},\n { \"skill\": [\"Big Data\", \"Tools Learning\"], \"reference\": \"Ability to think in principles and frameworks, an aptitude to learn new technologies and tools quickly.\"}]"}
{"0":" [{ \"skill\": \"Data Backend Engineering\", \"reference\": \"As a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp.\" }, { \"skill\": \"Data Modeling and Data Warehousing\", \"reference\": \"As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A\/B testing and company investments.\" }, { \"skill\": \"High Performing Data Systems\", \"reference\": \"Understanding of high performing and scalable data systems.\" }, { \"skill\": \"Experience in ETL Pipelines\", \"reference\": \"Experience in building and orchestrating ETL pipelines.\" }, { \"skill\": \"Data Lake\/Warehouse Landscape\", \"reference\": \"Experience with Data Lake or Data Warehouse landscape.\" }]"}
{"0":" [{ \"skill\": \"Machine Learning, AI\/ML Expertise\", \"reference\": \"Building, deploying, measuring, and maintaining machine learning models\" },\n { \"skill\": \"Software Development, DevOps\", \"reference\": \"Thorough understanding of software development lifecycle, DevOps\" },\n { \"skill\": \"Programming Skills, Communication\", \"reference\": \"Strong programming skills in Python, Scala, Go, Rust or other languages. Excellent written and verbal communication\" }]"}
{"0":" [\n  { \"skill\": \"Java\/Full Stack\/Software Programmer\", \"reference\": \"Required Skills For Java \/Full Stack\/Software Programmer\" },\n  { \"skill\": \"Data Analyst\/Data Scientist\", \"reference\": \"Required Skills For Data Science\/Machine learning Positions\" },\n  { \"skill\": \"Machine Learning Engineer\", \"reference\": \"Required Skills For Data Science\/Machine learning Positions\" }\n]"}
{"0":" [{ \"skill\": \"Data Engineering Lead\", \"reference\": \"Design and implement data architecture and strategy\" },\n{ \"skill\": \"Databricks, Unity Catalog, Privacera, Collibra\", \"reference\": \"Need: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\" },\n{ \"skill\": \"Technical deliverables planning\", \"reference\": \"Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort\" },\n{ \"skill\": \"Vendor and internal team collaboration\", \"reference\": \"Collaborate with vendor and internal teams to address technical issues.\" },\n{ \"skill\": \"Scheduling and prioritization\", \"reference\": \"Drive planning and prioritization in conjunction with the SCRUM master\" },\n{ \"skill\": \"Team coordination\", \"reference\": \"Coordinate with onsite and offshore teams for timely project delivery.\" },\n{ \"skill\": \"Feature prioritization\", \"reference\": \"Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\" },\n{ \"skill\": \"Issue management\", \"reference\": \"Collect and manage user-reported issues, ensuring prompt resolution and communication.\" },\n{ \"skill\": \"Test planning\", \"reference\": \"Define comprehensive test plans, oversee their execution, and document results.\" }]"}
{"0":" [{ \"skill\": [\"Python\", \"Bash\", \"Django\"], \"reference\": \"Full stack data engineer, working with any tech stack and data. Python Must. Django, bash, Pandas.\" },\n { \"skill\": [\"MongoDB\", \"Teradata\"], \"reference\": \"Experience with MongoDB, Teradata, coding capabilities is highly preferred\" }]"}
{"0":" [\n  { \"skill\": [\"4+ years' experience developing data centric applications\", \"Role: Data Engineer\"], \"reference\": \"About the job\" },\n  { \"skill\": [\"Scala and Databricks\", \"Experience in developing data centric apps\"], \"reference\": \"Requirements\" },\n  { \"skill\": [\"Data Formats (JSON, XML, Parquet)\", \"Data Modeling\"], \"reference\": \"Advanced knowledge of data formats (JSON, XML, Parquet, etc.) and data modeling practices.\" },\n  { \"skill\": [\"Data Lake file systems\", \"Large volumes of data\"], \"reference\": \"Advanced understanding of best practices for structuring and organizing Data Lake file systems for large volumes of data.\" }\n]"}
{"0":" [{ \"skill\": [\"Prototype\", \"Build AI products\", \"Cutting edge\"], \"reference\": \"Prototype and build cutting edge, proof-of-concept AI products\" },\n{ \"skill\": [\"Collaboration\", \"Successful solutions\"], \"reference\": \"Collaborate with product managers, designers, researchers, and executives to create successful and compelling solutions\" },\n{ \"skill\": [\"Challenging problems\", \"State-of-the-art techniques\"], \"reference\": \"Solve challenging cutting-edge problems, using state-of-the-art techniques, services, and best practices\" }]"}
{"0":" [{ \"skill\": \"SQL (DML, DDL)\", \"reference\": \"2-4 years SQL (both DML and DDL)Exposure to 1-2 relational Database Systems\" },\n{ \"skill\": \"Data Visualization\/Analysis\", \"reference\": \"2-4 years Data Visualization, Analysis, or reporting\" },\n{ \"skill\": \"Relational Database Experience\", \"reference\": \"Exposure to 1-2 relational Database Systems (Postgres, Oracle, SQL Server, Snowflake, RedShift, BigQuery etc.)\" }]"}
{"0":" [\n    { \"skill\": \"Data Engineering\", \"reference\": \"As a Data Engineer at uConnect, you'll be at the forefront demonstrating the ROI of our products to both existing and prospective customers. You'll spearhead initiatives related to analytics tooling, guide scoping conversations, and contribute significantly to data programming.\" },\n    { \"skill\": \"Data Management\", \"reference\": \"In this role, you will have a direct impact on our data-driven journey by managing data pipelines and an event-driven analytics model.\" },\n    { \"skill\": \"Data Analysis and Visualization\", \"reference\": \"Creating data visualizations and dashboards is part of your responsibilities along with research on tools, techniques, and regulatory issues related to data.\" }\n]"}
{"0":" [{ \"skill\": [\"Azure infrastructure\", \"Senior Data Engineer\"], \"reference\": \"The Senior Data Engineer for the Azure infrastructure will be responsible for the day to day operations of a large data warehouse.\"},\n { \"skill\": [\"Big Data\", \"Dynamic ETL pipelines\"], \"reference\": \"Utilizing experience with Big Data, this position will drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.\"},\n { \"skill\": [\"Data warehouse\", \"Business alignment\"], \"reference\": \"Will work closely with the business, product team, and the technical staff to ensure alignment to goals and objectives.\"}]"}
{"0":" [\n  { \"skill\": \"Data Analysis\", \"reference\": \"Analyzes needs and requirements of existing and proposed systems, analyzing data to spot anomalies, trend and correlate similar data sets, and evaluating failures, defects, systemic problems and hardware.\" },\n  { \"skill\": \"SQL Expertise\", \"reference\": \"May also develop, implement, and maintain data systems to meet designs, models and specifications. This person will analyze client production issues and create SQL statements to fix the issue.\" },\n  { \"skill\": \"Database Management\", \"reference\": \"2-4 years Data Visualization, Analysis, or reporting, exposure to 1-2 relational Database Systems (Postgres, Oracle, Sql Server, Snowflake, RedShift, BigQuery etc.)\" }\n]"}
{"0":" [\n    { \"skill\": \"Data Engineering\", \"reference\": \"Support dynamic, long-term federal government enterprise big data programs\" },\n    { \"skill\": \"Agile Environment\", \"reference\": \"Working in an Agile environment to produce big data analytics solutions\" },\n    { \"skill\": \"Multi-disciplinary Teamwork\", \"reference\": \"Collaborate with client stakeholders and technical employees, optimize data collection, storage, and usage\" }\n]"}
{"0":" [\n    { \"skill\": \"Data Engineering, Machine Learning\", \"reference\": \"Design, construct, install, test, and maintain highly scalable data management systems. Collaborate with our data science team to design and develop machine learning models and algorithms.\" },\n    { \"skill\": \"Machine Learning, Algorithms\", \"reference\": \"Create high-performance algorithms, predictive models, and prototypes. Develop custom software components and analytics applications.\" },\n    { \"skill\": \"Data Acquisition, Data Modeling\", \"reference\": \"Research opportunities for data acquisition and new uses for existing data. Create data set processes for data modeling, mining, and production. Employ a variety of languages and tools to marry systems together.\" }\n]"}
{"0":" [\n    { \"skill\": \"Data Engineering\", \"reference\": \"The Data Engineer will be focusing on building out data feeds and tooling from our application platform\" },\n    { \"skill\": \"Software Development\", \"reference\": \"S\/he will have an understanding of application development\" },\n    { \"skill\": \"Problem Solving\", \"reference\": \"The ideal candidate will have professional experience building data pipelines in a technical environment. S\/he will have the ability to drive efforts from start to finish as a self-motivator, and solve code level problems quickly and efficiently.\" }\n]"}
{"0":" [{ \"skill\": [\"Senior Data Engineer\", \"Airflow pipelines\", \"Python\", \"Data science and analytics\"], \"reference\": \"About the job\" }, { \"skill\": [\"Expertise with Airflow\", \"SageMaker pipelines\", \"ETL libraries\"], \"reference\": \"We are looking for an experienced Senior Data Engineer to join our data team.\" }, { \"skill\": [\"Data transformation and integration\", \"Deployment Optimization platform\", \"Pipeline health monitoring\"], \"reference\": \"Develop reusable Python modules and libraries for data transformation and integration\" }, { \"skill\": [\"Python programming skills\", \"Data structures, algorithms\", \"Distributed computing\"], \"reference\": \"What You Will Work On\" }, { \"skill\": [\"Git, CI\/CD, automated testing frameworks\"], \"reference\": \"Maintain and improve existing Airflow DAGs that move data through our analytics pipeline\" }]"}
{"0":" [\n  { \"skill\": \"Data Engineering\", \"reference\": \"About the job\" },\n  { \"skill\": \"Communication\", \"reference\": \"Because our clients are mostly US-based organizations, we look for the ability to communicate with professional proficiency in English, verbally and in writing.\" },\n  { \"skill\": \"Cross-Team Collaboration\", \"reference\": \"You are responsible for collaborating with peers and senior ICs in other functional departments\u2014most often data solutions, analytics engineering, and data analytics\u2014to develop and implement data engineering approaches that support our client engagement goals.\" },\n  { \"skill\": \"Engagement Delivery\", \"reference\": \"Consistently seeks out, delivers and maintains small projects or surface-specific tasks with the help of more experienced Engineers. Capable of self-directed project delivery and can help the team to prioritize work that maximizes team impact.\" },\n  { \"skill\": \"Tools & Technologies\", \"reference\": \"Programming languages (e.g. SQL, Python)Data Processing (e.g. Apache Spark, dbt)Cloud-based data warehouses (e.g., Snowflake, Google BigQuery)\" },\n  { \"skill\": \"Domain Expertise\", \"reference\": \"You have a strong foundation of knowledge in domains in which you're working.\" },\n  { \"skill\": \"Organizational Accountability\", \"reference\": \"Curiosity & Versatility You are generally comfortable with new contexts and roles and can point to multiple occasions where you've changed approach or tools quickly and efficiently in response to a situation.\" },\n  { \"skill\": \"Collaboration & Partnership\", \"reference\": \"You can reliably assist and support the facilitation of meetings and collaborative projects with team members and occasionally engage in discussions with external stakeholders, if needed.\" },\n  { \"skill\": \"Effective Communication\", \"reference\": \"Usually communicate effectively, clearly, concisely and in an audience-oriented way, actively listening to others to ensure understanding.\" },\n  { \"skill\": \"Essential Skills (Intangible Skills)\", \"reference\": \"\"Organizational Accountability You can manage your own time in consultation with others, effectively delivering individual tasks and assignments.Curiosity & Versatility You are generally comfortable with new contexts and roles and can point to multiple occasions where you've changed approach or tools quickly and efficiently in response to a situation.\" },\n  { \"skill\": \"Qualifications (Must Haves)\", \"reference\": \"\"Proven experience as a Data Engineer or related role, with a focus on designing and developing data pipelines.Strong programming skills in languages such as Python, Rust, Scala, or SQL.Deep knowledge of data warehousing and ETL\/ELT processes.\" },\n  { \"skill\": \"Physical Requirements\", \"reference\": \"\"Frequent sitting at a desk performing work on a computerReasonable accommodations may be made to enable individuals with disabilities to perform the essential functions\" },\n  { \"skill\": \"Compensation\", \"reference\": \"The pay range for this role is up to $115,000 if based in the USA and up to $58,000 if based in El Salvador.Please note that compensation packages are finalized after the interview process is concluded.\" },\n  { \"skill\": \"Core Company Values\", \"reference\": \"Velir is an established mid-sized agency with a top-tier portfolio of clients, ranging from the world's largest non-profits to Fortune 500 brands. We pride ourselves on our people-first culture and a low-ego workplace that embraces experimentation, collaboration and continuous improvement.\" },\n  { \"skill\": \"Bonus points for Data Modeling & Transformation\", \"reference\": \"You have a solid working knowledge of how to convert, cleanse, and structure data into usable formats\/structures to prepare it for analysis. You can effectively transform raw data into analysis-ready models for different use cases.\" },\n  { \"skill\": \"Data Infrastructure\", \"reference\": \"Experience with data warehouses like Snowflake, BigQuery, Databricks, or similar.Experience with streaming solutions such as Spark Streaming, Kafka, or Flink is desirable but not required.Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\" },\n  { \"skill\": \"Familiarity with machine learning operations (MLOps) techniques and platforms\", \"reference\": \"\"Experience with data warehouses like Snowflake, BigQuery, Databricks, or similar.Experience with streaming solutions such as Spark Streaming, Kafka, or Flink is desirable but not required.Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\" },\n  { \"skill\": \"Competency-based approach to pay\", \"reference\": \"We use a competency-based approach to base pay, which means it is based on the competencies and skills demonstrated for this role.Core Company ValuesVelir is an established mid-sized agency with a top-tier portfolio of clients, ranging from the world's largest non-profits to Fortune 500 brands. We pride ourselves on our people-first culture and a low-ego workplace that embraces experimentation, collaboration and continuous improvement.\" },\n  { \"skill\": \"Equal opportunity employer\", \"reference\": \"As an equal opportunity employer, we are firmly committing to diversity, equity, and inclusion in our hiring efforts. We recognize that we need team members from all backgrounds and experiences to successfully shape a positive employee experience as well as deliver our product and service solutions.\" }\n]"}
{"0":" [{ \"skill\": [\"Data Engineering\", \"Big Data Technologies\"], \"reference\": \"In this role you will be responsible for building the data and reporting infrastructure to power our systems, working with our team of engineers, product managers and designers; helping us create a better experience for the millions of Nav Small business users.\" },\n { \"skill\": [\"ETL\/ELT\", \"Data Warehousing\"], \"reference\": \"Experience with ETL\/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and\/or Fivetran You love building data-driven products and have expertise in one or more programming languages (ideally Python).\" },\n { \"skill\": [\"SQL\", \"Postgres\"], \"reference\": \"You have strong SQL experience, with expert level skill in Postgres, Snowflake and \/or AWS Redshift.\" }]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"The Data Engineer will optimize data pipelines for our cutting-edge real-time streaming cloud-hosted analytics platform.\"},\n{ \"skill\": \"Streaming Analytics Applications\", \"reference\": \"Expertise in databases and query optimization, including PostgresSQL, ElasticSearch, MongoDB, Redis, and Druid, with additional experience in NoSQL and graph databases being advantageous, Experience in horizontally scaling databases.\"},\n{ \"skill\": \"Cloud-Agnostic Solutions\", \"reference\": \"Proven success in deploying products in the cloud and SaaS model, with expertise in building optimized processing pipelines for streaming analytics applications and cloud-agnostic solutions (e.g., Kubernetes, Docker).\"}]"}
{"0":" [\n  { \"skill\": [\"Build scalable data pipelines\", \"Apache Airflow\"], \"reference\": \"Build scalable and fault-tolerant data pipelines in Google Cloud Platform using Apache Airflow\" },\n  { \"skill\": [\"Experience with Python, SQL\", \"AWS or GCP\"], \"reference\": \"3+ years of experience in Python and SQL You have experience with Apache Airflow and familiarity with AWS or GCP\" }\n]"}
{"0":" [{ \"skill\": \"Data infrastructure, ETL design, data warehousing, schema design, dimensional data modeling\", \"reference\": \"4+ years experience with data infrastructure, ETL design, data warehousing, schema design and dimensional data modeling\" },\n        { \"skill\": \"SQL, Python, or similar languages\", \"reference\": \"Experience with SQL, Python, or similar languages\" },\n        { \"skill\": \"Designing real-time pipelines\", \"reference\": \"Experience with designing and implementing real-time pipelines\" },\n        { \"skill\": \"Code management tooling\", \"reference\": \"Experience with code management tooling such as Git, Github\" },\n        { \"skill\": \"Data migrations in production settings\", \"reference\": \"Experience with data migrations in production settings\" },\n        { \"skill\": \"Deep understanding of modern data infrastructure\", \"reference\": \"You have a deep understanding of modern data tooling and infrastructure\" },\n        { \"skill\": \"Working independently with guidance\", \"reference\": \"Comfortable working independently with periodic guidance from engineering & business teams\" },\n        { \"skill\": \"Scale and automation\", \"reference\": \"You are a strong believer in scale and automation\" },\n        { \"skill\": \"Entrepreneurial, problem-solving, troubleshooting\", \"reference\": \"You are an entrepreneurial \u2014 you take initiative, solve problems and love to troubleshoot. You are a great collaborator and can communicate effectively.\" },\n        { \"skill\": \"Data quality, validation, dashboards, reporting tools\", \"reference\": \"Experience with data quality and validation, SQL performance tuning and E2E process optimization, experience creating reports and dashboards with modern business intelligence tools (Tableau, Metabase preferred), Experience working with Postgres, Hevo, and cloud or on-prem Big Data\/MPP analytics platform\" },\n        { \"skill\": \"African Fintech Ecosystem\", \"reference\": \"Experience in a high-growth team and\/or startup experience, Ability to communicate and prioritise effectively with a distributed team around the world\" },\n        { \"skill\": \"Travel (post-COVID)\", \"reference\": \"Opportunities for travel (Post-Covid19)\" }]"}
{"0":" [\n    { \"skill\": \"Data Engineering\", \"reference\": \"As a Data Engineer, you will work on challenging data engineering projects.\" },\n    { \"skill\": \"Database Management and Analytics\", \"reference\": \"You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics.\" },\n    { \"skill\": \"Technical Expertise\", \"reference\": \"Must-Haves: Minimum of 6 years of experience as a data engineer, Strong SQL skills in multiple database platforms, Experience with Snowflake, Databricks, Spark SQL, PySpark, and Python, Cloud experience (Azure, AWS, or GCP), Developing and maintaining ETL pipelines, Database design principles, Data modeling, schema development, and documentation.\" },\n    { \"skill\": \"Problem-solving\", \"reference\": \"Outstanding problem-solving skills\" },\n    { \"skill\": \"Communication Skills\", \"reference\": \"Excellent verbal and written communication skills and the ability to interact professionally with a diverse group, including executives, managers, and subject matter experts.\" }\n]"}
{"0":" [{ \"skill\": \"Java \/ Software Programming\", \"reference\": \"Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT, Highly motivated, self-learner and technically inquisitive, Experience in programming language Java and understanding of the software development life cycle, Project work on skills, Knowledge of Core Java, JavaScript, C++ or software programming, Spring boot, Microservices, Docker, Jenkins and REST API's experience\" },\n { \"skill\": \"Data Science \/ Machine Learning\", \"reference\": \"Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT, Highly motivated, self-learner and technically inquisitive, Experience in programming language Java and understanding of the software development life cycle, Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools\" },\n { \"skill\": \"Full Stack \/ Software Programmer\", \"reference\": \"Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT, Highly motivated, self-learner and technically inquisitive, Experience in programming language Java and understanding of the software development life cycle, Project work on skills, Knowledge of Core Java, JavaScript, C++ or software programming, Spring boot, Microservices, Docker, Jenkins and REST API's experience\" },\n { \"skill\": \"Data Science \/ Machine Learning\", \"reference\": \"Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT, Highly motivated, self-learner and technically inquisitive, Experience in programming language Java and understanding of the software development life cycle, Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools\" },\n { \"skill\": \"NLP, Text mining, Tableau, PowerBI, Tensorflow\", \"reference\": \"Preferred skills for Data Science\/Machine Learning Positions: NLP, Text mining, Tableau, PowerBI, Tensorflow\" }]"}
{"0":" [{ \"skill\": \"Data Analysis\", \"reference\": \"Analyzes needs and requirements of existing and proposed systems, and develops technical, structural, and organizational specifications.\" },\n{ \"skill\": \"SQL Expertise\", \"reference\": \"2-4 years SQL (both DML and DDL) exposure to 1-2 relational Database Systems.\" },\n{ \"skill\": \"Data Visualization\", \"reference\": \"Experience with Data Visualization, Analysis, or reporting.\" }]"}
{"0":" [\n    { \"skill\": \"Data Engineering\", \"reference\": \"Data Engineer to help develop best practices and consultative advice across a wide range of big data technologies.\" },\n    { \"skill\": \"Cloud-based environments\", \"reference\": \"Experience with cloud-based environments with distributed SQL pools.\" },\n    { \"skill\": \"ETL\/ELT processes\", \"reference\": \"Experience with designing and implementing ETL\/ELT processes in support of cloud-based data analytics environments.\" }\n]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"The Data Engineer will focus on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake\/Data Warehouse.\"},\n { \"skill\": \"Data Pipeline Development\", \"reference\": \"The ideal candidate will have professional experience in building data pipelines in a technical environment. S\/he will have an understanding of application development, data warehousing, demonstrate strong business judgment and be able to prioritize in a fast-paced environment.\"},\n { \"skill\": \"Data Architecture Design\", \"reference\": \"Collaborate and partner with application teams to understand data collection\/generation and design and partner to build and implement data feeds from our product tech stack.\"},\n { \"skill\": \"Database Expertise\", \"reference\": \"AWS experience is a plus, Ability to drive efforts from start to finish as a self-motivator, Knowledge in Data Warehousing is a MUST\"},\n { \"skill\": \"SQL Proficiency\", \"reference\": \"3+ years SQL experience is a must\"},\n { \"skill\": \"Data-Driven Problem Solving\", \"reference\": \"Solve code level problems quickly and efficiently\"},\n { \"skill\": \"Team Collaboration\", \"reference\": \"Participate in demos and code reviews, Promote software best approach, standards, and processes. Shape development processes to promote a high-quality output while continuing to iterate quickly.\"},\n { \"skill\": \"Agile Environment Management\", \"reference\": \"Incorporate best practices for security, performance, and data privacy into data pipelines, Proven ability to maintain performance level in a fast-paced agile environment\"},\n { \"skill\": \"Data Warehousing Experience\", \"reference\": \"MySQL & Vertica Experience a plus\/preferred\"}"}
{"0":" [\n  { \"skill\": \"Data Engineering\", \"reference\": \"As an Associate Data Engineer, you'll play a pivotal role in crafting innovative data engineering solutions\" },\n  { \"skill\": \"Problem Solving\", \"reference\": \"Your problem-solving skills will be put to the test as you work in partnership with our IT team to ensure data integrity in our \\\"data lake\\\"\" },\n  { \"skill\": \"Data Management & Integrity\", \"reference\": \"Manage new data sources in compliance with HIPAA and HCC, including access protocols\" }\n]"}
{"0":" [\n  { \"skill\": [\"Python\", \"Machine Learning\", \"SQL\"], \"reference\": \"Look for strong job history, Python, Machine Learning, SQL\" },\n  { \"skill\": [\"Experience with a variety of data management technologies\"], \"reference\": \"7+ years of industry experience measuring product performance and user behavior, Experience working with a variety of data management technologies\" },\n  { \"skill\": [\"Data Pipelines\", \"Data Warehousing Strategy\"], \"reference\": \"Create and maintain data pipelines to provide insights and drive business decisions, Establish data warehousing strategy (ex. Kimball, Data Vault, etc.)\" }\n]"}
{"0":" [\n    { \"skill\": [\"Good communicator\", \"Well organized\", \"Attention to detail\"], \"reference\": \"Key Skill(s) Good communicator, both written and verbal Well organized Attention to detail Team Player\"},\n    { \"skill\": [\"Team Player\"], \"reference\": \"\"},\n    { \"skill\": [\"PowerBI\", \"SQL\", \"Databricks\"], \"reference\": \"Key Technology \/Certification(s) PowerBI, SQL, Databricks\"}\n]"}
{"0":" [\n    { \"skill\": \"Data Engineer\", \"reference\": \"DataStage Data Engineer US\/GC\/Certain EADs \u2013 6 month remote contract.\" },\n    { \"skill\": \"Data Management\", \"reference\": \"Responsible for maintaining enterprise-grade platforms that enable data-driven solutions.\" },\n    { \"skill\": \"Problem Solving\", \"reference\": \"Analyzes and designs technical solutions to address production problems.\" }\n]"}
{"0":" [{ \"skill\": \"Java Full Stack Developer\", \"reference\": \"We are looking for entry-level software programmers, Java Full stack developers, Python\/Java developers\" },\n { \"skill\": \"Data Analysts \/ Data Scientists\", \"reference\": \"We are also looking for data analysts\/Data Scientists, Machine Learning engineers\" },\n { \"skill\": \"Machine Learning Engineers\", \"reference\": \"We are also looking for data analysts\/Data Scientists, Machine Learning engineers\" },\n { \"skill\": \"Highly Motivated and Self-Learner\", \"reference\": \"Highly motivated, self-learner, and technically inquisitive\" },\n { \"skill\": \"Technically Inquisitive\", \"reference\": \"Highly motivated, self-learner, and technically inquisitive\" },\n { \"skill\": \"Understanding of Software Development Life Cycle\", \"reference\": \"Experience in programming language Java and understanding of the software development life cycle\" },\n { \"skill\": \"Programming Language Java Knowledge\", \"reference\": \"Experience in programming language Java and understanding of the software development life cycle\" },\n { \"skill\": \"Project Work on Required Skills\", \"reference\": \"Project work on the skills\" },\n { \"skill\": \"Knowledge of Statistics, SAS, Python\", \"reference\": \"Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools\" }]"}
{"0":" [{ \"skill\": [\"Data Engineering\", \"Power BI Development\"], \"reference\": \"As a Data Engineer II with a specialization in MS Power BI, you will lead the development and maintenance of data-driven solutions, creating compelling visualizations, and ensuring data integrity.\"},\n { \"skill\": [\"Data Pipeline Management\", \"Optimization\"], \"reference\": \"Data Pipeline Development & Management: Design, construct, install, and maintain large-scale processing systems and other infrastructure. Manage and optimize data pipelines, ensuring data availability, accuracy, and optimal performance.\"},\n { \"skill\": [\"Cross-Functional Collaboration\", \"Data Governance\"], \"reference\": \"Collaborate with IT, analytics, and business teams to ensure seamless integration of systems and tools. Collaborate with data governance teams to ensure data quality, compliance, and consistency.\"}]"}
{"0":" [\n    { \"skill\": [\"Data Engineer\", \"Remote\"], \"reference\": \"Urgent Need Data Engineer (REMOTE) Location: Remote\" },\n    { \"skill\": [\"4+ years' experience developing data centric applications\"], \"reference\": \"\" },\n    { \"skill\": [\"Scala, DataBricks\", \"Strong understanding of streaming and batch data processing best practices\"], \"reference\": \"\" },\n    { \"skill\": [\"Advanced knowledge of data formats\", \"Data modelling experience\", \"Shaping and transforming data into 3NF and dimensional models\"], \"reference\": \"\" },\n    { \"skill\": [\"Advanced understanding of Data Lake file systems for large volumes of data\"], \"reference\": \"\" }\n]"}
{"0":" [\n    { \"skill\": [\"Data Engineer\", \"SHAPE the Future of Healthcare Analytics\"], \"reference\": \"Join Us as a Data Engineer - Shape the Future of Healthcare Analytics with HANDLE Global!\" },\n    { \"skill\": [\"SQL Expert\", \"Python \/ Spark Proficiency\"], \"reference\": \"5+ years of experience with SQL. and 3+ years with Python \/ Spark\" },\n    { \"skill\": [\"Modern Data Stack Knowledge\", \"Lifelong Learning Attitude\", \"Problem-Solving Skills\"], \"reference\": \"Intrigued by the term \u2018modern data stack' and embody a lifelong learning attitude.Demonstrated prowess in problem-solving.\" },\n    { \"skill\": [\"Cloud Data Warehouse Expertise\", \"Data Quality Standards Knowledge\", \"Version Control\"], \"reference\": \"Proficient with cloud data warehouses such as Databricks, Snowflake, BigQuery, etc.Knowledge of data quality standards and schema enforcement.Familiarity with version control platforms like GitLab or Github.\" },\n    { \"skill\": [\"Agile Mindset\", \"Startup Experience\"], \"reference\": \"Preferred: Agile or startup environment background\" },\n    { \"skill\": [\"Medallion Style Pipelines\", \"Query Optimization\", \"dbt, Delta Live Tables, BI Semantics Layers Experience\"], \"reference\": \"Expertise in query optimizationExperience with dbt, Delta Live Tables, or BI semantics layers for data model building and maintenance\" },\n    { \"skill\": [\"CI\/CD Knowledge\", \"Data Dictionaries\", \"Dagster, Airflow, Prefect Experience\"], \"reference\": \"Experience with CI\/CD pipelines and platforms like AWS\/Azure\/GCP.Track record of creating comprehensive data dictionaries.Familiarity with data orchestration tools such as Airflow, Prefect, Dagster, etc.\" },\n    { \"skill\": [\"DAGS & Mermaid Experience\", \"Data Orchestration Tools Knowledge\"], \"reference\": \"A penchant for DAGS and Mermaid. Experience with data orchestration tools such as Airflow, Prefect, Dagster, etc.\" }\n]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"About the job\" }, { \"skill\": \"Building data pipelines\", \"reference\": \"The Data Engineer will be focusing on building out data feeds and tooling from our application platform\" }, { \"skill\": \"Application development, Data Warehousing\", \"reference\": \"S\/he will have an understanding of application development, data warehousing\" }]"}
{"0":" [{ \"skill\": [\"ETL\", \"SQL\", \"NoSQL\", \"Hadoop\", \"Spark\", \"Kafka\", \"Apache Nifi\", \"Talend\", \"Informatica\", \"AWS\", \"Azure\", \"GCP\", \"Python\", \"Pandas\", \"R\", \"Matplotlib\", \"Seaborn\", \"Tableau\", \"TensorFlow\", \"PyTorch\", \"AWS Redshift\", \"Azure Synapse\"], \"reference\": \"Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse.\" },\n        { \"skill\": [\"Solid work track record\", \"Delivering results\"], \"reference\": \"Must have a solid work track record of delivering results.\"},\n        { \"skill\": [\"Excellent communication skills\"], \"reference\": \"Must have excellent communication skills\"}]"}
{"0":" [\n    { \"skill\": \"Data Engineering\", \"reference\": \"The Data Engineer will be focusing on building out data feeds and tooling from our application platform, enabling rapid ingestion into our centralized Data Lake\/Data Warehouse.\" },\n    { \"skill\": \"Problem-Solving\", \"reference\": \"Participate in demos and code reviews, promote software best approach, standards, and processes, shape development processes to promote high-quality output while continuing to iterate quickly.\" },\n    { \"skill\": \"Data Warehousing\", \"reference\": \"Knowledge in Data Warehousing is a MUST.\" }\n]"}
{"0":" [{ \"skill\": \"High-growth industry professionals\", \"reference\": \"We are inviting professionals in high-growth industries\" },\n { \"skill\": \"Job seekers with next move\/opportunity\", \"reference\": \"designed to help job seekers get discovered by our partners based on their anticipated hiring needs\" },\n { \"skill\": \"Support resources for career endeavors\", \"reference\": \"Provide optional support and resources for job seekers in their career endeavors\" }]"}
{"0":" [{ \"skill\": \"Data Engineering, Analytics\", \"reference\": \"Maintain data architectures, transform data into actionable insights, work with application engineers and data analysts.\"},\n { \"skill\": \"Cloud Platforms (GCP), Data Management\", \"reference\": \"Experience with Cloud Platforms in particular GCP and Snowflake, knowledge of data governance, and data management best practices.\"},\n { \"skill\": \"Data Architecture, SQL, Communication\", \"reference\": \"Develop and maintain data architectures focusing on analytics, strong proficiency in SQL, data modelling, and communication skills for non-technical stakeholders.\"}]"}
{"0":" [\n  { \"skill\": \"High-growth industries\", \"reference\": \"We are inviting professionals in high-growth industries\" },\n  { \"skill\": \"F4S Talent Pool\", \"reference\": \"The F4S Talent Pool is a pilot project\" },\n  { \"skill\": \"Predictive analytics, team motivation and performance\", \"reference\": \"Backed by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance\" }\n]"}
{"0":" [{ \"skill\": \"High-growth industry experience\", \"reference\": \"Inviting professionals in high-growth industries.\" },\n { \"skill\": \"Work style assessment\", \"reference\": \"F4S work style assessment which measures 48 key attitudes and motivations.\" },\n { \"skill\": \"Predictive analytics\", \"reference\": \"Backed by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance.\" }]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"Build and maintain data pipelines delivering core insights\" },\n { \"skill\": \"Python Expertise\", \"reference\": \"Expertise in Python\" },\n { \"skill\": \"Leadership\", \"reference\": \"Comfortable leading research and development projects that produce new designs, products, and processes\" }]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"Designing and implementing data ingestion pipelines from multiple sources, developing scalable frameworks for ingesting data sets, ensuring data integrity, reliability, and availability, maintaining data standardization and deduplication.\" },\n{ \"skill\": \"Cloud Solutions Expertise\", \"reference\": \"Expertise in designing and deploying data applications on cloud solutions like Azure or AWS.\" },\n{ \"skill\": \"SQL Proficiency\", \"reference\": \"Expert knowledge of SQL, experience with data modeling, ETL development, and data integration.\" }]"}
{"0":" [{ \"skill\": \"High-growth industries professional\", \"reference\": \"We are inviting professionals in high-growth industries\" },\n { \"skill\": \"Job seeker support and resources\", \"reference\": \"Provide optional support and resources for job seekers in their career endeavors\" },\n { \"skill\": \"Understanding personal motivations\", \"reference\": \"Backed by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations\" }]"}
{"0":" [\n    { \"skill\": \"Extensive expertise with data infrastructure and data engineering\", \"reference\": \"Develop and operate data infrastructure and pipelines to enable robust data for analytics and reporting in Marvel SNAP\" },\n    { \"skill\": \"Deep expertise in analytical database technologies (SQL and NoSQL)\", \"reference\": \"Empower the SNAP Marketing team with high-quality data to improve user acquisition (UA)\" },\n    { \"skill\": \"Proficiency in SQL and Python\", \"reference\": \"Enable SNAP Product Management, Analytics, Design, Engineering, and Production teams to gain insights from analytics quickly\" }\n]"}
{"0":" [{ \"skill\": \"Data Engineering\", \"reference\": \"As a founding Data Engineer at Rogo, you will help build out our real-time data pipelines for millions of unstructured financial documents to feed our financial LLM.\" },\n { \"skill\": \"AI Frontier\", \"reference\": \"We're creating a category-defining AI company built on top of foundational AI models like GPT-4.\" },\n { \"skill\": \"Cutting-edge Data Management\", \"reference\": \"Experience with Typescript, stream processing, and knowledge of Datadog.\" }]"}
{"0":" [{ \"skill\": \"ETL development\", \"reference\": \"Specialist in ETL development with a demonstrated understanding of transactional data processing, streaming data and data pipeline best practices.\" },\n { \"skill\": \"Streaming data\", \"reference\": \"Experience in build, unit test, and deployment of Informatica ETL processes. Knowledgeable in making REST API calls within data processes. Familiar with real-time data pipeline platforms, preferably StreamSets, AWS Glue or similar platform.\" },\n { \"skill\": \"Data streaming\", \"reference\": \"Hands on experience with data streaming in Apache Kafka.\" }]"}
{"0":" [\n  { \"skill\": \"Expert in Cosmos\/Azure, Data Engineering, API Architectures\", \"reference\": \"Primary Skillset: Expert in Cosmos \/ Azure, recommend architecture changes to stay with changing system demands Secondary Skillset: Data Engineering \/ API Architectures\" },\n  { \"skill\": \"Healthcare Domain Experience, Health Experience\", \"reference\": \"Industry Experience: Health Experience\" }\n]"}
{"0":" [{ \"skill\": \"High-growth Industries\", \"reference\": \"Inviting professionals in high-growth industries\" },\n { \"skill\": \"Job Seekers Discovery\", \"reference\": \"Help job seekers get discovered by our partners based on their anticipated hiring needs\" },\n { \"skill\": \"Talent Pool Development\", \"reference\": \"Expanding talent pool for future job opportunities\" }]"}
{"0":" [{ \"skill\": \"High-growth industries\", \"reference\": \"We are inviting professionals in high-growth industries\" },\n{ \"skill\": \"Job seekers discovery\", \"reference\": \"Help job seekers get discovered by our partners based on their anticipated hiring needs\" },\n{ \"skill\": \"Career endeavor support\", \"reference\": \"Provide optional support and resources for job seekers in their career endeavors\" }]"}
{"0":" [{ \"skill\": \"Data Engineer, SSIS, Snowflake Platform\", \"reference\": \"This person needs to have prior experience migrating to Snowflake and strong SSIS\" },\n { \"skill\": \"Sr. Data Engineer, Prior Healthcare Experience (Medicare, Medicaid)\", \"reference\": \"Requirements: Strong with SSIS and Exposure to Snowflake Platform; Prior healthcare experience (Medicare, Medicaid)\" },\n { \"skill\": \"Remote Work, Live in West Coast States\", \"reference\": \"The position is remote but the candidate has to live in one of the following states. Preferably closer to the west coast due to the PST schedule.\" }]"}
{"0":" [{ \"skill\": \"Java \/ Software Programming\", \"reference\": \"Experience in programming language Java and understanding of the software development life cycle\" },\n{ \"skill\": \"Machine Learning\/ Data Science\", \"reference\": \"Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\" },\n{ \"skill\": \"Full Stack \/ Software Programming\", \"reference\": \"Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle\" }]"}
{"0":" [\n    { \"skill\": \"Strong problem solving skills\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Strong communication skills\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Strong SQL skills\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Expert in using data warehousing solutions\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Experience with data ingestion services\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Experience with business analytics for marketing and sales\", \"reference\": \"\"About this role: As our first data engineer, you will have the opportunity to make important contributions to various aspects of our data platform. Your main responsibilities are: Build a highly scalable data warehouse Propose, design, and implement data ingestion pipelines (ELT\/ETL) Maintain our local and cloud data platforms Understand and interpret business intelligence requirements and translate them into technical solutions Build business analytics and dashboards to address sales and marketing needs.\" },\n    { \"skill\": \"Strong programming skills in JavaScript and Python\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Experience with HubSpot\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Experience working with marketing and sales teams\", \"reference\": \"\"About you: You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.\" },\n    { \"skill\": \"Strong sense of ownership\", \"reference\": \"\"About this role: As our first data engineer, you will have the opportunity to make important contributions to various aspects of our data platform. Your main responsibilities are: Build a highly scalable data warehouse Propose, design, and implement data ingestion pipelines (ELT\/ETL) Maintain our local and cloud data platforms Understand and interpret business intelligence requirements and translate them into technical solutions Build business analytics and dashboards to address sales and marketing needs.\" }\n]"}
{"0":" [{ \"skill\": \"Snowflake and AWS\", \"reference\": \"3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)\" },\n        { \"skill\": \"Snowflake Developer\", \"reference\": \"Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts\" },\n        { \"skill\": \"Snowflake Services\", \"reference\": \"Must have previous experience working with security datasets, Strong programming skills in SQL\" },\n        { \"skill\": \"SQL Efficiency\", \"reference\": \"Robust understanding of data partitioning and other optimization techniques in Snowflake. Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption.\" },\n        { \"skill\": \"Languages\", \"reference\": \"Highly skilled in one or more languages such as Python, Scala\" },\n        { \"skill\": \"SDLC Tools\", \"reference\": \"Strong knowledge of SDLC tools and technologies\" },\n        { \"skill\": \"Data Integration\", \"reference\": \"Skilled in Data Integration from different sources such as APIs, databases, flat files, event streaming\" },\n        { \"skill\": \"Database Design\", \"reference\": \"Good understanding of Data Modeling and Database Design Principles\" },\n        { \"skill\": \"ETL Tools\", \"reference\": \"Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed.\" },\n        { \"skill\": \"Scalable Distributed Tech\", \"reference\": \"Strong experience with scalable and distributed Data Technologies such as Spark\/PySpark, DBT and Kafka, to be able to handle large volumes of data\" },\n        { \"skill\": \"Data Warehousing AWS RedShift\", \"reference\": \"Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT\/ETL processes that extract data from source systems, transform it (DBT), and load it into the data warehouse.\" },\n        { \"skill\": \"Apache Airflow\", \"reference\": \"Strong experience in Orchestration using Apache Airflow\" },\n        { \"skill\": \"Cloud Computing\", \"reference\": \"Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS\/ECR, IAM, CloudWatch, Redshift, etc\" },\n        { \"skill\": \"Data Quality and Governance\", \"reference\": \"Good understanding of Data Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that data is accurate, complete, and consistent.\" },\n        { \"skill\": \"Problem Solving\", \"reference\": \"Good Problem-Solving skills: being able to troubleshoot data processing pipelines and identify performance bottlenecks and other issues\" },\n        { \"skill\": \"Architecture, Observability\", \"reference\": \"Care about architecture, observability, testing, and building reliable infrastructure and data pipelines.\" },\n        { \"skill\": \"Storage Layer\", \"reference\": \"Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and performance tuning.\" },\n        { \"skill\": \"Incident Resolution\", \"reference\": \"Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL queries and database operations.\" },\n        { \"skill\": \"Modern Analytics\", \"reference\": \"Assess best practices and design schemas that matches business needs for delivering a modern analytics solution (descriptive, diagnostic, predictive, prescriptive)\" },\n        { \"skill\": \"Agile Team Participation\", \"reference\": \"Be an active member of our Agile team, participating in all ceremonies and continuous improvement activities.\" }]"}
{"0":" [\n  { \"skill\": [\"SQL\", \"Tableau Desktop \/ Creator\"], \"reference\": \"Role should have strong experience in writing SQL queries to extract data for reports.\" },\n  { \"skill\": [\"AWS - Athena\", \"AWS - S3 Glue\", \"RedShift\", \"SNS\/SQS\", \"Lambda\", \"Tableau API Integration\"], \"reference\": \"Must Haves AWS - Athena, AWS - S3Gluehands-on development of reporting applications\" },\n  { \"skill\": [\"MS Excel\"], \"reference\": \"Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\" }\n]"}
{"0":" [\n  { \"skill\": \"Data Engineering\", \"reference\": \"Design, build, and maintain our data architecture, infrastructure, and systems.\" },\n  { \"skill\": \"Data Pipelines & Workflows\", \"reference\": \"Develop scalable data pipelines for efficient processing of large volumes of data.\" },\n  { \"skill\": \"Collaboration\", \"reference\": \"Work with data analysts and scientists to design and implement data models.\" }\n]"}
{"0":" [{ \"skill\": \"Snowflake and AWS expertise\", \"reference\": \"3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)\" },\n        { \"skill\": \"Cybersecurity and Financial sector experience\", \"reference\": \"Implementing cyber-security use cases. Working in the Financial sector.\" },\n        { \"skill\": \"SQL proficiency, Data integration skills\", \"reference\": \"Strong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation.\" },\n        { \"skill\": \"Data security measures, Data architecture and Database Design\", \"reference\": \"Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption. Robust understanding of data partitioning and other optimization techniques in Snowflake.\" },\n        { \"skill\": \"Additional programming languages, SDLC tools expertise\", \"reference\": \"Skilled in one or more languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration, storage, processing and manipulation. Strong knowledge of SDLC tools and technologies.\" },\n        { \"skill\": \"Data Integration, Data Modeling and Database Design\", \"reference\": \"Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed. Strong understanding of Data Modeling and Database Design Principles.\" },\n        { \"skill\": \"Scalable data technologies, DWH solutions knowledge\", \"reference\": \"Strong experience with scalable and distributed Data Technologies such as Spark\/PySpark, DBT and Kafka to handle large volumes of data. Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift.\" },\n        { \"skill\": \"Data quality assurance, Troubleshooting skills\", \"reference\": \"Strong experience in orchestration using Apache Airflow. Expertise in Cloud Computing in AWS. Good understanding of Data Quality and Governance including implementation of data quality checks and monitoring processes.\" },\n        { \"skill\": \"Agile teamwork, problem-solving abilities\", \"reference\": \"Contribute to data engineering support by collaborating closely with cross-functional teams. Troubleshoot complex data issues, incidents and resolve bottlenecks in SQL queries.\" },\n        { \"skill\": \"Architecture, observability, documentation\", \"reference\": \"Evaluate and implement cutting-edge technologies and continue learning and expanding skills in data engineering and cloud platforms. Document data engineering processes and data flows.\" }]"}
{"0":" [\n    { \"skill\": \"4+ years experience building and scaling data infrastructure, Highly motivated self-starter, Expertise in SQL and Python\", \"reference\": \"You have 4+ years of experience in building and scaling data infrastructure at a growth-stage startup. Highly motivated self-starter who is eager to make an impact and unafraid of tackling large, complex problems while delivering high-quality results.\" },\n    { \"skill\": \"Expertise in real-time and batch data pipelines using tools like dbt, Airflow, Spark, Kafka, S3, Airbyte, Census, etc., Experience with Data Warehouses and Data Lakes\", \"reference\": \"Also experienced in building real-time and batch data pipelines using tools such as dbt, Airflow, Spark, Kafka, S3, Airbyte, Census, etc. Also experienced in data transformation, event tracking frameworks, and experimentation frameworks.\" },\n    { \"skill\": \"Solid understanding of using data to inform product roadmap, Strong written communication skills, Documentation-first culture\", \"reference\": \"Solid understanding of using data to inform the product roadmap. We\u2019re a documentation-first culture. Desire and ability to work autonomously and drive your work.\" }\n]"}
{"0":" [\n  { \"skill\": \"Data Engineering\", \"reference\": \"Design and develop our data and data pipeline architecture, optimizing data flow and collection for cross-functional teams.\" },\n  { \"skill\": \"Data Pipeline Builder\", \"reference\": \"Experienced data pipeline builder and data wrangler who enjoys optimizing data systems.\" },\n  { \"skill\": \"Data Architecture Optimization\", \"reference\": \"Support our software developers, database architects, data analysts and data scientists on data initiatives and ensure that optimal data delivery architecture is consistent throughout projects.\" }\n]"}
{"0":" [{ \"skill\": [\"PySpark\", \"ETL\", \"Azure Data Bricks\/Data Factory\"], \"reference\": \"Top skills: Pyspark Azure Databricks and\/or Data Factory Experience leading and managing offshore teams. ETL High volumes of data Pyspark Python Experience Leading or managing offshore teams\" },\n        { \"skill\": [\"Experience with Large Scale Projects\", \"Hands-on Experience\"], \"reference\": \"Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept. Candidates need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool.\" },\n        { \"skill\": [\"Data Engineer Responsibilities\", \"Agile Methodology\"], \"reference\": \"Responsibilities Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data Factory. Working with event-based streaming technologies to ingest and process data. Design and Implement Data Lakehouse. Working within an Agile delivery \/ DevOps methodology to deliver proof of concept and production implementation in iterative sprints.\" }]"}
{"0":" [\n  { \"skill\": [\"Hadoop\", \"Spark\", \"Kafka\"], \"reference\": \"Experience with big data tools\" },\n  { \"skill\": [\"Relational SQL\", \"NoSQL databases\", \"Oracle, MS SQL Server, Postgres, Cassandra\"], \"reference\": \"Experience with relational SQL and NoSQL databases\" },\n  { \"skill\": [\"Azkaban\", \"Luigi\", \"Airflow\"], \"reference\": \"Experience with data pipeline and workflow management tools\" },\n  { \"skill\": [\"Informatica\", \"MuleSoft\", \"Talend\", \"TIBCO\"], \"reference\": \"Experience with data integration services solutions from vendors\" },\n  { \"skill\": [\"AWS (EC2, Glue, EMR, RDS, Redshift)\", \"Amazon Redshift\"], \"reference\": \"Experience with cloud-based data services such as AWS\" },\n  { \"skill\": [\"Storm\", \"Spark-Streaming\", \"Kafka\"], \"reference\": \"Experience with stream-processing systems\" },\n  { \"skill\": [\"Python\", \"R\", \"Java\", \"C++\", \"Scala\"], \"reference\": \"Experience with object-oriented\/object function scripting languages\" }\n]"}
{"0":" [{\n  \"skill\": \"AWS services, Oracle, Informatica\",\n  \"reference\": \"About the job\"\n}, {\n  \"skill\": \"Big data experience, Data compaction using Hive, Alternative approaches\",\n  \"reference\": \"Need to have experience and knowledge around oracle and informatica Experience working with big data - have all structured data and currently do data compaction using hive They aren't sure if hive is the right approach, would want someone to help them figure out what the best approach would be\"\n}, {\n  \"skill\": \"Strong ETL development experience\",\n  \"reference\": \"They aren't sure if hive is the right approach, would want someone to help them figure out what the best approach would be\"\n}, {\n  \"skill\": \"Architect level understanding, Hands-on execution, Python as core language, PySpark processing experience\",\n  \"reference\": \"Want someone at an architect level, but need to be more hands on than a traditional architect - this person would work with architects to help implement the work they are doing Python as core language - using pyspark processing so need experience there as well\"\n}, {\n  \"skill\": \"Data modeling \/ data analytics knowledge (optional)\",\n  \"reference\": \"Working with modeling and analytics users as their customers so knowledge in the data modeling \/ data analytics space would be a plus, not necessary though\"\n}]"}
{"0":" [\n    { \"skill\": \"Java, Full stack, Software Programmer, Bachelors degree or Masters\", \"reference\": \"Required Skills for Java \/Full stack\/Software Programmer\" },\n    { \"skill\": \"Data Science, Machine learning, Bachelors degree or Masters\", \"reference\": \"Required Skills for Data Science\/Machine Learning Positions\" }\n]"}
{"0":" [{ \"skill\": [\"PySprak\", \"Azure Data Brick\", \"ETL\"], \"reference\": \"They need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool. Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept. Candidates need more thought leadership skills.\" },\n { \"skill\": [\"Azure Cloud\", \"Data Engineer\", \"Agile Delivery\"], \"reference\": \"Responsibilities include Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data Factory.\"},\n { \"skill\": [\"Python\", \"Leadership\", \"Offshore Team Management\"], \"reference\": \"Data engineer has ownership of designing and developing large scale data solutions. They have a lot of doers currently and need someone who can help guide the team. Experience leading or managing offshore teams.\" }]"}
{"0":" [\n  { \"skill\": \"Data Analytics\", \"reference\": \"Support various areas of the business such as marketing analytics, sales and revenue and operations.\" },\n  { \"skill\": \"Data Modeling\", \"reference\": \"Model data from the data warehouse and cloud storage to support the development of insights for the business teams.\" },\n  { \"skill\": \"Collaboration\", \"reference\": \"Work collaboratively with team members to collect business requirements, define successful analytics outcomes, and design data models.\" },\n  { \"skill\": \"Engineering Best Practices\", \"reference\": \"Apply engineering best practices to analytics code, maintain documentation and definitions, advocate for coding standards, and review code.\" },\n  { \"skill\": \"Data Warehouse Experience\", \"reference\": \"Experience using Snowflake and dbt a plus, working with data warehouses, data transformation tools, technical documentation creation.\" },\n  { \"skill\": \"Adaptability\", \"reference\": \"Demonstrated ability to learn new systems quickly and master data integrity among multiple systems.\" }\n]"}
{"0":" [{ \"skill\": \"Complex project management\", \"reference\": \"Experience in managing and communicating complex projects\" },\n{ \"skill\": \"Data governance & change management\", \"reference\": \"Knowledgeable on data governance concepts, Data Access Management, Change Management\" },\n{ \"skill\": \"Cross functional teamwork\", \"reference\": \"Collaborating with cross functional teams to accomplish project goals\" }]"}
{"0":" [{ \"skill\": [\"Data\/Backend Engineer\", \"Design and develop sophisticated schemas\", \"Work closely with data science team\"], \"reference\": \"The Loaded Product team is looking for a talented Engineer who will work alongside our pipeline engineer to warehouse and make available data to our various analytics products.\"},\n{ \"skill\": [\"Pragmatic, productive and creative development philosophy\", \"Passionate about problem-solving\"], \"reference\": \"Ideally, you have a pragmatic, productive, and creative development and design philosophy. A passion and pride for the work they do and the problems that they solve.\"},\n{ \"skill\": [\"Expertise in building scalable solutions\", \"Experience with data-heavy products\"], \"reference\": \"Expertise in building scalable solutions in a modern development environment, Experience contributing to and designing data-heavy products.\"}]"}
{"0":" [\n    { \"skill\": \"Java, Full Stack Development, Software Programming\", \"reference\": \"Required skills for Java \/Full Stack\/Software Programmer: Bachelors or Masters degree in Computer Science, motivated, self-learner, experience in Java programming language and understanding of the software development life cycle.\" },\n    { \"skill\": \"Python, Data Analytics, Machine Learning\", \"reference\": \"Required skills for Data Science\/Machine learning Positions: Bachelors or Masters degree in Computer Science, motivated, experience in Java programming and statistics, knowledge of Python, data visualization tools, and machine learning technologies like NLP, Text mining.\" },\n    { \"skill\": \"C++, Spring Boot, Microservices, Docker, Jenkins, REST API\", \"reference\": \"Required skills for Software Programmer: Bachelors or Masters degree in Computer Science, motivated, self-learner, experience in programming languages like Java, understanding of software development life cycle, project work on specific technologies.\" },\n    { \"skill\": \"Statistics, SAS, Python, Computer Vision\", \"reference\": \"Required skills for Data Science\/Machine learning Positions: Bachelors or Masters degree in Computer Science, motivated, experience in Java programming and statistics, knowledge of Python, data visualization tools, and machine learning technologies like NLP, Text mining.\" },\n    { \"skill\": \"NLP, Text Mining, Tableau, PowerBI, Tensorflow\", \"reference\": \"Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow\" }\n]"}
{"0":" [\n    { \"skill\": [\"Data Engineer\", \"Enterprise Data warehouse\/Datamart\", \"ETL background\"], \"reference\": \"Required Experience Data Engineer with Enterprise Data warehouse\/Datamart and ETL background.\" },\n    { \"skill\": [\"Python\", \"DataFrame\", \"APIs\", \"Batch processing\", \"Data pipelines\", \"Strong Python\"], \"reference\": \"Strong Python (DataFrame, APIs, Batch processing, Data pipelines) experience.\" },\n    { \"skill\": [\"Azure platform\", \"Azure Data Lake\", \"Data Bricks\", \"Delta Lake\", \"Data Factory\", \"Azure SQL Database\"], \"reference\": \"Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL Database\" },\n    { \"skill\": [\"Insurance domain\", \"P&C Insurance\"], \"reference\": \"Experience in Property & Casualty (P&C) Insurance domain is required.\" },\n    { \"skill\": [\"Data analysis\", \"Predictive modeling\", \"Policy performance\", \"Claims data\", \"Customer behavior\"], \"reference\": \"Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc.\" },\n    { \"skill\": [\"SQL experience\", \"Excel skills (Pivot Tables)\", \"Data modeling for Enterprise Data Warehouse ecosystems\"], \"reference\": \"Strong SQL experience with excellent Excel skills (i.e. Pivot Tables).Understanding of Data modeling for Enterprise Data Warehouse ecosystems\" },\n    { \"skill\": [\"Spark\", \"PySpark\", \"Data lake environment\", \"Developing data pipelines driven by events\/queues\"], \"reference\": \"Knowledge in Spark, PySpark preferable.Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events\/queues.\" },\n    { \"skill\": [\"File formats\", \"JSON\", \"Parquet\", \"CSV\"], \"reference\": \"Working knowledge on different file formats such as JSON, Parquet, CSV, etc.\" },\n    { \"skill\": [\"Data encryption\", \"Data masking\", \"Database experience in SQL Server\"], \"reference\": \"Familiarity with data encryption, data maskingDatabase experience in SQL Server\" }\n]"}
{"0":" [\n  { \"skill\": [\"PySpark\", \"Azure Databricks\", \"Data Factory\"], \"reference\": \"Top skills: Pyspark Azure Databricks and\/or Data Factory Experience leading and managing offshore teams. ETL\" },\n  { \"skill\": [\"ETL\", \"High volumes of data\"], \"reference\": \"Must Have Azure Cloud, Azure Data Brick, ETL, High volumes of data, PySpark, Python Experience Leading or managing offshore teams\" },\n  { \"skill\": [\"DevOps environment\", \"Azure Databricks\", \"Data Lake\"], \"reference\": \"Experience in building ETL pipelines, Hands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics\" }\n]"}
{"0":" [\n    { \"skill\": [\"Data Engineering\", \"ETL\", \"Data Modeling\"], \"reference\": \"Seeking a mid-level Data Engineer to perform ETL and Data Modeling.\" },\n    { \"skill\": [\"Database Design\", \"Integration with Third-Party Solutions\"], \"reference\": \"Must be U.S. Citizens or Permanent Residents. Essential functions and responsibilities: Conduct complex data mapping efforts between systems in a clear and well defined manner.\" },\n    { \"skill\": [\"Data Analysis\", \"Database Development\"], \"reference\": \"Essential functions and responsibilities: Conduct complex data mapping efforts between systems in a clear and well defined manner.\" },\n    { \"skill\": [\"Mentorship\", \"Improving Development Standards\"], \"reference\": \"Train and test for industry certifications, Identify, set, monitor, and achieve individual goals, Leverage mentorship and peer relationships to increase proficiency in development.\" },\n    { \"skill\": [\"ETL Experience\", \"Data Modeling Experience\", \"Database Systems Integration\"], \"reference\": \"Qualifications required: Bachelor's Degree in computer science, Information Technology, or related fields; 3+ years of experience developing data\/database systems and integrating with third-party solutions.\" },\n    { \"skill\": [\"Expertise in SQL\", \"Cloud Data Warehouse Technologies\"], \"reference\": \"Experience with Data Bricks (preferably with Azure) Experience with cloud data warehouse technologies, Azure preferred; Expertise with SQL 3+ years of experience working in a hybrid agile development methodology.\" },\n    { \"skill\": [\"Hybrid Agile Development\", \"Communication Skills\"], \"reference\": \"Excellent verbal and written communication skills.\" },\n    { \"skill\": [\"Working as Full-Time Employee\", \"No Visa Sponsorship Required\"], \"reference\": \"The ability to work as a full-time employee without requiring visa sponsorship now or in the future is required.\" }\n]"}
{"0":" [{\"skill\": [\"W2Python\", \"pyspark\"], \"reference\": \"Locals to United StatesOnly W2Python with pyspark data engineer\"},\n {\"skill\": [\"GIS\", \"data engineer\"], \"reference\": \"with GIS background preferably\"}]"}
{"0":" [{ \"skill\": [\"Data Engineer\", \"Enterprise Data warehouse\", \"Datamart ETL\"], \"reference\": \"Required Experience Data Engineer with Enterprise Data warehouse\/Datamart and ETL background.\"},\n{ \"skill\": [\"P&C Insurance Domain Expertise\", \"Strong Python Experience\"], \"reference\": \"Experience in Property & Casualty (P&C) Insurance domain is required. Strong Python experience.\"},\n{ \"skill\": [\"Azure Platform\", \"Data Processing Technologies\"], \"reference\": \"Experience in Azure platform including Azure Data Lake, Databricks, Delta Lake, Data Factory, SQL Database.\"},\n{ \"skill\": [\"Trends & Pattern Analysis\", \"Predictive Modeling\", \"Insurance Data\"], \"reference\": \"Experience in interpretation of insurance data to identify trends, patterns, and anomalies that can impact business performance and profitability.\"},\n{ \"skill\": [\"Data Analytics\", \"Policy Performance\"], \"reference\": \"Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc.\"},\n{ \"skill\": [\"SQL Expertise\", \"Excel Pivot Tables\"], \"reference\": \"Strong SQL experience with excellent Excel skills (i.e. Pivot Tables).\"},\n{ \"skill\": [\"Data Modeling\", \"Enterprise Data Warehouse Ecosystems\"], \"reference\": \"Understanding of Data modeling for Enterprise Data Warehouse ecosystems.\"},\n{ \"skill\": [\"Spark, PySpark\"], \"reference\": \"Knowledge in Spark, PySpark preferable.\"},\n{ \"skill\": [\"Data Lake Environment\", \"Event-Driven Pipelines\"], \"reference\": \"Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events\/queues.\"},\n{ \"skill\": [\"File Formats\", \"Encryption, Data Masking\"], \"reference\": \"Working knowledge on different file formats such as JSON, Parquet, CSV, etc. Familiarity with data encryption, data masking.\"},\n{ \"skill\": [\"SQL Server Database Experience\"], \"reference\": \"Database experience in SQL Server.\"}]"}
{"0":" [{ \"skill\": \"Snowflake SQL, Python programming\", \"reference\": \"2+ years of SQL experience, at least 1 year of basic Python programming knowledge\" },\n{ \"skill\": \"ETL\/ELT and Data Modeling\", \"reference\": \"prior experience using ETL, ELT, and basic data modeling\" },\n{ \"skill\": \"Cloud Technology Experience\", \"reference\": \"Experience with cloud technology is preferred\" }]"}
{"0":" [{ \"skill\": [\"Hive\", \"Kafka\", \"Python\", \"Spark\", \"UNIX\", \"Hadoop Platform\"], \"reference\": \"Required (Individual Role)  Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies\" },\n{ \"skill\": [\"Data Ingestion Pipeline Process\", \"Exception Handling\", \"Metadata Management\", \"Automated Data Integration\", \"Data Models\", \"Data Mappings\", \"Business Rules\"], \"reference\": \"Proficient in the data ingestion pipeline process, exception handling, and metadata management on Hadoop platforms.\" },\n{ \"skill\": [\"UNIX Shell Scripting\", \"Automated Technology Controls Development\", \"Enabling Data Access\", \"Data Cleaning\", \"Filtering\", \"Transforming\", \"Enriching Data\"], \"reference\": \"Hands-on experience creating automated data integration applications and Demonstrated experience cleaning, filtering, transforming data, and\/or enriching data.\" }]"}
{"0":" [\n  { \"skill\": \"Data Engineering\", \"reference\": \"Junior Data Engineer - US\/Canada Only, 1 year of project experience\" },\n  { \"skill\": \"AI and Machine Learning Solutions\", \"reference\": \"Work closely with subject matter experts to build and scale out machine learning and AI solutions\" },\n  { \"skill\": \"Data Pipeline Development\", \"reference\": \"Data pipeline development: Designed, implemented, managed data pipelines\" }\n]"}
{"0":" [\n  { \"skill\": [\"Data Engineering\", \"GCP Requirements\"], \"reference\": \"Must have skills: Should be able to handle GCP requirements\" },\n  { \"skill\": [\"ETL Management\", \"Data Integration\"], \"reference\": \"Experience with Big Data Technologies (Hadoop, ETL Tool, Spark. Hive etc..)\" },\n  { \"skill\": [\"Cloud Experience\", \"GitHub\"], \"reference\": \"Should be able to handle GCP requirements - ingesting into GCS\/ Big Query, working with GCP components\" }\n]"}
{"0":" [{ \"skill\": [\"Hive\", \"Kafka\", \"Python\", \"Spark\", \"UNIX\", \"Hadoop Platform\"], \"reference\": \"Required (Individual Role) Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka Significant experience in Python programming\" },\n { \"skill\": [\"Data Ingestion Pipeline Process\", \"Exception Handling\", \"Metadata Management\"], \"reference\": \"Proficient in the data ingestion pipeline process, exception handling, and metadata management on Hadoop platforms.\" },\n { \"skill\": [\"Automated Data Integration Applications\", \"Data Models\", \"Business Rules Specifications\"], \"reference\": \"Hands-on experience creating automated data integration applications using data models, data mappings and business rules specifications to load data warehouses, operational data stores, data marts, and data lakes while programmatically handling exceptions including late arriving, missing, or erroneous data.\" },\n { \"skill\": [\"UNIX Shell Scripting\", \"Automated Technology Controls\"], \"reference\": \"Experience in UNIX shell scripting Demonstrated experience developing\/expanding automated technology controls (e.g., Data Quality checks, Data Movement controls).\" },\n { \"skill\": [\"Data Access Management\", \"Databases or Dashboards\"], \"reference\": \"Demonstrated experience enabling access to data by way of databases or dashboards.\" },\n { \"skill\": [\"Cleaning, Filtering, Transforming, and Enriching Data\"], \"reference\": \"Experience in cleaning, filtering, transforming data, and\/or enriching data\" }]"}
{"0":" [\n    { \"skill\": \"Azure Data Engineering\", \"reference\": \"You will be working with all levels of technology from backend data processing technologies (Databricks\/Apache Spark) to other Cloud computing technologies \/ Azure Data Platform. You should be a strong analytical thinker, detail-oriented and love working with data with a strong background in data engineering and application development.\" },\n    { \"skill\": \"Data Analysis\", \"reference\": \"Querying and analyzing small and large data sets to discover patterns and deliver meaningful insights.\" },\n    { \"skill\": \"Collaboration\", \"reference\": \"Collaborate with cross-functional teams. Analyze and define with product teams the data migration and data integration strategies. Apply experience in analytics, data visualization and modeling to find solutions for a variety of business and technical problems. Integrate source systems with information management solutions and target systems for automated migration processes.\" }\n]"}
{"0":" [{ \"skill\": [\"B2 English level\", \"3+ years of experience in data integration\"], \"reference\": \"Qualifications B2 English level (Upper-intermediate) or higher, 3+ years of experience in data integration and database development.\"},\n{ \"skill\": [\"Proficiency in Core SQL, Azure SQL, Azure Data Factory, Databricks\"], \"reference\": \"Proficiency in Core SQL and Azure SQL, extensive experience with Azure Data Factory and Databricks.\"},\n{ \"skill\": [\"Experience with SSAS, Strong expertise in designing and implementing complex stored procedures\", \"Excellent problem-solving skills, attention to detail\"], \"reference\": \"Experience with SSAS, strong expertise in designing and implementing complex stored procedures, Excellent problem-solving skills and attention to detail.\"},\n{ \"skill\": [\"Long-term contract, Independent Contractor with Venon Solutions LLC\", \"Full-time PST, fully committed\"], \"reference\": \"Type of contract: Independent Contractor with Venon Solutions LLC, Contract duration: Long-term, Working hours: Full-time PST, fully committed.\"},\n{ \"skill\": [\"2 weeks of PTO (paid time off), Holidays from North American calendar\"], \"reference\": \"What do we offer?: Type of contract: Independent Contractor with Venon Solutions LLC, Benefits: 2 weeks of PTO (paid time off)Holidays: from the North American calendar.\"}]"}
{"0":" [{ \"skill\": \"SQL, Python or other relational database programming language\", \"reference\": \"Interest, familiarity or experience with SQL, Python or other relational database programming language\" },\n{ \"skill\": \"Terminal, R, or using a Unix command line interface\", \"reference\": \"Experience in the following: Terminal, R, or using a Unix command line interface\" },\n{ \"skill\": \"Managing projects, Familiarity with Catalist data, progressive politics, voter files, and\/or commercial data\", \"reference\": \"Preferred Skills & Abilities - Interest, familiarity or experience with SQL, Python or other relational database programming language; Experience in the following: Terminal, R, or using a Unix command line interface; Experience managing projects; Familiarity with Catalist data, progressive politics, voter files, and\/or commercial data\" }]"}
{"0":" [\n    { \"skill\": [\"ETL development\", \"Data warehouse\", \"2+ years of hands-on experience\"], \"reference\": \"7+ Years of Experience\" },\n    { \"skill\": [\"Informatica Mappings, Workflows, and processes\", \"2+ years of hands-on experience\"], \"reference\": \"2+ years of hands-on experience in ETL development for a data warehouse\" },\n    { \"skill\": [\"SQL programming\", \"Large data assets\", \"Proficient in programming against\"], \"reference\": \"Proficient in programming against large data assets with a working knowledge of SQL\" },\n    { \"skill\": [\"Integration technologies and processes\", \"Expertise\"], \"reference\": \"Expertise with integration technologies and processes\" },\n    { \"skill\": [\"Airflow, automated job scheduling tools\", \"Hands-on experience or Knowledge\"], \"reference\": \"Hands-on experience or Knowledge about Airflow, or automated job scheduling tools.\" },\n    { \"skill\": [\"API\/REST, JSON\", \"Experience\"], \"reference\": \"Experience with API\/REST, JSON\" },\n    { \"skill\": [\"Agile Development team\", \"Deliver features incrementally\"], \"reference\": \"Experience working on an Agile Development team and delivering features incrementally.\" },\n    { \"skill\": [\"Git repositories\", \"Experience\"], \"reference\": \"Experience with Git repositories\" },\n    { \"skill\": [\"Python development\", \"Pytest framework, or other testing frameworks\"], \"reference\": \"Experience developing in Python and Experience with Pytest framework, or other testing frameworks.\" },\n    { \"skill\": [\"Cloud platforms (AWS, Azure)\", \"Strong preference towards AWS\", \"Database design practices\", \"Building data automation pipelines\"], \"reference\": \"Experience with cloud platforms (AWS, Azure) with strong preference towards AWS. Experience in Database design practices. Experience building data automation pipelines.\" },\n    { \"skill\": [\"Azure DevOps, JIRA or similar project tracking software\", \"Experience\"], \"reference\": \"Experience with both Windows and Linux\" },\n]"}
{"0":" [{ \"skill\": \"Data Engineering experience\", \"reference\": \"3-7 years of Data Engineering experience with a focus on large data sets\" },\n { \"skill\": \"Cloud data platform expertise\", \"reference\": \"Experience with at least one cloud data platform (Snowflake, Redshift, BigQuery)\" },\n { \"skill\": \"Advanced SQL knowledge and Python experience\", \"reference\": \"Advanced SQL knowledge and Python experience is a plus\" },\n { \"skill\": \"Knowledge of Data Warehouse methodologies\", \"reference\": \"Knowledge of Data Warehouse methodologies and modeling\" },\n { \"skill\": \"Experience with source control tools\", \"reference\": \"Experience with source control tools such as Git, SVN, and TFS\" }]"}
{"0":" [{ \"skill\": \"SQL, Python programming\", \"reference\": \"2+ years of SQL experience at least 1 year of basic Python programming knowledge\" }, { \"skill\": \"Snowflake Data Engineer training and placement\", \"reference\": \"Become a Snowflake Data Engineer with this paid career program\" }, { \"skill\": \"Adaptability, flexibility, communication skills\", \"reference\": \"You'll join a fast-growing business where change takes place quickly; since the role is client-facing, strong communication and interpersonal skills are a must.\" }]"}
{"0":" [\n  { \"skill\": [\"AWS\", \"Bigdata\", \"Databricks\", \"Ab Initio\"], \"reference\": \"Mandatory Skills: AWS, Bigdata, Databricks, Ab Initio\" },\n  { \"skill\": [\"Healthcare Passion\", \"Enterprise Apps\", \"Environmental Constraints\"], \"reference\": \"Candidate Should Have A passion for healthcare and the potential for technology to improve people's lives. The aptitude to deliver enterprise or customer-facing applications in a variety of scenarios.\" },\n  { \"skill\": [\"Quality Software Development\", \"Innovation\", \"Influencing\"], \"reference\": \"Engineers can articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern to deliver value. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary.\" },\n  { \"skill\": [\"Cloud Certifications\", \"Hadoop Ecosystem\"], \"reference\": \"Experience in helping to distil complex ideas into an architectural and implementation plan, then building them out. The ability to critically evaluate the trade-offs and implementation impacts of emerging vs. more established technologies.\" },\n  { \"skill\": [\"Data Engineering\", \"Architecture & Implementation\"], \"reference\": \"Experience in helping to distil complex ideas into an architectural and implementation plan, then building them out. The ability to critically evaluate the trade-offs and implementation impacts of emerging vs. more established technologies.\" },\n  { \"skill\": [\"Big Data Tools\", \"Performance Measurement\"], \"reference\": \"Knowledge of Big Data tool performance\/tuning\/measurement and usage patterns to drive appropriate tool selections and use.\" }\n]"}
{"0":" [{ \"skill\": \"Azure Data Engineer\", \"reference\": \"As an Azure Data Engineer will work with clients, internal business stakeholders, and other data engineers to translate business requirements into modern and innovative data integration routines to move, cleanse, and transform large data sets in an efficient and scalable manner.\" },\n{ \"skill\": \"Data management\", \"reference\": \"Deep experience in developing robust, scalable, and resilient data management systems. Bachelor\u2019s degree from an accredited university or college and\/or demonstrated technical knowledge and equivalent work experience. 3+ years of experience developing queries and ELT\/ETL solutions using Microsoft tools.\" },\n{ \"skill\": \"Cloud solutions\", \"reference\": \"Experience with Azure services, 2+ years of experience in architecting, designing, developing, implementing, and optimizing cloud solutions leveraging Azure services. 1+ years of experience working with one or more languages commonly used for data operations including SQL, Python, Scala, and R.\" }]"}
{"0":" [{ \"skill\": \"Data Integration\", \"reference\": \"The Data Integration Engineer is primarily responsible for building and maintaining core data integration platforms.\" }, { \"skill\": \"Problem-solving and Critical Thinking\", \"reference\": \"The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills,\" }, { \"skill\": \"Learning New Methodologies\", \"reference\": \"display a passion for learning new methods and technologies.\" }]"}
{"0":" [{ \"skill\": [\"Python\", \"Data Engineer\", \"Lead\"], \"reference\": \"About the job\"},\n { \"skill\": [\"DWH\", \"PL\/SQL programming\"], \"reference\": \"Job Description\"},\n { \"skill\": [\"Optimization of stored Procedures\", \"Test Automation (using Python)\"], \"reference\": \"Job Description\"}]"}
