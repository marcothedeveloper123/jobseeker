[
    {
        "skill": "5+ years of network engineering",
        "reference": "HODL 5+ years of network engineering background including offensive/defensive security"
    },
    {
        "skill": "Deep experience in cloud infrastructure",
        "reference": "Experience with public (AWS or Azure or Google), private and/or hybrid cloud infrastructure"
    },
    {
        "skill": "Network security management tools",
        "reference": "Experience with network security management tools and techniques"
    },
    {
        "skill": "Security testing tools",
        "reference": "Familiarity with security testing tools (performance and threat-based)"
    },
    {
        "skill": "Network and security design",
        "reference": "Proficient in network and security design, implementation, and administration leveraging industry standard platforms from vendors such as: Palo Alto Networks, Cisco, Juniper, and Aruba"
    },
    {
        "skill": "Monitoring and management of intrusion detection systems",
        "reference": "Previous experience monitoring and management of intrusion detection systems and firewall devices"
    },
    {
        "skill": "Defensive security systems",
        "reference": "Demonstrated experience researching, building and implementing defensive security systems that are used against internal and external attack vectors"
    },
    {
        "skill": "Excellent communication skills",
        "reference": "Excellent communication skills: demonstrated ability to explain complex technical issues to both technical and non-technical audiences"
    },
    {
        "skill": "Analytical problem-solving skills",
        "reference": "Excellent analytical and problem-solving skills"
    },
    {
        "skill": "Performing well under pressure",
        "reference": "Ability to perform well under pressure, high attention to detail"
    },
    {
        "skill": "Strong desire/interest in infrastructure engineering and security",
        "reference": "A strong desire / interest in learning new technology and Highly motivated and passionate about infrastructure engineering and security"
    },
    {
        "skill": "Data pipeline development",
        "reference": "Key Responsibilities include: Data pipeline development"
    },
    {
        "skill": "Data modeling",
        "reference": "Key Responsibilities include: Data modeling"
    },
    {
        "skill": "Data Integration",
        "reference": "Key Responsibilities include: Data Integration"
    },
    {
        "skill": "Data Engineering, Database Development, Azure SQL Server",
        "reference": "The Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools."
    },
    {
        "skill": "ETL Processes, SSIS, Data Factory, SQL",
        "reference": "Experience coding the extraction, transformation, and loading (ETL) processes Database development primarily in SSIS, Data Factory and SQL"
    },
    {
        "skill": "BI Solutions, Azure Data Warehouse, Power BI/SSRS, Reporting",
        "reference": "Complete analysis, design and development of BI solutions using Azure SQL Server. Familiarity with Data Warehouse concepts."
    },
    {
        "skill": "Data Science Initiatives Support, Performance Tuning, Troubleshooting, Safety Culture",
        "reference": "Assist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives. Participate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation."
    },
    {
        "skill": "Modeling and Implementation",
        "reference": "Collaborate with other developers to create and implement best approach solution(s). Review queries for performance issues, making changes as needed."
    },
    {
        "skill": "Teamwork and Collaboration, Business Partnering",
        "reference": "Partner with the business to determine end user database/reporting needs and requirements. Collaborate with other developers to create and implement best approach solution(s). Troubleshoot Azure tools, systems, and software."
    },
    {
        "skill": "Data Engineering",
        "reference": "Data Engineer role will be technical liaison between multiple groups, build a data platform, and manage data pipeline architecture."
    },
    {
        "skill": "Data Science",
        "reference": "Work with data science and analytics teams to strive for greater functionality in our data systems."
    },
    {
        "skill": "Cloud Services Experience",
        "reference": "Experience with AWS cloud services and other cloud service providers such as Snowflake, Airflow."
    },
    {
        "skill": "Java, Software Programming, Self-learner",
        "reference": "Highly motivated, self-learner, and technically inquisitive; Experience in programming language Java and understanding of the software development life cycle"
    },
    {
        "skill": "Statistics, Python, Data Visualization Tools",
        "reference": "Experience in programming language Java and understanding of the software development life cycle; Knowledge of Statistics, Python, data visualization tools"
    },
    {
        "skill": "NLP, Text Mining, Tableau, Time Series Analysis",
        "reference": "Preferred skills: NLP, Text mining, Tableau, Time series analysis"
    },
    {
        "skill": "ETL pipelines",
        "reference": "Design, build and maintain robust scalable ETL pipelines"
    },
    {
        "skill": "Data platform",
        "reference": "Contribute to making our data platform world-class"
    },
    {
        "skill": "Quality control",
        "reference": "Ensure data integrity and quality"
    },
    {
        "skill": "Scala, ZIO, Functional",
        "reference": "Expert using Scala within ZIO"
    },
    {
        "skill": "Greenfield development, Streaming platform, Global rollout",
        "reference": "Working on a greenfield development project of the client's primary customer facing streaming platform which will replace their existing platform and be rolled out globally."
    },
    {
        "skill": "Front-end Software Engineering",
        "reference": "Experienced Staff Front-end Software Engineer with React, TypeScript, and related technologies expertise"
    },
    {
        "skill": "Collaboration & Teamwork",
        "reference": "We believe that accomplishing our ambitious goals cannot be done by lone heroes. Lasting change requires work by synergistic teams"
    },
    {
        "skill": "Agile Development",
        "reference": "Experience in an Agile environment, Experience working in a DevOps environment (we are on AWS), Experience in a CI/CD environment"
    },
    {
        "skill": "R or Python",
        "reference": "Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python."
    },
    {
        "skill": "Data analysis and modeling",
        "reference": "Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python."
    },
    {
        "skill": "Statistical programs",
        "reference": "Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python."
    },
    {
        "skill": "Familiarity",
        "reference": "Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "Database applications",
        "reference": "Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "SQL, Javascript, XML, JSON, HTML",
        "reference": "Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "Basic understanding",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "SQL, Javascript, XML, JSON, HTML",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "Data Engineering",
        "reference": "Contribute to the design, development, and maintenance of data pipelines"
    },
    {
        "skill": "ETL (Extract, Transform, Load) Data",
        "reference": "Design, develop, execute data integrity standards, optimize performance"
    },
    {
        "skill": "Data Modelling and Analytics Reports",
        "reference": "Experience with cloud environment, data models"
    },
    {
        "skill": "Scalable Data Architecture",
        "reference": "Working on large-scale data model refactoring for better performance"
    },
    {
        "skill": "Data Engineering Best Practices",
        "reference": "Implementation of ETL, data integrity and monitoring"
    },
    {
        "skill": "Python, SQL and DBT proficiency",
        "reference": ""
    },
    {
        "skill": "Experience with Version Control Tools",
        "reference": "Proven track record in Agile methodologies"
    },
    {
        "skill": "Bachelor's Degree (Quantitative)",
        "reference": "Physics, Engineering, Computer Science or demonstrated equivalent quantitative experience"
    },
    {
        "skill": "Excellent Communication Skills",
        "reference": "Effectively collaborate with different teams within the data org"
    },
    {
        "skill": "Experience, Start up environment, Python",
        "reference": "Required Skills 3-6 years of experience (no more than 6 years) Start up environment experience Python"
    },
    {
        "skill": "SQL, DBT",
        "reference": "Start up environment experience Python, SQL, and DBT"
    },
    {
        "skill": "Java, Full Stack, Software Programming",
        "reference": "Required Skills for Java /Full Stack/Software Programmer"
    },
    {
        "skill": "Bachelor's or Master's in Computer Science",
        "reference": "Entry level candidates should be qualified with enough skills and have hands on project work at clients"
    },
    {
        "skill": "Technical Inquisitiveness, Excellent Communication",
        "reference": "Highly motivated, self-learner, technically inquisitive, excellent written and verbal communication skills"
    },
    {
        "skill": "Data Engineering",
        "reference": "About The Role"
    },
    {
        "skill": "Database and Data Pipelines",
        "reference": "What You'll Do"
    },
    {
        "skill": "ETL Technologies",
        "reference": "Experience with ETL technologies , such as Databricks, Fivetran , or dbt"
    },
    {
        "skill": "Data Engineer",
        "reference": "Anika Systems is seeking a passionate and talented Data Engineer."
    },
    {
        "skill": "Database Management",
        "reference": "Designs, develops, builds, analyzes, evaluates, and installs database management systems."
    },
    {
        "skill": "Data Architecture",
        "reference": "Responsibilities include designing, developing, building databases with respect to data architectures."
    },
    {
        "skill": "Data Backend Engineering",
        "reference": "As a Data Backend Engineer, you will build scalable data products and help design mission-critical data flow pipelines."
    },
    {
        "skill": "Data Modeling and Warehousing",
        "reference": "Contribute to growing data modeling and data warehousing engineering efforts in the team."
    },
    {
        "skill": "Effective Data Storage and Retrieval",
        "reference": "Design and develop efficient data storage, retrieval, and reporting models for various analytics."
    },
    {
        "skill": "Quantexa Data Engineering",
        "reference": "Role: Quantexa Data engineer"
    },
    {
        "skill": "Certification in Relevant Tools",
        "reference": "Reason they are asking for Certification"
    },
    {
        "skill": "Data Manipulation Skills",
        "reference": "1. End to End usage of Quantexa"
    },
    {
        "skill": "Data Engineering",
        "reference": "Design, build, support and maintain various data pipelines and processes with an end goal of providing data intelligence to the progressive community."
    },
    {
        "skill": "Problem-solving, creativity",
        "reference": "A highly motivated individual with excellent technical skills, a strong desire to learn new skills, and interest in progressive politics. Values creativity and problem-solving."
    },
    {
        "skill": "Data architecture designs, SQL databases",
        "reference": "Experience working with SQL databases, ETL processes, distributed computing systems (Hadoop ecosystem), data cleaning, database maintenance tasks."
    },
    {
        "skill": "Relational database management systems",
        "reference": "1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle"
    },
    {
        "skill": "Data solution lifecycle understanding",
        "reference": "Understanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing"
    },
    {
        "skill": "ETL development/reporting experience",
        "reference": "Some experience in ETL development or reporting is a plus"
    },
    {
        "skill": "Azure CI/CD, Data Lake, Azure SQL",
        "reference": "Resource needed for Data Engineer with experience in Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate DevOps pipeline."
    },
    {
        "skill": "Data Sources integration, Central Time Zone",
        "reference": "They need the Data Engineer to connect to all the Data Sources, primarily their Data Lake. Customer's interest in a resource in Central Time Zone for real-time collaboration."
    },
    {
        "skill": "Data Engineer",
        "reference": "Team Remotely Inc. is a staffing and recruitment agency offering remote positions for Data Engineers with 1 year of experience"
    },
    {
        "skill": "Remote Position",
        "reference": "Team Remotely Inc. is a staffing and recruitment agency offering remote positions for Data Engineers with 1 year of experience"
    },
    {
        "skill": "1 Year Experience",
        "reference": "Team Remotely Inc. is a staffing and recruitment agency offering remote positions for Data Engineers with 1 year of experience"
    },
    {
        "skill": "UI Design Changes",
        "reference": "Responsibilities include designing user interface changes, reviewing requirements, implementing UI components using React"
    },
    {
        "skill": "Web-Based DB Applications",
        "reference": "Responsibilities include designing user interface changes, reviewing requirements, implementing UI components using React"
    },
    {
        "skill": "React Concepts",
        "reference": "Responsibilities include designing user interface changes, reviewing requirements, implementing UI components using React"
    },
    {
        "skill": "JavaScript",
        "reference": "Writing application interface codes using JavaScript, developing front-end architecture and ensuring performance"
    },
    {
        "skill": "Front-end Architecture",
        "reference": "Writing application interface codes using JavaScript, developing front-end architecture and ensuring performance"
    },
    {
        "skill": "Performance Monitoring",
        "reference": "Writing application interface codes using JavaScript, developing front-end architecture and ensuring performance"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline"
    },
    {
        "skill": "Data Analysis",
        "reference": "Acquire, assemble, transform and analyze data"
    },
    {
        "skill": "Data Visualization",
        "reference": "Data visualization experience (e.g Tableau, Looker, etc.)"
    },
    {
        "skill": "e-commerce (retail) experience",
        "reference": "MUST HAVE: e-commerce(retail) experience, 5-7 years experience with Python, Experience working with Google BigQuery and SQL"
    },
    {
        "skill": "5-7 years experience with Python",
        "reference": "MUST HAVE: e-commerce(retail) experience, 5-7 years experience with Python, Experience working with Google BigQuery and SQL"
    },
    {
        "skill": "Experience working with Google BigQuery and SQL",
        "reference": "MUST HAVE: e-commerce(retail) experience, 5-7 years experience with Python, Experience working with Google BigQuery and SQL"
    },
    {
        "skill": "IT",
        "reference": "Desired Skills and Experience - IT"
    },
    {
        "skill": "Data Engineering",
        "reference": "Contribute to building, operating, and scaling data platforms"
    },
    {
        "skill": "Data Pipeline Building",
        "reference": "Experienced data pipeline builder"
    },
    {
        "skill": "Data Wrangling",
        "reference": "Optimize data systems from the ground up"
    },
    {
        "skill": "Data Engineering",
        "reference": "About Our Team And What We'll Build Together"
    },
    {
        "skill": "Analytics and BI Reporting",
        "reference": "Establish theSkimm's data warehousing strategy (ex. Kimball, Data Vault, etc.)"
    },
    {
        "skill": "Leadership and Collaboration",
        "reference": "Commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support; A focus on accuracy and detail as you build ensuring data pipelines produce clear, predictable results"
    },
    {
        "skill": "Deep understanding of business problems",
        "reference": "Understand deeply the business problem that we are trying to solve by our analytical solution"
    },
    {
        "skill": "Experience in handling data sources",
        "reference": "Discover the client's existing data sources, consult with IT teams, define technical architecture"
    },
    {
        "skill": "Data ingestion subsystem implementation",
        "reference": "Implement the data ingestion subsystem"
    },
    {
        "skill": "Data analysis pipeline development",
        "reference": "Implement the data analysis pipelines"
    },
    {
        "skill": "Results integration into business UIs",
        "reference": "Integrate the results into business UIs or pre-existing client software systems"
    },
    {
        "skill": "Relevant tertiary qualification",
        "reference": "Relevant tertiary qualification, preferably at Masters level or above"
    },
    {
        "skill": "Strong programming skills",
        "reference": "Strong programming skills"
    },
    {
        "skill": "GCP, Airflow and Spark experience",
        "reference": "Experience in GCP, Airflow and Spark"
    },
    {
        "skill": "Proficiency in Python and SQL",
        "reference": "Solid experience in Python and SQL"
    },
    {
        "skill": "Good problem-solving skills",
        "reference": "Good problem-solving skills"
    },
    {
        "skill": "Effective communication skills",
        "reference": "Good communication skills"
    },
    {
        "skill": "Java, Full Stack, Software Programmer",
        "reference": "Required Skills For Java /Full Stack/Software Programmer Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Project work on the skills Knowledge of Core Java, JavaScript, C++ or software programming Spring boot, Microservices, Docker, Jenkins and REST API's experience Excellent written and verbal communication skills"
    },
    {
        "skill": "Data Science/Machine Learning",
        "reference": "Required Skills For Data Science/Machine learning Positions Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorFlow"
    },
    {
        "skill": "Candidate Skills",
        "reference": "We assist in filing for STEM extension and also for H1b and Green card filing to Candidates We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients."
    },
    {
        "skill": "Microsoft",
        "reference": "The client is placing heavy emphasis on Microsoft and Azure Fabric experience"
    },
    {
        "skill": "Azure Fabric",
        "reference": "The client is placing heavy emphasis on Microsoft and Azure Fabric experience"
    },
    {
        "skill": "Information Management Engineer",
        "reference": "The client is placing heavy emphasis on Microsoft and Azure Fabric experience"
    },
    {
        "skill": "Data Migration",
        "reference": "Assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich)"
    },
    {
        "skill": "Client Data Lake",
        "reference": "Assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich)"
    },
    {
        "skill": "De-duplication",
        "reference": "Assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich)"
    },
    {
        "skill": "Clean and Enrich",
        "reference": "Assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich)"
    },
    {
        "skill": "Azure Fabric",
        "reference": "Focused on using a medallion data model for data architecture"
    },
    {
        "skill": "Data Architecture",
        "reference": "Focused on using a medallion data model for data architecture"
    },
    {
        "skill": "Medallion Data Model",
        "reference": "Focused on using a medallion data model for data architecture"
    },
    {
        "skill": "Tools",
        "reference": "Focused on using a medallion data model for data architecture"
    },
    {
        "skill": "Delta Parquet Files",
        "reference": "The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL"
    },
    {
        "skill": "Python",
        "reference": "The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL"
    },
    {
        "skill": "Spark",
        "reference": "The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL"
    },
    {
        "skill": "Notebooks",
        "reference": "The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL"
    },
    {
        "skill": "T-SQL",
        "reference": "The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL"
    },
    {
        "skill": "Fabric Lakehouse/Warehouse",
        "reference": "We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF"
    },
    {
        "skill": "Dataflows Gen2",
        "reference": "We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF"
    },
    {
        "skill": "Data Pipelines",
        "reference": "We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF"
    },
    {
        "skill": "Direct Lake",
        "reference": "We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF"
    },
    {
        "skill": "Data Collection and Integration",
        "reference": "Gather, clean, and integrate data from diverse sources."
    },
    {
        "skill": "Data Analysis",
        "reference": "Perform statistical analysis to derive meaningful insights."
    },
    {
        "skill": "Predictive Modeling",
        "reference": "Develop predictive models for forecasting manufacturing outcomes."
    },
    {
        "skill": "Data Engineering",
        "reference": "About the job"
    },
    {
        "skill": "Data Visualization",
        "reference": "Responsibilities"
    },
    {
        "skill": "Maintaining Technical Foundation",
        "reference": "Responsibilities"
    },
    {
        "skill": "Python, SQL, AWS",
        "reference": "Main Responsibilities: Build the data pipelines that power our business"
    },
    {
        "skill": "Data Pipelines, Fast-Paced Environments",
        "reference": "2+ years of experience building data pipelines in a fast-paced environment"
    },
    {
        "skill": "Big Data Technologies, Snowflake, Terraform",
        "reference": "Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks"
    },
    {
        "skill": "Data Backend Engineering",
        "reference": "Responsible for building elegant and scalable data products, managing key user journey metrics, and designing mission-critical data flow pipelines."
    },
    {
        "skill": "Data Modeling and Data Warehousing",
        "reference": "Building effective data models, efficient data storage, retrieval, and reporting, and working with cross-functional teams."
    },
    {
        "skill": "Scalable Data Systems",
        "reference": "Designing and creating high performing and scalable systems to handle terabytes of data."
    },
    {
        "skill": "Machine Learning, AI/ML expertise",
        "reference": "Design, implement and deploy Machine Learning solutions to solve complex problems"
    },
    {
        "skill": "Software Development lifecycle experience",
        "reference": "Thorough understanding of software development lifecycle"
    },
    {
        "skill": "Programming skills in relevant languages",
        "reference": "Strong programming skills in Python, Scala, Go, Rust or other languages"
    },
    {
        "skill": "Excellent communication and interpersonal skills",
        "reference": "Excellent written and verbal communication skills and interpersonal skills"
    },
    {
        "skill": "MLOps platforms knowledge",
        "reference": "Experience with MLOps platforms, such as Kubeflow or MLFlow"
    },
    {
        "skill": "ML frameworks proficiency",
        "reference": "Experience with ML frameworks, such as scikit-learn, Tensorflow, PyTorch"
    },
    {
        "skill": "Big Data tools experience",
        "reference": "Experience with Big Data tools \u2013 Spark, S3, Athena/Trino"
    },
    {
        "skill": "GenAI tools knowledge",
        "reference": "Experience with GenAI tools, such as Langchain, LlamaIndex, and open source Vector DBs"
    },
    {
        "skill": "Advanced degree in related field",
        "reference": "Advanced degree in Computer Science, Machine Learning or related field"
    },
    {
        "skill": "Minimum 5 years experience in AI/ML engineering",
        "reference": "A minimum of 5 years experience in AI/ML engineering, with a track record of handling increasingly complex projects"
    },
    {
        "skill": "Java, Full Stack, Software Programming",
        "reference": "Required Skills For Java /Full Stack/Software Programmer"
    },
    {
        "skill": "Data Science, Machine Learning",
        "reference": "Required Skills For Data Science/Machine learning Positions"
    },
    {
        "skill": "Data Engineering",
        "reference": "Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra."
    },
    {
        "skill": "Architecture Design",
        "reference": "Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra."
    },
    {
        "skill": "Strategy Implementation",
        "reference": "Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra."
    },
    {
        "skill": "Technical Challenges Resolution",
        "reference": "Engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration."
    },
    {
        "skill": "Team Coordination",
        "reference": "Engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration."
    },
    {
        "skill": "Collaboration",
        "reference": "Engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration."
    },
    {
        "skill": "Leadership",
        "reference": "Your leadership will extend to planning and prioritization, facilitating effective coordination between onsite and offshore teams, driving feature prioritization through collaboration with product managers."
    },
    {
        "skill": "Prioritization",
        "reference": "Your leadership will extend to planning and prioritization, facilitating effective coordination between onsite and offshore teams, driving feature prioritization through collaboration with product managers."
    },
    {
        "skill": "Planning",
        "reference": "Your leadership will extend to planning and prioritization, facilitating effective coordination between onsite and offshore teams, driving feature prioritization through collaboration with product managers."
    },
    {
        "skill": "Python",
        "reference": "Full stack data engineer, working with any tech stack and data. Python Must. Django, bash, Pandas."
    },
    {
        "skill": "Django",
        "reference": "Full stack data engineer, working with any tech stack and data. Python Must. Django, bash, Pandas."
    },
    {
        "skill": "Bash",
        "reference": "Full stack data engineer, working with any tech stack and data. Python Must. Django, bash, Pandas."
    },
    {
        "skill": "Pandas",
        "reference": "Full stack data engineer, working with any tech stack and data. Python Must. Django, bash, Pandas."
    },
    {
        "skill": "MongoDB",
        "reference": "Experience with SQL Server is required Experience with MongoDB, Teradata, coding capabilities is highly preferred"
    },
    {
        "skill": "Teradata",
        "reference": "Experience with SQL Server is required Experience with MongoDB, Teradata, coding capabilities is highly preferred"
    },
    {
        "skill": "SQL Server",
        "reference": "Experience with SQL Server is required Experience with MongoDB, Teradata, coding capabilities is highly preferred"
    },
    {
        "skill": "4+ years' experience",
        "reference": "Requirements"
    },
    {
        "skill": "Developing data centric apps",
        "reference": "Requirements"
    },
    {
        "skill": "Scala and Databricks",
        "reference": "Requirements"
    },
    {
        "skill": "Creating data transformation and validation logic",
        "reference": "Requirements"
    },
    {
        "skill": "Data formats (JSON, XML, Parquet)",
        "reference": "Requirements"
    },
    {
        "skill": "Data Modeling",
        "reference": "Requirements"
    },
    {
        "skill": "Data Lake file systems",
        "reference": "Requirements"
    },
    {
        "skill": "Advanced understanding of best practices",
        "reference": "Best Practices (Data Lake File Systems)"
    },
    {
        "skill": "Structuring and organizing Data Lake file systems for large volumes of data",
        "reference": "Best Practices (Data Lake File Systems)"
    },
    {
        "skill": "Shaping and transforming data into third normal form and dimensional models",
        "reference": "Best Practices (Data Lake File Systems)"
    },
    {
        "skill": "Data Science",
        "reference": "As a Data Science Engineer, your primary responsibility will be to build out exciting new AI-driven tools."
    },
    {
        "skill": "Engineering",
        "reference": "As a Data Science Engineer, your primary responsibility will be to build out exciting new AI-driven tools."
    },
    {
        "skill": "AI-driven Tools",
        "reference": "As a Data Science Engineer, your primary responsibility will be to build out exciting new AI-driven tools."
    },
    {
        "skill": "Machine Learning",
        "reference": "Your work will frequently be on the bleeding edge of AI, incorporating the latest innovations in LLMs, machine learning, and data science."
    },
    {
        "skill": "Latest Innovations",
        "reference": "Your work will frequently be on the bleeding edge of AI, incorporating the latest innovations in LLMs, machine learning, and data science."
    },
    {
        "skill": "Cutting Edge",
        "reference": "Your work will frequently be on the bleeding edge of AI, incorporating the latest innovations in LLMs, machine learning, and data science."
    },
    {
        "skill": "Prototype",
        "reference": "Responsibilities include Prototype and build cutting edge, proof-of-concept AI products, Collaborate with product managers, designers, researchers, and executives to create successful solutions, and Solve challenging cutting-edge problems using state-of-the-art techniques."
    },
    {
        "skill": "Collaboration",
        "reference": "Responsibilities include Prototype and build cutting edge, proof-of-concept AI products, Collaborate with product managers, designers, researchers, and executives to create successful solutions, and Solve challenging cutting-edge problems using state-of-the-art techniques."
    },
    {
        "skill": "Problem Solving",
        "reference": "Responsibilities include Prototype and build cutting edge, proof-of-concept AI products, Collaborate with product managers, designers, researchers, and executives to create successful solutions, and Solve challenging cutting-edge problems using state-of-the-art techniques."
    },
    {
        "skill": "Database Design",
        "reference": "Designs, models, documents, and guides the logical and conceptual relationship of data and database changes for complex applications."
    },
    {
        "skill": "Data Analysis",
        "reference": "Analyze needs and requirements of existing and proposed systems, analyzes client production issues and create SQL statements to fix the issue, analyze data to spot anomalies, trend and correlate similar data sets, Analyze large, complex datasets to drive actionable insights and recommendations."
    },
    {
        "skill": "SQL Expertise",
        "reference": "2-4 years of relevant experience with 2-4 years SQL (both DML and DDL) Experience, exposure to 1-2 relational Database Systems, 0-2 years building and maintaining an enterprise Data Warehouse."
    },
    {
        "skill": "Scripting Knowledge",
        "reference": "1-2 years with at least 1 scripting language (Javascript, Python, PHP, Bash, Powershell)"
    },
    {
        "skill": "Data Visualization",
        "reference": "2-4 years Data Visualization, Analysis, or reporting Experience"
    },
    {
        "skill": "Data Engineering, Python, Javascript, PHP, SQL, Statistical Analysis, Data Visualization, Communication",
        "reference": "Required Skills: Proven experience in data engineering, including data pipeline development, ETL processes, and data warehousing; Strong self-management and team leadership abilities; Proficiency in Python, Javascript, and/or PHP being ideal; Mid-to-advanced level SQL skills in any flavor; Sound statistical analysis skills for meaningful data interpretation; A knack for creating precise data visualizations; Strong communication skills; Working remotely with a cross-functional team and collaborating with other engineers"
    },
    {
        "skill": "BigQuery, Google Analytics, Self-directed Learning, Data Collection Policies & Regulatory Constraints, DevOps/CI Pipeline Management, Event-driven Frameworks",
        "reference": "Preferred Skills: Familiarity with BigQuery or similar data tools; Exposure to Google Analytics; Strong aptitude for self-directed learning and growth; Experience in auditing application-level security with product engineers; Proficiency in code validation using test automation; Hands-on experience in DevOps/CI pipeline management; Familiarity with event-driven frameworks like WordPress; Knowledge of data collection policies and regulatory constraints"
    },
    {
        "skill": "Transforming the Data Industry, Contribution & Impact, Leadership, Ownership, Shape Future of Data",
        "reference": "Why You Should Join Us: At uConnect, you will have the opportunity to contribute to our mission and make a meaningful impact; Lead and own data systems from discussions to delivery; Grow personally as an individual contributor; Shape the future of data at uConnect"
    },
    {
        "skill": "Senior Data Engineer, Azure Infrastructure",
        "reference": "About the job"
    },
    {
        "skill": "ETL Pipeline Design, Big Data Experience",
        "reference": "Essential Job Functions Drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines."
    },
    {
        "skill": "Azure Synapse Analytics Pipelines, Data Management",
        "reference": "Utilizing experience with Big Data, this position will drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines. Design, implement, and document data load processes from disparate data sources into Azure Synapse Pipelines."
    },
    {
        "skill": "Azure DevOps, GitHub Integration",
        "reference": "Work with Continuous Integration/Delivery using Azure DevOps and Github."
    },
    {
        "skill": "Data Engineering Skills, Scrum Team",
        "reference": "Self-organize as part of a small-size scrum team and apply data engineering skills."
    },
    {
        "skill": "MS SQL/T-SQL, Apache Spark (PySpark)",
        "reference": "Minimum three (3) years' experience with MS SQL/T-SQL; Minimum three (3) years' experience with Azure SQL; Minimum three (3) years' experience with Apache Spark (PySpark)."
    },
    {
        "skill": "Azure Data Factory, Azure Synapse",
        "reference": "Minimum of three (3) years' experience with Azure Data Factory or Azure Synapse building dynamic ETL pipelines."
    },
    {
        "skill": "Dynamic Spark Notebooks, Python",
        "reference": "Minimum of three (3) years' experience with Apache Spark (PySpark); Minimum three (3) years' experience building dynamic Spark notebooks in Azure Synapse Spark or Azure Databricks; Minimum three (3) years' experience with Python."
    },
    {
        "skill": "Public Cloud, Data Warehouse Architecture",
        "reference": "Experience working with parquet, json , delta, avro and csv files; In-depth understanding of data management (e.g. permissions, recovery, security and monitoring); Experience with Data Warehouse Architecture."
    },
    {
        "skill": "Analytic Skills, Problem Solving",
        "reference": "Strong analytic skills related to working with structured, semi-structured and unstructured datasets.; Excellent analytical and organization skills required; Ability to understand user requirements; Client service mindset; Excellent verbal and written communication skills; Excellent problem solving skills."
    },
    {
        "skill": "Agile Frameworks",
        "reference": "Familiarity with Agile frameworks a plus."
    },
    {
        "skill": "Database Design and Development",
        "reference": "Designs, models, documents, and guides the logical and conceptual relationship of data and database changes for complex applications."
    },
    {
        "skill": "Data Analysis and Problem Solving",
        "reference": "Analyzes needs and requirements of existing and proposed systems, and develops technical, structural, and organizational specifications. May also develop, implement, and maintain data systems to meet designs, models and specifications. This person will analyze client production issues and create SQL statements to fix the issue."
    },
    {
        "skill": "Agile/Scrum Process",
        "reference": "Participate in Agile/Scrum process to refine, prioritize, and build solutions to meet customer needs."
    },
    {
        "skill": "Data Engineering, Agile Environment",
        "reference": "Support one or more dynamic, long-term federal government enterprise big-data programs"
    },
    {
        "skill": "Data Analytics Solutions Development",
        "reference": "Work as a key member of a multi-disciplinary team in an agile environment"
    },
    {
        "skill": "Data Architecture Optimization",
        "reference": "Collaborate with client stakeholders and technical employees to optimize data collection, storage, and usage"
    },
    {
        "skill": "Data Engineering, Machine Learning",
        "reference": "Design, construct, install, test, and maintain highly scalable data management systems. Collaborate with our data science team to design and develop machine learning models and algorithms."
    },
    {
        "skill": "Machine Learning Frameworks, Algorithms",
        "reference": "Experience with data modeling, data warehousing, and building ETL pipelines. Knowledge of programming languages including Python and SQL. Familiarity with PostgreSQL, DBT, Machine Learning, or equivalent."
    },
    {
        "skill": "Data Modeling, Driving Business Results",
        "reference": "Proven ability to drive business results with data-based insights."
    },
    {
        "skill": "Data Engineering, Data Pipelines",
        "reference": "The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse"
    },
    {
        "skill": "Business Judgment, Prioritization",
        "reference": "S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment"
    },
    {
        "skill": "Collaboration, Partnership",
        "reference": "Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack"
    },
    {
        "skill": "Airflow, Python, Data Engineering",
        "reference": "Join our data team as Senior Data Engineer responsible for maintaining and improving Airflow pipelines."
    },
    {
        "skill": "SageMaker, Pipelines, ETL library",
        "reference": "Develop new features using Python and migrate existing Airflow pipelines to SageMaker utilizing our proprietary ETL library."
    },
    {
        "skill": "Data Transformation, Integration, Code Reviews",
        "reference": "Work closely with data scientists and analysts on new pipelines and data transformation logic while implementing reusable modules for Python."
    },
    {
        "skill": "Data Engineering fundamentals",
        "reference": "Understand the fundamentals of how to create a robust and efficient infrastructure that makes data accessible and ready for analysis"
    },
    {
        "skill": "Communication proficiency",
        "reference": "Look for the ability to communicate with professional proficiency in English, verbally and in writing"
    },
    {
        "skill": "Functional Skills & Knowledge",
        "reference": "Responsible for building the infrastructure to support storing and movement of data"
    },
    {
        "skill": "Designs and maintains cloud-based data pipelines",
        "reference": "Designs and maintains data pipelines, cloud data warehouse, and storage solutions"
    },
    {
        "skill": "Knowledge in ETL / ELT processes",
        "reference": "Has the working knowledge and skills to ensure best practices in data quality, manage ETL (Extract, Transform, Load) / ELT processes, and establish data architecture"
    },
    {
        "skill": "Smaller analytics tasks or product improvements",
        "reference": "A solid understanding of discipline basics, wherein you can competently execute on smaller analytics tasks or product improvements with the guidance of senior engineers and management."
    },
    {
        "skill": "Constructive feedback on peer work",
        "reference": "Provides constructive feedback on peer code or technical design document reviews"
    },
    {
        "skill": "Participates in career development",
        "reference": "With career development Participates in design and bug fixes under direct supervision. Writes, tests, and documents code as per BDC guidelines."
    },
    {
        "skill": "Confidently contribute to internal analytics projects",
        "reference": "Confidently contributes to internal analytics projects, eventually improving our in-house approach by regularly planning technical work with the team."
    },
    {
        "skill": "Cross-Team Collaboration",
        "reference": "Responsible for collaborating with peers and senior ICs in other functional departments"
    },
    {
        "skill": "Delivery of smaller projects or tasks within scope",
        "reference": "Ensure that smaller and/or lower complexity data engineering projects are delivered on time, within scope, and within budget"
    },
    {
        "skill": "Self-directed project delivery",
        "reference": "Capable of self-directed project delivery and can help the team to prioritize work that maximizes team impact"
    },
    {
        "skill": "Data Processing (e.g., Apache Spark, dbt)",
        "reference": "Programming languages (e.g. SQL, Python), Data Processing (e.g. Apache Spark, dbt), Cloud-based data warehouses (e.g., Snowflake, Google BigQuery)"
    },
    {
        "skill": "Domain Expertise",
        "reference": "You have a strong foundation of knowledge in domains in which you're working"
    },
    {
        "skill": "Organizational Accountability",
        "reference": "Manage your own time in consultation with others, effectively delivering individual tasks and assignments"
    },
    {
        "skill": "Curiosity & Versatility",
        "reference": "Generally comfortable with new contexts and roles and can point to multiple occasions where you've changed approach or tools quickly and efficiently in response to a situation"
    },
    {
        "skill": "Collaboration & Partnership",
        "reference": "Can reliably assist and support the facilitation of meetings and collaborative projects with team members and occasionally engage in discussions with external stakeholders, if needed"
    },
    {
        "skill": "Effective Communication",
        "reference": "Usually communicate effectively, clearly, concisely and in an audience-oriented way, actively listening to others to ensure understanding"
    },
    {
        "skill": "Proven experience as a Data Engineer",
        "reference": "Proven experience as a Data Engineer or related role, with a focus on designing and developing data pipelines"
    },
    {
        "skill": "Strong programming skills",
        "reference": "Strong programming skills in languages such as Python, Rust, Scala, or SQL."
    },
    {
        "skill": "Deep knowledge of data warehousing and ETL/ELT processes",
        "reference": "Deep knowledge of data warehousing and ETL/ELT processes."
    },
    {
        "skill": "Experience with cloud platforms",
        "reference": "Familiarity with cloud platforms such as AWS, Azure, or Google Cloud."
    },
    {
        "skill": "Familiarity with machine learning operations (MLOps)",
        "reference": "Familiarity with machine learning operations (MLOps) techniques and platforms is a plus but not required."
    },
    {
        "skill": "Strong analytical skills",
        "reference": "Strong analytical and problem-solving skills."
    },
    {
        "skill": "Excellent communication skills",
        "reference": "Effective Communication You usually communicate effectively, clearly, concisely and in an audience-oriented way, actively listening to others to ensure understanding."
    },
    {
        "skill": "data engineering",
        "reference": "In this role you will be responsible for building the data and reporting infrastructure to power our systems."
    },
    {
        "skill": "big data technologies",
        "reference": "You have experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS."
    },
    {
        "skill": "data warehousing, etl/elt",
        "reference": "Have experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran"
    },
    {
        "skill": "ETL processes, database systems, real-time streaming applications",
        "reference": "Write ETL processes to support the ingestion and normalization of various data formats from social media, news, and web sources; Design robust database systems and developing tools for query and analytic processing, especially for real-time streaming applications"
    },
    {
        "skill": "Performance analysis, optimization",
        "reference": "Conduct performance analysis and empirical studies to evaluate tradeoffs (e.g., cost vs. throughput/latency); Develop, manage, and own the database architecture for our real-time streaming cloud-hosted analytics platform; Lead build automation, continuous integration, deployment efforts while ensuring compliance with security requirements"
    },
    {
        "skill": "Data pipelines, query optimization, NoSQL",
        "reference": "Expertise in databases and query optimization, including PostgresSQL, ElasticSearch, MongoDB, Redis, and Druid, with additional experience in NoSQL and graph databases being advantageous; Experience in horizontally scaling databases"
    },
    {
        "skill": "Scalable, Fault-tolerant data pipelines",
        "reference": "Build scalable and fault-tolerant data pipelines in Google Cloud Platform using Apache Airflow"
    },
    {
        "skill": "Data analysis & transformation",
        "reference": "Inspect, analyze, and transform data using SQL based tools like BigQuery and dbt"
    },
    {
        "skill": "Web application development",
        "reference": "Design, implement, and test features on our web application; Build integrations with external services"
    },
    {
        "skill": "Python & SQL expertise",
        "reference": "3+ years of experience in Python and SQL"
    },
    {
        "skill": "Apache Airflow & AWS/GCP familiarity",
        "reference": "You have experience with Apache Airflow and familiarity with AWS or GCP"
    },
    {
        "skill": "Data infrastructure, ETL design, data warehousing",
        "reference": "4+ years experience with data infrastructure, ETL design, data warehousing, schema design and dimensional data modeling"
    },
    {
        "skill": "SQL, Python or similar languages",
        "reference": "Experience with SQL, Python, or similar languages"
    },
    {
        "skill": "Designing real-time pipelines, code management tooling",
        "reference": "Experience with designing and implementing real-time pipelines and Experience with code management tooling such as Git, Github"
    },
    {
        "skill": "Data Engineering, Data Management, Analytics",
        "reference": "As a Data Engineer, you will work hands-on with challenging data engineering, data management, and analytics projects."
    },
    {
        "skill": "SQL, Snowflake, Databricks, Spark SQL, PySpark, Python",
        "reference": "Must-Haves: Minimum of 6 years of experience as a data engineer, Strong SQL skills in multiple database platforms."
    },
    {
        "skill": "Cloud Experience (Azure, AWS, or GCP), ETL Pipelines, Database Design",
        "reference": "Cloud experience: Azure, AWS, or GCP, Develop and maintain ETL pipelines, Database design and principles, Data modeling, schema development, and data-centric documentation."
    },
    {
        "skill": "Data Ingestion, Integration, Visualization, Code Performance Optimization",
        "reference": "Experience integrating data from a variety of data source types, Recommend and advise on optimal data models for data ingestion, integration, and visualization, Experience improving code performance and query optimization."
    },
    {
        "skill": "CI/CD, Problem-Solving, Communication",
        "reference": "Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environment, Outstanding problem-solving skills, Excellent verbal and written communication skills."
    },
    {
        "skill": "Java, Software Programmers",
        "reference": "For Java /Software Programmers"
    },
    {
        "skill": "Data Science/Machine Learning",
        "reference": "For data Science/Machine learning Required Skills"
    },
    {
        "skill": "Full Stack/Software Programmer",
        "reference": "Required SKILLS For Java /Full Stack/Software Programmer"
    },
    {
        "skill": "Data Modeling, Analyzing Needs and Requirements",
        "reference": "Designs, models, documents, and guides the logical and conceptual relationship of data and database changes for complex applications. Analyzes needs and requirements of existing and proposed systems, and develops technical, structural, and organizational specifications."
    },
    {
        "skill": "Development, Implementation, and Maintenance of Data Systems",
        "reference": "May also develop, implement, and maintain data systems to meet designs, models and specifications. This person will analyze client production issues and create SQL statements to fix the issue."
    },
    {
        "skill": "SQL Skills, Database Management Experience",
        "reference": "2-4 years of relevant experience, 2-4 years SQL (both DML and DDL), Exposure to 1-2 relational Database Systems, 2-4 years Data Visualization, Analysis, or reporting."
    },
    {
        "skill": "Data Engineering",
        "reference": "A Data Engineer will help develop best practices and provide consultative advice across a wide range of big data technologies."
    },
    {
        "skill": "ETL/ELT Processes",
        "reference": "Experience with designing and implementing ETL/ELT processes in support of cloud-based data analytics environments."
    },
    {
        "skill": "SQL, T-SQL & Cloud Experience",
        "reference": "5+ years of hands-on experience maintaining SQL databases, including expert level skills in SQL and T-SQL. 5+ years of hands-on experience working in cloud-based environments with distributed SQL pools."
    },
    {
        "skill": "Data Engineering",
        "reference": "The Data Engineer will focus on building data feeds and tooling from the application platform, ingestion into a centralized Data Lake/Data Warehouse."
    },
    {
        "skill": "Data Pipeline Design & Development",
        "reference": "Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse."
    },
    {
        "skill": "Software Best Practices",
        "reference": "Promote software best practices, standards, and processes"
    },
    {
        "skill": "Data Engineering",
        "reference": "As an Associate Data Engineer, you'll play a pivotal role in crafting innovative data engineering solutions."
    },
    {
        "skill": "Business Intelligence",
        "reference": "Position Overview"
    },
    {
        "skill": "Data Migration and Transformation",
        "reference": "Identify and resolve quality issues during complex data migration and transformation."
    },
    {
        "skill": "Good communicator",
        "reference": "Key Skill(s)  Good communicator, both written and verbal Well organized Attention to detail Team Player"
    },
    {
        "skill": "Team Player",
        "reference": "Key Skill(s)  Good communicator, both written and verbal Well organized Attention to detail Team Player"
    },
    {
        "skill": "Attention to detail",
        "reference": "Key Skill(s)  Good communicator, both written and verbal Well organized Attention to detail Team Player"
    },
    {
        "skill": "PowerBI",
        "reference": "Key Technology /Certification(s)  PowerBI, SQL, Databricks"
    },
    {
        "skill": "SQL",
        "reference": "Key Technology /Certification(s)  PowerBI, SQL, Databricks"
    },
    {
        "skill": "Databricks",
        "reference": "Key Technology /Certification(s)  PowerBI, SQL, Databricks"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Data Engineer, you'll provide your talents in contributing to the success of the client's team by delivering various tasks."
    },
    {
        "skill": "ETL Development",
        "reference": "Requires experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems."
    },
    {
        "skill": "Problem-Solving & Communication",
        "reference": "Excellent analytical, organizational, and problem-solving skills. Ability to communicate both verbally and in writing with technical and non-technical staff."
    },
    {
        "skill": "Java, Full Stack, Software Programmer",
        "reference": "Required Skills For Java /Full Stack/Software Programmer..."
    },
    {
        "skill": "Data Science, Machine Learning",
        "reference": "Required Skills For Data Science/Machine learning Positions..."
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "Responsibilities: Data Pipeline Development & Management, MS Power BI Development & Management."
    },
    {
        "skill": "MS Power BI Development",
        "reference": "Responsibilities: Data Pipeline Development & Management, MS Power BI Development & Management."
    },
    {
        "skill": "Data Analysis & Optimization",
        "reference": "Data Analysis & Optimization, Team Collaboration & Leadership."
    },
    {
        "skill": "Team Collaboration & Leadership",
        "reference": "Data Analysis & Optimization, Team Collaboration & Leadership."
    },
    {
        "skill": "Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Remote Location",
        "reference": "About the job"
    },
    {
        "skill": "Urgent Need",
        "reference": "About the job"
    },
    {
        "skill": "4+ years' experience",
        "reference": "Requirements"
    },
    {
        "skill": "data centric applications",
        "reference": "Requirements"
    },
    {
        "skill": "Scala, DataBricks",
        "reference": "Requirements"
    },
    {
        "skill": "4+ years' experience",
        "reference": ""
    },
    {
        "skill": "Data Transformation",
        "reference": ""
    },
    {
        "skill": "Validation Logic",
        "reference": ""
    },
    {
        "skill": "Streaming and Batch Data Processing",
        "reference": ""
    },
    {
        "skill": "Advanced knowledge of data formats",
        "reference": ""
    },
    {
        "skill": "Dimensional Models",
        "reference": ""
    },
    {
        "skill": "Advanced understanding",
        "reference": ""
    },
    {
        "skill": "Organizing Data Lake file systems",
        "reference": ""
    },
    {
        "skill": "Large volumes of data",
        "reference": ""
    },
    {
        "skill": "Data Engineer",
        "reference": "Join Us as a Data Engineer - Shape the Future of Healthcare Analytics with HANDLE Global!"
    },
    {
        "skill": "Shaping Future of Healthcare Analytics",
        "reference": "Join Us as a Data Engineer - Shape the Future of Healthcare Analytics with HANDLE Global!"
    },
    {
        "skill": "SQL",
        "reference": "Essentials: Bachelor\u2019s degree in Computer Science, IT, or equivalent combination of education, training, and experience. 5+ years of experience with SQL. and 3+ years with Python / Spark"
    },
    {
        "skill": "5+ Years",
        "reference": "Essentials: Bachelor\u2019s degree in Computer Science, IT, or equivalent combination of education, training, and experience. 5+ years of experience with SQL. and 3+ years with Python / Spark"
    },
    {
        "skill": "Python / Spark",
        "reference": "Essentials: Bachelor\u2019s degree in Computer Science, IT, or equivalent combination of education, training, and experience. 5+ years of experience with SQL. and 3+ years with Python / Spark"
    },
    {
        "skill": "3+ Years",
        "reference": "Essentials: Bachelor\u2019s degree in Computer Science, IT, or equivalent combination of education, training, and experience. 5+ years of experience with SQL. and 3+ years with Python / Spark"
    },
    {
        "skill": "Agile",
        "reference": "Knowledge in constructing medallion (bronze, silver, gold) style pipelines. Expertise in query optimization"
    },
    {
        "skill": "Startup Experience",
        "reference": "Knowledge in constructing medallion (bronze, silver, gold) style pipelines. Expertise in query optimization"
    },
    {
        "skill": "Data Engineering, Data Pipelines, SQL, Unix/Batch Scripting",
        "reference": "The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse."
    },
    {
        "skill": "Application Development, Data Warehousing, Business Judgment, Prioritization",
        "reference": "The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment."
    },
    {
        "skill": "Data Architecture, Feeds, Ingestion, Data as a Service",
        "reference": "Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack. Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse."
    },
    {
        "skill": "ETL",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "SQL",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "NoSQL",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Hadoop",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Spark",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Kafka",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Apache Nifi",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Talend",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Informatica",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "AWS",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Azure",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "GCP",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Python",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Pandas",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "R",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Matplotlib",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Seborn",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Tableau",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "TensorFlow",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "PyTorch",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "AWS Redshift",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Azure Synapse",
        "reference": "Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse."
    },
    {
        "skill": "Work Authorization",
        "reference": "Must be located and authorized to work in the United States without any work restrictions, now or in the future."
    },
    {
        "skill": "US Located",
        "reference": "Must be located and authorized to work in the United States without any work restrictions, now or in the future."
    },
    {
        "skill": "Deliver Results",
        "reference": "Must have a solid work track record of delivering results; Must have excellent communication skills"
    },
    {
        "skill": "Excellent Communication",
        "reference": "Must have a solid work track record of delivering results; Must have excellent communication skills"
    },
    {
        "skill": "Solid Work Track Record",
        "reference": "Must have a solid work track record of delivering results; Must have excellent communication skills"
    },
    {
        "skill": "Data Engineering",
        "reference": "Build data feeds and tooling from application platform, enable rapid ingestion into the Data Lake/Data Warehouse"
    },
    {
        "skill": "Application Development",
        "reference": "Understanding of application development, working across business areas"
    },
    {
        "skill": "Data Warehousing",
        "reference": "Design and build data feeds from the product tech stack for rapid ingestion into Data Lake/Data Warehouse"
    },
    {
        "skill": "Job seeking",
        "reference": "We are inviting professionals in high-growth industries to join our expanding talent pool."
    },
    {
        "skill": "Work style assessment",
        "reference": "You will be asked to complete the F4S work style assessment measuring attitudes and motivations in the context of work."
    },
    {
        "skill": "Predictive analytics",
        "reference": "Backed by 20+ years of research, F4S's revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance."
    },
    {
        "skill": "Data Engineering, Analytics, SQL, Data Modeling, Database",
        "reference": "Focus on maintaining data architectures, transforming data into actionable insights with deep analytical expertise."
    },
    {
        "skill": "Cloud Platforms, GCP, Snowflake, Data Management Best Practices, Remote Teamwork",
        "reference": "Experience with Cloud Platforms and Snowflake, familiarity with software development and project management frameworks."
    },
    {
        "skill": "Data Architectures, Analytical Requirements, Scalable Analytics, Machine Learning Use Cases, Data Quality Monitoring",
        "reference": "Key responsibilities include designing systems that collect, transform, store data, collaborating with stakeholders for solutions and ensuring architecture supports analytics."
    },
    {
        "skill": "High-growth industries",
        "reference": "We are inviting professionals in high-growth industries"
    },
    {
        "skill": "Talent pool expansion",
        "reference": "A pilot project designed to help job seekers get discovered by our partners"
    },
    {
        "skill": "F4S Talent Pool process",
        "reference": "Once you express your interest, you will be asked to complete the F4S work style assessment"
    },
    {
        "skill": "High-growth industries",
        "reference": "We are inviting professionals in high-growth industries"
    },
    {
        "skill": "Career development support",
        "reference": "Provide optional support and resources for job seekers"
    },
    {
        "skill": "Team motivation understanding",
        "reference": "Backed by 20+ years of research, F4S\u2019s revolutionary predictive analytics"
    },
    {
        "skill": "Data Engineering, Data Pipelines, Python, AWS",
        "reference": "Build and maintain data pipelines delivering core insights, collaborate with data scientists, ensure privacy and security."
    },
    {
        "skill": "Leadership, Technical Accuracy, Team Coordination",
        "reference": "Manage research projects, check team's work, coordinate with managers, and work on orchestration tools."
    },
    {
        "skill": "Database Warehouse, Snowflake, Healthcare Experience",
        "reference": "Familiarity with database warehouses, especially Snowflake, experience in healthcare or insurance."
    },
    {
        "skill": "Data Engineering Experience",
        "reference": "8+ years of data engineering experience with a proven record of integrating and standardizing data for use in a centralized database."
    },
    {
        "skill": "Cloud Solutions Expertise",
        "reference": "Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS."
    },
    {
        "skill": "SQL Proficiency",
        "reference": "Expert knowledge of SQL."
    },
    {
        "skill": "high-growth industries",
        "reference": "We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool."
    },
    {
        "skill": "professionals",
        "reference": "We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool."
    },
    {
        "skill": "job seekers",
        "reference": "We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool."
    },
    {
        "skill": "F4S Talent Pool",
        "reference": "The F4S Talent Pool is a pilot project designed to help job seekers get discovered by our partners based on their anticipated hiring needs."
    },
    {
        "skill": "pilot project",
        "reference": "The F4S Talent Pool is a pilot project designed to help job seekers get discovered by our partners based on their anticipated hiring needs."
    },
    {
        "skill": "F4S work style assessment",
        "reference": "Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work."
    },
    {
        "skill": "90% reliability",
        "reference": "Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work."
    },
    {
        "skill": "Extensive expertise with data infrastructure and engineering",
        "reference": "Develop and operate data infrastructure and pipelines to enable robust data for analytics and reporting in Marvel SNAP"
    },
    {
        "skill": "Experience in large-scale distributed data systems (Spark, Flink)",
        "reference": "Demonstrated experience in large-scale distributed data systems (Spark, Flink)"
    },
    {
        "skill": "Deep expertise in analytical database technologies (SQL and NoSQL)",
        "reference": "Extensive expertise with data infrastructure and data engineering"
    },
    {
        "skill": "Proficiency in SQL and Python",
        "reference": "Demonstrated experience in large-scale distributed data systems (Spark, Flink)"
    },
    {
        "skill": "Experience with database technologies and ETL/ELT",
        "reference": "Develop and operate data infrastructure and pipelines to enable robust data for analytics and reporting in Marvel SNAP"
    },
    {
        "skill": "Experience with orchestration and automation tools (Airflow, Beam)",
        "reference": "Partner with data scientists, analysts, and software engineers to ensure high-quality and relevant data collection"
    },
    {
        "skill": "Experience with marketing platforms and data tools (Braze, AppsFlyer)",
        "reference": "Enable SNAP Marketing team with high-quality data to improve user acquisition (UA)"
    },
    {
        "skill": "Collaborative cross-functional work environment",
        "reference": "Demonstrated success in a highly collaborative cross-functional work environment"
    },
    {
        "skill": "Passionate player of mobile games",
        "reference": "Passionate player of mobile games"
    },
    {
        "skill": "Mindset for serving a diverse and global player base",
        "reference": "Nice to Have But Not Necessary: Experience working in online video games, preferably mobile free-to-play games"
    },
    {
        "skill": "Data Engineering, Python, SQL, Multi-Threading",
        "reference": "Hard Requirements: 3+ years of industry experience as a data engineer Highly proficient with Python and SQL, intuitive understanding of concurrency primitives, experience with Postgres, Snowflake or Elasticsearch, deploying ETL pipelines"
    },
    {
        "skill": "ETL Pipelines, Mission-critical Deployment",
        "reference": "Experience deploying and monitoring mission-critical ETL pipelines with large and heterogenous data sources, Apache Airflow experience"
    },
    {
        "skill": "Financial Services Experience, AWS, Telemetry Tooling",
        "reference": "Bonus Requirements: Financial Services work experience, Typescript, stream processing knowledge, AWS or other cloud environment, Datadog and telemetry tooling"
    },
    {
        "skill": "ETL development, ETL architecture, real-time data streaming",
        "reference": "Develops data pipelines in both batch ETL and real-time streaming architectures."
    },
    {
        "skill": "Data modeling, data integration initiatives",
        "reference": "Develops data models to define new or modify existing data structures in support of data integration initiatives."
    },
    {
        "skill": "Technical knowledge for business projects, source system analysis, complex transformation assessment, target system exploration",
        "reference": "Provides expert technical knowledge of data solutions for business projects. Provides source system analysis, data discovery, complex transformation assessment and target system exploration to understand information data requirements and anticipate user needs."
    },
    {
        "skill": "Collaboration with other teams, Agile process adherence",
        "reference": "Contributes to data pipeline design, coding, and technical / functional reviews while collaborating with source system developers, data engineers and functional subject matter experts. Adheres to best practices for data movement, data quality, data profiling, data cleansing and other data pipeline related activities."
    },
    {
        "skill": "Data processing understanding, T-SQL proficiency, technical ETL specifications",
        "reference": "Able to interpret business needs and turn them into a technical plan of attack with pros and cons of various approaches to the data processing options. Demonstrates a solid understanding of technical standards and processes related to batch and real-time data pipeline development."
    },
    {
        "skill": "Teamwork, collaboration skills",
        "reference": "Expertise in SQL query transactions and optimization, especially T-SQL. Understand nulls, cardinality, joins, data types to develop technical ETL specifications and technical metadata. Ability to integrate an application solution into the broader business and IT ecosystem in which it will operate."
    },
    {
        "skill": "Data pipeline automation, quality assurance activities",
        "reference": "Firm understanding of cloud data processing and data streaming architectures, especially in AWS. Desire experience working with financial and/or claims data requiring compliance, balancing and integrity checks, especially payment-related data, PCI compliant data and banking industry formats such as NACHA."
    },
    {
        "skill": "Expert in Cosmos / Azure, Data Engineering / API Architectures",
        "reference": "Primary Skillset: Expert in Cosmos / Azure, recommend architecture changes to stay with changing system demands Secondary Skillset: Data Engineering / API Architectures"
    },
    {
        "skill": "Health Experience",
        "reference": "Industry Experience: Health Experience"
    },
    {
        "skill": "Leading Large-Scale Projects, Communication & Teamwork",
        "reference": "Your Impact Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our clients\u2019 business Translate clients requirements to system design and develop a solution that delivers business value Lead, design, develop and deliver large-scale data systems, data processing and data transformation projects"
    },
    {
        "skill": "Data Platforms & Database Experience",
        "reference": "Hands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud) Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)"
    },
    {
        "skill": "Data Pipeline & Integration",
        "reference": "Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc."
    },
    {
        "skill": "Data Modeling & Warehouse Design",
        "reference": "Ability to handle module or track level responsibilities and contributing to tasks \u201chands-on\u201d Experience in data modeling, warehouse design and fact/dimension implementations"
    },
    {
        "skill": "Code Repository Management & CI Integration",
        "reference": "Experience working with code repositories and continuous integration"
    },
    {
        "skill": "High-growth industries",
        "reference": "We are inviting professionals in high-growth industries."
    },
    {
        "skill": "Expanding talent pool",
        "reference": "The F4S Talent Pool is a pilot project designed to help job seekers get discovered by our partners based on their anticipated hiring needs."
    },
    {
        "skill": "F4S work style assessment",
        "reference": "Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work."
    },
    {
        "skill": "Predictive analytics",
        "reference": "Backed by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance."
    },
    {
        "skill": "Job seeker support",
        "reference": "Provide optional support and resources for job seekers in their career endeavors."
    },
    {
        "skill": "SSIS",
        "reference": "Requirements: Strong with SSIS and Exposure to Snowflake Platform"
    },
    {
        "skill": "Snowflake",
        "reference": "Requirements: Strong with SSIS and Exposure to Snowflake Platform"
    },
    {
        "skill": "Data Engineer",
        "reference": "Requirements: Strong with SSIS and Exposure to Snowflake Platform"
    },
    {
        "skill": "Prior Healthcare Experience",
        "reference": "Prior healthcare experience (Medicare, Medicaid)"
    },
    {
        "skill": "Medicare, Medicaid",
        "reference": "Prior healthcare experience (Medicare, Medicaid)"
    },
    {
        "skill": "Remote Worker",
        "reference": "The position is remote but the candidate has to live in one of the following states. Preferably closer to the west coast due to the PST schedule."
    },
    {
        "skill": "Preferred West Coast Schedule",
        "reference": "The position is remote but the candidate has to live in one of the following states. Preferably closer to the west coast due to the PST schedule."
    },
    {
        "skill": "Problem solving, SQL skills, Data warehousing",
        "reference": "Strong problem solving skills, Strong SQL skills, Expert in using data warehousing solutions"
    },
    {
        "skill": "Data engineering, Analytics, Business intelligence",
        "reference": "Data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions."
    },
    {
        "skill": "Programming, Data ingestion, Dashboards",
        "reference": "Strong programming skills in JavaScript and Python, Experience with data ingestion services, Experience with building business analytics and dashboards"
    },
    {
        "skill": "Snowflake and AWS experience",
        "reference": "3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)"
    },
    {
        "skill": "Data Security Measures Knowledge",
        "reference": "Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption"
    },
    {
        "skill": "Programming Skills and Data Optimization Techniques",
        "reference": "Strong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation"
    },
    {
        "skill": "SQL, Tableau, Data Analysis",
        "reference": "Role should have strong experience in writing SQL queries to extract data for reports."
    },
    {
        "skill": "AWS - Athena, AWS - S3 Glue, Redshift",
        "reference": "Must Haves include AWS - Athena, AWS - S3Glue, hands-on development of reporting applications using MS Excel and SQL"
    },
    {
        "skill": "Continuous Improvement, Agile Methodology, Problem Solving",
        "reference": "The Ethos of continuous improvement and interest in learning new things.Strong analytical thinking and structured problem-solving ability."
    },
    {
        "skill": "Data Engineering",
        "reference": "Responsible for designing, building, and maintaining data architecture"
    },
    {
        "skill": "Data Processing",
        "reference": "Design, implement and maintain scalable data pipelines and workflows that process large volumes of data efficiently and accurately"
    },
    {
        "skill": "Data Analytics",
        "reference": "Collaborate with data analysts and scientists to design and implement data models"
    },
    {
        "skill": "Snowflake and AWS experience",
        "reference": "3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)"
    },
    {
        "skill": "Data integration and security expertise",
        "reference": "Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts. Must have previous experience working with security datasets"
    },
    {
        "skill": "Strong programming skills in SQL, proficiency in Python/Scala",
        "reference": "Strong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing and manipulation"
    },
    {
        "skill": "Data Engineering",
        "reference": "Scale and contribute to a world-class data team. Lead infrastructure initiatives and contribute to analytics projects in a fast-paced, high-growth startup environment."
    },
    {
        "skill": "Data Infrastructure Management",
        "reference": "You will manage and build out our data platform and suite of data tools."
    },
    {
        "skill": "Data Product Development",
        "reference": "Own the development of data products end-to-end, including scoping, architecting, coding, testing, and rolling out insights across the company."
    },
    {
        "skill": "Data Engineering",
        "reference": "Design and develop data architecture, optimize data flow, and collection for cross-functional teams."
    },
    {
        "skill": "Data Pipeline Builder",
        "reference": "Experienced in data pipeline building and data wrangling."
    },
    {
        "skill": "Optimizing Data Systems",
        "reference": "Enjoy optimizing data systems and building them from the ground up."
    },
    {
        "skill": "Supporting Teams",
        "reference": "Support software developers, database architects, data analysts, and data scientists on data initiatives."
    },
    {
        "skill": "Self-Motivation",
        "reference": "Be self-motivated and comfortable supporting the data needs of multiple teams, systems, and products."
    },
    {
        "skill": "Data Architecture Optimization",
        "reference": "Excited by prospect of optimizing or even redesigning company\u2019s data architecture."
    },
    {
        "skill": "Principal Duties",
        "reference": "Work together with Director, Data Engineering and software development team to realize data architecture, etc."
    },
    {
        "skill": "Optimal Data Pipeline Architecture Creation",
        "reference": "Create and maintain optimal data pipeline architecture, assemble large complex data sets that meet functional/non-functional business requirements."
    },
    {
        "skill": "Internal Process Improvement",
        "reference": "Identify, design, implement internal process improvements like automating manual processes or optimizing data delivery."
    },
    {
        "skill": "Extract Transform Load Data",
        "reference": "Build the infrastructure required for optimal extraction, transformation and loading of data from various sources using SQL and AWS \u2018big data\u2019 technologies."
    },
    {
        "skill": "Data Tools Development",
        "reference": "Develop analytics tools to provide actionable insights."
    },
    {
        "skill": "Supporting Stakeholders",
        "reference": "Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues."
    },
    {
        "skill": "Data Separation and Security",
        "reference": "Keep our data separated and secure across national boundaries through multiple data centers and AWS regions."
    },
    {
        "skill": "Data Tools for Analytics Teams",
        "reference": "Create data tools for analytics and data scientist team members to assist them in building life sciences offerings."
    },
    {
        "skill": "Promote Data Best Practices",
        "reference": "Encourage a \u2018culture of data\u2019 across the organization."
    },
    {
        "skill": "Minimum Qualifications",
        "reference": "Strong SQL Server Database development, maintenance experience; working knowledge SSRS reporting."
    },
    {
        "skill": "Cloud Platform Experience",
        "reference": "Proficiency in at least one high-level programming language, e.g., Python or Java; data storage and retrieval experience on AWS highly preferred."
    },
    {
        "skill": "AWS Data Services",
        "reference": "Familiarity with AWS data services, data processing and analytics services."
    },
    {
        "skill": "Bonus Experience",
        "reference": "Experience in working with sensitive data and compliance regulations like CFR Part 11, HIPAA, GDPR; familiarity with clinical and pharma data; life sciences background preferred; experience in writing production-level SQL and working with various databases and reporting systems."
    },
    {
        "skill": "Experience",
        "reference": "5+ years overall experience, at least 3 in data management and/or data-centric software development. Life sciences background preferred."
    },
    {
        "skill": "Education Requirements",
        "reference": "BS/MS degree in Computer Science, Computer Engineering, or other technical discipline."
    },
    {
        "skill": "Special Requirements",
        "reference": "Remote role with possible travel; fully remote, though some travel may be required for this role."
    },
    {
        "skill": "Pyspark, Azure Databricks, Data Factory Experience",
        "reference": "Candidates need to have experience coding within Pyspark not just enabling the tool. They also need knowledge of Azure Cloud, Azure Data BrickETL, High volumes of data, PySpark and Python."
    },
    {
        "skill": "Data Engineer, Large-scale Projects",
        "reference": "Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data Factory. Work with other members of the project team to support delivery of additional project components."
    },
    {
        "skill": "ETL, DevOps Methodology",
        "reference": "Design and Implement Data Lakehouse. Work within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints."
    },
    {
        "skill": "Big data tools",
        "reference": "Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with data pipeline and workflow management tools"
    },
    {
        "skill": "Data pipeline management",
        "reference": "Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with data pipeline and workflow management tools"
    },
    {
        "skill": "Relational & NoSQL databases",
        "reference": "Experience with relational SQL and NoSQL databases, including Oracle, MS SQL Server, Postgres, Cassandra, etc.Experience with cloud-based data services such as AWS"
    },
    {
        "skill": "Cloud-based data services",
        "reference": "Experience with relational SQL and NoSQL databases, including Oracle, MS SQL Server, Postgres, Cassandra, etc.Experience with cloud-based data services such as AWS"
    },
    {
        "skill": "Data integration solutions",
        "reference": "Experience with data integration services solutions from vendors such as Informatica, MuleSoft, Talend, TIBCO, etc.Experience with stream-processing systems: Storm, Spark-Streaming, Kafka etc.Experience with object-oriented/object function scripting languages"
    },
    {
        "skill": "Stream processing systems",
        "reference": "Experience with data integration services solutions from vendors such as Informatica, MuleSoft, Talend, TIBCO, etc.Experience with stream-processing systems: Storm, Spark-Streaming, Kafka etc.Experience with object-oriented/object function scripting languages"
    },
    {
        "skill": "Object-oriented languages",
        "reference": "Experience with data integration services solutions from vendors such as Informatica, MuleSoft, Talend, TIBCO, etc.Experience with stream-processing systems: Storm, Spark-Streaming, Kafka etc.Experience with object-oriented/object function scripting languages"
    },
    {
        "skill": "AWS services, Big data, ETL development",
        "reference": "Working on AWS services - Glue, S3, Lambdas, EMR, Redshift, EC2, RDS, sagemaker, etc., experience with Oracle and Informatica, Experience working with big data - have all structured data and currently do data compaction using Hive. Need to figure out best approach for hive. Strong ETL development experience, hands-on architect level working with architects on implementation, Python as core language with pyspark processing."
    },
    {
        "skill": "Architect level involvement, Hands-on work",
        "reference": "Need to be more hands on than a traditional architect - this person would work with architects to help implement the work they are doing. Working with modeling and analytics users as their customers."
    },
    {
        "skill": "Java, Full stack, Software Programming",
        "reference": "For Java /Full stack/Software Programmer positions required skills include Bachelors degree or Masters in relevant fields, experience in programming languages, understanding of software development life cycle, project work, knowledge of core Java, JavaScript, C++, Spring Boot, Microservices, Docker, Jenkins, REST APIs and excellent communication skills"
    },
    {
        "skill": "Data Science/Machine Learning",
        "reference": "For Data Science/Machine learning Positions required skills include Bachelors or Masters in relevant fields, project work on the technologies needed, experience with programming languages like Java, understanding of software development life cycle, knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools and excellent communication skills. Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow"
    },
    {
        "skill": "Client matching",
        "reference": "We are looking for the right matching candidates for our clients. Please apply via job posting"
    },
    {
        "skill": "Pyspark, Azure Databricks, Data Factory Experience",
        "reference": "Top skills: Pyspark Azure Databricks and/or Data Factory Experience leading and managing offshore teams. ETL"
    },
    {
        "skill": "High volumes of data, ETL, Design and Develop High Performant Data Ingestion Pipelines",
        "reference": "Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data Factory"
    },
    {
        "skill": "DevOps, Agile Delivery, Security Standards Compliance",
        "reference": "Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints. Working with contractors to deliver products and services for the bottlers"
    },
    {
        "skill": "Data Analytics, Engineering, SQL, Snowflake, dbt, Looker",
        "reference": "About The Role Work collaboratively with data engineers and data scientists to drive business value through turning data models into actionable insights for business teams. This role will support data-based decision making by modeling data from the data warehouse and cloud storage."
    },
    {
        "skill": "Marketing, Sales, Revenue, Operations",
        "reference": "Discovery Education is looking for an experienced Data Analytics Engineer to join our team to support various areas of the business such as marketing analytics, sales and revenue and operations."
    },
    {
        "skill": "Data engineering, Visualization and Maintenance",
        "reference": "Interconnectivity between industrial automation data, MIS, and data warehouse. Helping influence and implement an end-to-end vision for how production field data will flow through the organization."
    },
    {
        "skill": "Complex projects, Cross functional teams, Timelines",
        "reference": "Experience in managing and communicating complex projects and collaborating with cross functional teams to accomplish project goals within expected timelines."
    },
    {
        "skill": "Data governance, Data Access Management, Change management",
        "reference": "Knowledgeable on data governance concepts and implementation, Data Access Management, and Change Management."
    },
    {
        "skill": "Data/Backend Engineering, Passionate, Problem-solving",
        "reference": "Ideally, you have..."
    },
    {
        "skill": "Pragmatic, Productive, Creative Development and Design",
        "reference": "A pragmatic, productive, and creative development and design philosophy"
    },
    {
        "skill": "Data-heavy Products, Scalable Solutions, Modern Dev Environment",
        "reference": "Expertise in building scalable solutions in a modern development environment"
    },
    {
        "skill": "Java, Full Stack, Software Programmer",
        "reference": "Required Skills For Java /Full Stack/Software Programmer"
    },
    {
        "skill": "Data Science, Machine Learning",
        "reference": "Required Skills For Data Science/Machine learning Positions"
    },
    {
        "skill": "Data Engineer",
        "reference": "Required Experience - Data Engineer with Enterprise Data warehouse/Datamart and ETL background."
    },
    {
        "skill": "Enterprise Data warehouse/Datamart",
        "reference": "Required Experience - Data Engineer with Enterprise Data warehouse/Datamart and ETL background."
    },
    {
        "skill": "Python",
        "reference": "Strong Python experience, Experience in data analysis and predictive modeling for supporting decision-making processes."
    },
    {
        "skill": "Data Analysis",
        "reference": "Strong Python experience, Experience in data analysis and predictive modeling for supporting decision-making processes."
    },
    {
        "skill": "Predictive Modeling",
        "reference": "Strong Python experience, Experience in data analysis and predictive modeling for supporting decision-making processes."
    },
    {
        "skill": "Azure Platform",
        "reference": "Experience with Azure platform including Azure Data Lake, Databricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL Database"
    },
    {
        "skill": "Databricks",
        "reference": "Experience with Azure platform including Azure Data Lake, Databricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL Database"
    },
    {
        "skill": "Delta Lake",
        "reference": "Experience with Azure platform including Azure Data Lake, Databricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL Database"
    },
    {
        "skill": "Property & Casualty Insurance Domain Knowledge",
        "reference": "Experience in Property & Casualty (P&C) Insurance domain is required."
    },
    {
        "skill": "Data Analysis",
        "reference": "Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc."
    },
    {
        "skill": "Policy Performance",
        "reference": "Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc."
    },
    {
        "skill": "Claims Data",
        "reference": "Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc."
    },
    {
        "skill": "SQL",
        "reference": "Strong SQL experience with excellent Excel skills (i.e. Pivot Tables)."
    },
    {
        "skill": "Excel (Pivot Tables)",
        "reference": "Strong SQL experience with excellent Excel skills (i.e. Pivot Tables)."
    },
    {
        "skill": "Data Modeling",
        "reference": "Understanding of Data modeling for Enterprise Data Warehouse ecosystems."
    },
    {
        "skill": "Enterprise Data Warehouse Ecosystems",
        "reference": "Understanding of Data modeling for Enterprise Data Warehouse ecosystems."
    },
    {
        "skill": "Spark",
        "reference": "Knowledge in Spark, PySpark preferable."
    },
    {
        "skill": "PySpark",
        "reference": "Knowledge in Spark, PySpark preferable."
    },
    {
        "skill": "Data Lake Environment",
        "reference": "Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues."
    },
    {
        "skill": "Streaming",
        "reference": "Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues."
    },
    {
        "skill": "Data Pipelines",
        "reference": "Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues."
    },
    {
        "skill": "File Formats",
        "reference": "Working knowledge on different file formats such as JSON, Parquet, CSV, etc."
    },
    {
        "skill": "JSON",
        "reference": "Working knowledge on different file formats such as JSON, Parquet, CSV, etc."
    },
    {
        "skill": "Parquet",
        "reference": "Working knowledge on different file formats such as JSON, Parquet, CSV, etc."
    },
    {
        "skill": "CSV",
        "reference": "Working knowledge on different file formats such as JSON, Parquet, CSV, etc."
    },
    {
        "skill": "Data Encryption",
        "reference": "Familiarity with data encryption, data masking."
    },
    {
        "skill": "Masking",
        "reference": "Familiarity with data encryption, data masking."
    },
    {
        "skill": "SQL Server Experience",
        "reference": "Database experience in SQL Server."
    },
    {
        "skill": "Pyspark",
        "reference": "Top skills: Pyspark, Azure Databricks and/or Data Factory Experience leading and managing offshore teams. ETL"
    },
    {
        "skill": "Azure Databricks",
        "reference": "Top skills: Pyspark, Azure Databricks and/or Data Factory Experience leading and managing offshore teams. ETL"
    },
    {
        "skill": "ETL",
        "reference": "Top skills: Pyspark, Azure Databricks and/or Data Factory Experience leading and managing offshore teams. ETL"
    },
    {
        "skill": "High volumes of data",
        "reference": "Responsibilities Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data FactoryWorking with event based streaming technologies to ingest and process data"
    },
    {
        "skill": "Python",
        "reference": "Responsibilities Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data FactoryWorking with event based streaming technologies to ingest and process data"
    },
    {
        "skill": "Agile delivery / DevOps methodology",
        "reference": "Responsibilities Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data FactoryWorking with event based streaming technologies to ingest and process data"
    },
    {
        "skill": "DevOps environment",
        "reference": "Experience in building ETL pipelines, Hands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics, Experience working with structured and unstructured data"
    },
    {
        "skill": "Relevant experience",
        "reference": "Experience in building ETL pipelines, Hands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics, Experience working with structured and unstructured data"
    },
    {
        "skill": "Innovation",
        "reference": "Experience in building ETL pipelines, Hands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics, Experience working with structured and unstructured data"
    },
    {
        "skill": "Data Engineering",
        "reference": "Seeking a mid-level Data Engineer to perform ETL and Data Modeling"
    },
    {
        "skill": "ETL/Data Mapping",
        "reference": "Conduct complex data mapping efforts between systems in a clear manner"
    },
    {
        "skill": "Data Analysis/Model Development",
        "reference": "Perform data analysis and develop the data models to support ETL development"
    },
    {
        "skill": "Complex Data Migrations",
        "reference": "Complete complex data migrations (ETL) processes to support development efforts"
    },
    {
        "skill": "Collaboration/Solution Design",
        "reference": "Collaborate with Software Architects, Software Developers, and BI Developers to design appropriate solutions for our internal team and external clients"
    },
    {
        "skill": "Mentorship",
        "reference": "Mentor software developers in data architecture and database design best practices"
    },
    {
        "skill": "Database Development Tasks",
        "reference": "Perform and assign database development tasks of medium complexity across multiple projects"
    },
    {
        "skill": "Third Party Integration",
        "reference": "Work directly with third party solutions to design, document, and develop data integrations"
    },
    {
        "skill": "Database Design/Implementation",
        "reference": "Create, contribute to, review and collaborate on data and database designs and implementations"
    },
    {
        "skill": "Data/Database Management",
        "reference": "Work with IT to deploy solutions; Innovate and contribute to improving development standards, techniques, tools, and processes"
    },
    {
        "skill": "Training & Support",
        "reference": "Support production systems as urgent and critical issues arise; including non-business hours support on a rotating basis. Participate in the development personnel interview process"
    },
    {
        "skill": "Agile Development Experience",
        "reference": "Experience with working in a hybrid agile development methodology"
    },
    {
        "skill": "Communication Skills",
        "reference": "Excellent verbal and written communication skills"
    },
    {
        "skill": "Employment Status",
        "reference": "The ability to work as a full-time employee without requiring visa sponsorship now or in the future is required."
    },
    {
        "skill": "W2Python",
        "reference": "Locals to United StatesOnly W2Python with pyspark data engineer"
    },
    {
        "skill": "pyspark",
        "reference": "Locals to United StatesOnly W2Python with pyspark data engineer"
    },
    {
        "skill": "data engineer",
        "reference": "Locals to United StatesOnly W2Python with pyspark data engineer"
    },
    {
        "skill": "GIS background",
        "reference": "with GIS background preferably"
    },
    {
        "skill": "preferably",
        "reference": "with GIS background preferably"
    },
    {
        "skill": "Data Engineer, Enterprise Data Warehouse/Datamart ETL, P&C Insurance domain",
        "reference": "Required Experience Data Engineer with Enterprise Data warehouse/Datamart and ETL background.Experience in Property & Casualty (P&C) Insurance domain is required."
    },
    {
        "skill": "Python, Azure Platform, Data Analysis, Predictive Modeling",
        "reference": "Strong Python experience, Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL Database, Experience in interpretation of insurance data for trend analysis."
    },
    {
        "skill": "Data Modelling, Spark/PySpark, Data Lake Environment",
        "reference": "Knowledge in Spark, PySpark preferable.Familiarity working in a data lake environment, leveraging data streaming and developing data pipelines driven by events/queues."
    },
    {
        "skill": "SQL, SQL Server, File Formats, Database Experience",
        "reference": "Strong SQL experience with excellent Excel skills (i.e. Pivot Tables), Understanding of Data modeling for Enterprise Data Warehouse ecosystems, Knowledge in Spark, PySpark preferable, Familiarity with data encryption, data masking, Database experience in SQL Server."
    },
    {
        "skill": "Bachelor's Degree in Information Science/Computer Science",
        "reference": "Qualifications: Bachelor's degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science."
    },
    {
        "skill": "SQL, Python programming",
        "reference": "2+ years of SQL experience at least 1 year of basic Python programming knowledge"
    },
    {
        "skill": "Cloud technology, ETL/ELT, data modeling",
        "reference": "Prior experience using cloud technology, ETL, ELT, and basic data modeling experience"
    },
    {
        "skill": "Snowflake Data Engineer training",
        "reference": "8 weeks of fully-funded comprehensive training to become a Snowflake Data Engineer"
    },
    {
        "skill": "Data Engineering",
        "reference": "Junior Data Engineer - US/Canada Only, 1 year of project experience"
    },
    {
        "skill": "Machine Learning and AI Solutions",
        "reference": "Enhance the intelligence of CVS Health infrastructure by detecting, predicting, and recommending solutions for issues"
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "Design, implement, and manage data pipelines"
    },
    {
        "skill": "Data Modeling",
        "reference": "Create and maintain data models to ensure quality, scalability, and efficiency"
    },
    {
        "skill": "Data Integration",
        "reference": "Integrate data from disparate sources for unified views of infrastructure platform and application data"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Utilize big data technologies such as Kafka to process large volumes of data efficiently"
    },
    {
        "skill": "Data Security Measures",
        "reference": "Implement measures for sensitive information protection and compliance with data and privacy regulations"
    },
    {
        "skill": "Performance Optimization",
        "reference": "Monitor and optimize data pipelines and systems for performance, scalability, and cost-effectiveness"
    },
    {
        "skill": "Team Player",
        "reference": "Willing to teach, share knowledge, and work with others for team success"
    },
    {
        "skill": "Communication",
        "reference": "Exceptional verbal, written, organizational, presentation, and communication skills"
    },
    {
        "skill": "Creativity",
        "reference": "Ability to come up with innovative ideas based on requirements"
    },
    {
        "skill": "Attention to Detail",
        "reference": "Systematically and accurately research future solutions and current problems"
    },
    {
        "skill": "Strong Work Ethic",
        "reference": "Innate drive to work extremely well"
    },
    {
        "skill": "Passion",
        "reference": "Drive to deliver better products and services than expected to customers"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Experience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub."
    },
    {
        "skill": "Cloud Platforms",
        "reference": "Experience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub."
    },
    {
        "skill": "GCP requirements",
        "reference": "Experience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub."
    },
    {
        "skill": "Data Engineering",
        "reference": "Experience managing ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases"
    },
    {
        "skill": "ETL management",
        "reference": "Experience managing ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases"
    },
    {
        "skill": "Programming",
        "reference": "Proficient in Python and SQL."
    },
    {
        "skill": "Data Analysis",
        "reference": "Proficient in Python and SQL."
    },
    {
        "skill": "Kafka",
        "reference": "Required (Individual Role) Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka"
    },
    {
        "skill": "Python",
        "reference": "Required (Individual Role) Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka"
    },
    {
        "skill": "Spark",
        "reference": "Required (Individual Role) Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka"
    },
    {
        "skill": "UNIX",
        "reference": "Required (Individual Role) Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka"
    },
    {
        "skill": "Hadoop platform",
        "reference": "Required (Individual Role) Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka"
    },
    {
        "skill": "Python programming",
        "reference": "Significant experience in Python programming Proficient in the data ingestion pipeline process"
    },
    {
        "skill": "Data ingestion pipeline process",
        "reference": "Significant experience in Python programming Proficient in the data ingestion pipeline process"
    },
    {
        "skill": "UNIX shell scripting",
        "reference": "Experience in UNIX shell scripting Demonstrated experience developing/expanding automated technology controls (e.g., Data Quality checks, Data Movement controls)"
    },
    {
        "skill": "Automated technology controls",
        "reference": "Experience in UNIX shell scripting Demonstrated experience developing/expanding automated technology controls (e.g., Data Quality checks, Data Movement controls)"
    },
    {
        "skill": "Azure Data Engineering, Azure Platform, Cloud Computing",
        "reference": "You will be working with all levels of technology from backend data processing technologies (Databricks/Apache Spark) to other Cloud computing technologies / Azure Data Platform."
    },
    {
        "skill": "Analytical Thinking, Detail-Oriented, Data Engineering & Application Development",
        "reference": "Must be a strong analytical thinker, detail-oriented and love working with data with a strong background in data engineering and application development."
    },
    {
        "skill": "Technology Passionate, New Technologies Learning, Advanced Analytics & ML",
        "reference": "Must be a hand-on technologist passionate about learning new technologies and help improve the ways we can better leverage Advanced Analytics and Machine Learning."
    },
    {
        "skill": "B2 English level (Upper-intermediate) or higher",
        "reference": "Qualifications B2 English level (Upper-intermediate) or higher 3+ years of experience in data integration and database development."
    },
    {
        "skill": "3+ years of experience in data integration and database development",
        "reference": "Qualifications B2 English level (Upper-intermediate) or higher 3+ years of experience in data integration and database development."
    },
    {
        "skill": "Proficiency in Core SQL and Azure SQL",
        "reference": "Proficiency in Core SQL and Azure SQL. Extensive experience with Azure Data Factory and Databricks. Strong expertise in designing and implementing complex stored procedures. Experience with SSAS."
    },
    {
        "skill": "Extensive experience with Azure Data Factory and Databricks",
        "reference": "Proficiency in Core SQL and Azure SQL. Extensive experience with Azure Data Factory and Databricks. Strong expertise in designing and implementing complex stored procedures. Experience with SSAS."
    },
    {
        "skill": "Strong expertise in designing and implementing complex stored procedures",
        "reference": "Proficiency in Core SQL and Azure SQL. Extensive experience with Azure Data Factory and Databricks. Strong expertise in designing and implementing complex stored procedures. Experience with SSAS."
    },
    {
        "skill": "Experience with SSAS",
        "reference": "Proficiency in Core SQL and Azure SQL. Extensive experience with Azure Data Factory and Databricks. Strong expertise in designing and implementing complex stored procedures. Experience with SSAS."
    },
    {
        "skill": "Excellent problem-solving skills",
        "reference": "Offering a full-time, fully committed position for the role. Excellent problem-solving skills and attention to detail are essential."
    },
    {
        "skill": "Attention to detail",
        "reference": "Offering a full-time, fully committed position for the role. Excellent problem-solving skills and attention to detail are essential."
    },
    {
        "skill": "data services",
        "reference": "Data Services Engineer (DSE) at Catalist"
    },
    {
        "skill": "project management",
        "reference": "Oversees all projects assigned from start to finish; successfully delivers reports, lists and other material requests on time, accurately and in a client-friendly format with actionable insights"
    },
    {
        "skill": "problem solving",
        "reference": "Willingness to be a problem solver and produce results in a fast-paced environment"
    },
    {
        "skill": "ETL development",
        "reference": "2+ years of hands-on experience in ETL development for a data warehouse"
    },
    {
        "skill": "Data warehouse",
        "reference": "2+ years of hands-on experience in ETL development for a data warehouse"
    },
    {
        "skill": "Informatica Mappings, Workflows, and processes",
        "reference": "2+ years of hands-on experience in development, maintenance, and enhancements of Informatica Mappings, Workflows, and processes"
    },
    {
        "skill": "SQL knowledge",
        "reference": "Proficient in programming against large data assets with a working knowledge of SQL"
    },
    {
        "skill": "Programming against large data assets",
        "reference": "Proficient in programming against large data assets with a working knowledge of SQL"
    },
    {
        "skill": "Integration technologies",
        "reference": "Expertise with integration technologies and processes"
    },
    {
        "skill": "Processes",
        "reference": "Expertise with integration technologies and processes"
    },
    {
        "skill": "Airflow",
        "reference": "Hands-on experience or Knowledge about Airflow, or automated job scheduling tools."
    },
    {
        "skill": "Automated job scheduling tools",
        "reference": "Hands-on experience or Knowledge about Airflow, or automated job scheduling tools."
    },
    {
        "skill": "API/REST, JSON",
        "reference": "Experience with API/REST, JSON"
    },
    {
        "skill": "Agile Development",
        "reference": "Experience working on an Agile Development team and delivering features incrementally."
    },
    {
        "skill": "Incremental delivery",
        "reference": "Experience working on an Agile Development team and delivering features incrementally."
    },
    {
        "skill": "Git repositories",
        "reference": "Experience with Git repositories"
    },
    {
        "skill": "Python programming",
        "reference": "Experience developing in Python, Experience working with Pytest framework, or other testing frameworks"
    },
    {
        "skill": "Pytest framework",
        "reference": "Experience developing in Python, Experience working with Pytest framework, or other testing frameworks"
    },
    {
        "skill": "AWS, Azure cloud platforms",
        "reference": "Experience with cloud platforms (AWS, Azure) with strong preference towards AWS."
    },
    {
        "skill": "Database design practices",
        "reference": "Experience in Database design practices"
    },
    {
        "skill": "Data automation pipelines",
        "reference": "Experience building data automation pipelines"
    },
    {
        "skill": "Azure DevOps, JIRA",
        "reference": "Experience with Azure DevOps, JIRA or similar project tracking software."
    },
    {
        "skill": "Project tracking software",
        "reference": "Experience with Azure DevOps, JIRA or similar project tracking software."
    },
    {
        "skill": "Windows, Linux",
        "reference": "Experience working with both Windows and Linux."
    },
    {
        "skill": "Data Engineering, Large Data Sets, Cloud Data Platforms (Snowflake, Redshift, BigQuery), Advanced SQL Knowledge, Python Experience, Data Warehouse Methodologies, Modeling, Source Control Tools, Remote-First Environment",
        "reference": "You have: 3-7 years of Data Engineering experience with a focus on large data sets; Experience with at least one cloud data platform (Snowflake, Redshift, BigQuery); Advanced SQL knowledge and Python experience is a plus; Knowledge of Data Warehouse methodologies and modeling; Experience with source control tools such as Git, SVN, and TFS."
    },
    {
        "skill": "Data Pipelines Development, Testing, Deployment, Documentation, Schema Design, Collaborate with Stakeholders, Code Reviews, End User Support for Reporting and Analytics, Task Estimation",
        "reference": "In this role, you will: Assist in the development, testing, and deployment of data pipelines using SQL and/or Python; Contribute to and maintain documentation of data flows and schema design; Collaborate with senior developers to troubleshoot issues and optimize performance; Collaborate with business stakeholders to understand needs and collect feedback on solutions; Participate in code reviews to uphold high-quality standards; Provide support to end users in building reporting and analytics; Assist with the estimation of development tasks; Be detail-oriented, a fast learner, and scrappy."
    },
    {
        "skill": "Enthusiasm for Sharing Best Practices, Flexibility, Detail-Oriented, Fast Learner, Scrappy",
        "reference": "Be enthusiastic about sharing and adopting best practices with your team; Be flexible--you understand the needs of a startup; You understand the needs of a startup; Be detail-oriented, a fast learner, and scrappy."
    },
    {
        "skill": "SQL, Python programming knowledge",
        "reference": "2+ years of SQL experience at least 1 year of basic Python programming knowledge"
    },
    {
        "skill": "Snowflake Data Engineer",
        "reference": "Become a Snowflake Data Engineer with this paid career program"
    },
    {
        "skill": "Communication, Interpersonal skills",
        "reference": "Since the role is client-facing, strong communication and interpersonal skills are a must."
    },
    {
        "skill": "AWS, Bigdata, Databricks, Ab Initio",
        "reference": "Mandatory Skills: AWS, Bigdata, Databricks, Ab Initio"
    },
    {
        "skill": "Passion for healthcare technology",
        "reference": "Candidate Should Have A passion for healthcare and the potential for technology to improve people's lives."
    },
    {
        "skill": "Deep understanding of environmental constraints",
        "reference": "A deep understanding of environmental constraints to implementing and deploying differing application solutions."
    },
    {
        "skill": "Azure Data Engineer",
        "reference": "As an Azure Data Engineer will work with clients, internal business stakeholders, and other data engineers to translate business requirements into modern and innovative data integration routines."
    },
    {
        "skill": "Data Management Systems Development",
        "reference": "Deep experience in developing robust, scalable, and resilient data management systems"
    },
    {
        "skill": "ETL/ELT Pipelines",
        "reference": "3+ years of experience developing queries and ELT/ETL solutions using Microsoft tools (SQL, ADF, ADLS, Azure Synapse), working with structured, unstructured, and semi-structured datasets"
    },
    {
        "skill": "Data Integration",
        "reference": "The Data Integration Engineer is responsible for building and maintaining core data integration platforms."
    },
    {
        "skill": "Problem-solving, Critical Thinking",
        "reference": "The successful candidate must have excellent problem-solving and critical thinking skills."
    },
    {
        "skill": "Passion for Learning",
        "reference": "Display a passion for learning new methods and technologies."
    },
    {
        "skill": "Data Engineer",
        "reference": "Job Role: Data Engineer Lead(with QE Exp.)Location: RemoteEmployment: Contract"
    },
    {
        "skill": "Remote",
        "reference": "Job Role: Data Engineer Lead(with QE Exp.)Location: RemoteEmployment: Contract"
    },
    {
        "skill": "Contract",
        "reference": "Job Role: Data Engineer Lead(with QE Exp.)Location: RemoteEmployment: Contract"
    },
    {
        "skill": "Python",
        "reference": "Looking for a Data Engineer with knowledge in Python programming, DWH, Strong in PL/SQL programming, Optimization of stored Procedures and knowledge in Test Automation (using Python)."
    },
    {
        "skill": "DWH",
        "reference": "Looking for a Data Engineer with knowledge in Python programming, DWH, Strong in PL/SQL programming, Optimization of stored Procedures and knowledge in Test Automation (using Python)."
    },
    {
        "skill": "PL/SQL programming",
        "reference": "Looking for a Data Engineer with knowledge in Python programming, DWH, Strong in PL/SQL programming, Optimization of stored Procedures and knowledge in Test Automation (using Python)."
    },
    {
        "skill": "Optimization of stored Procedures",
        "reference": "Looking for a Data Engineer with knowledge in Python programming, DWH, Strong in PL/SQL programming, Optimization of stored Procedures and knowledge in Test Automation (using Python)."
    },
    {
        "skill": "Test Automation (using Python)",
        "reference": "Looking for a Data Engineer with knowledge in Python programming, DWH, Strong in PL/SQL programming, Optimization of stored Procedures and knowledge in Test Automation (using Python)."
    },
    {
        "skill": "Data Manipulation, Data Organization, Process Improvement",
        "reference": "Must be well versed in data manipulation, data organization, and process improvement"
    },
    {
        "skill": "Tableau, Creating Metrics Reports, Dashboards",
        "reference": "Must have extensive knowledge of Tableau, creating metrics reports, and creating dashboards with proper data visualization"
    },
    {
        "skill": "Python, R, Spark, Power BI",
        "reference": "Must be proficient in Python, R, Spark and/or Power Bi"
    },
    {
        "skill": "Git, GitHub, GitLab, Code Repository",
        "reference": "Some experience using Git, GitHub and GitLab for code repository"
    },
    {
        "skill": "Hadoop, Live Tableau Dashboards",
        "reference": "Some knowledge of Hadoop and connecting/setting up live Tableau dashboards"
    },
    {
        "skill": "AI/ML, RPA, Process Automation",
        "reference": "Some knowledge of AI/Machine Learning, RPA and how to automate processes"
    },
    {
        "skill": "Agile Working Processes, Scrumban, Product Management",
        "reference": "Some knowledge of agile working processes, Scrumban and product management processes"
    },
    {
        "skill": "Contractor Responsibilities",
        "reference": ""
    },
    {
        "skill": "AI, analytics",
        "reference": "Solve toughest problems faced by organizations globally with AI and analytics."
    },
    {
        "skill": "Data Engineering",
        "reference": "Expanding Data Engineering practice."
    },
    {
        "skill": "Azure Data Engineer",
        "reference": "Join the growing team of analytics experts."
    },
    {
        "skill": "Analytical skills, data combining",
        "reference": "Strong analytical skills and ability to combine data from different sources."
    },
    {
        "skill": "Efficiency, business alignment",
        "reference": "Strive for efficiency by aligning data systems with business goals."
    },
    {
        "skill": "Data Manipulation (DBX, Spark, SQL)",
        "reference": "Top 3 Skills: Data Manipulation (DBX, Spark, SQL)"
    },
    {
        "skill": "Supply Chain and Cloud Operation",
        "reference": "About the job"
    },
    {
        "skill": "Analyze, design and develop enterprise data and information architecture",
        "reference": "Job Description"
    },
    {
        "skill": "Data Engineering",
        "reference": "Must have minimum of 3 years hands on experience as a data engineer or similar position."
    },
    {
        "skill": "Machine Learning, AI",
        "reference": "Combine machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing under a holistic vision to scale and transform businesses."
    },
    {
        "skill": "Cloud Infrastructure Experience",
        "reference": "Experience with Azure preferred"
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "Build the infrastructure required for optimal, automated extraction, transformation, and loading of data from a wide variety of data sources using SQL and other 'big data' technologies such as Databricks."
    },
    {
        "skill": "Advanced Analytics & Data Modeling",
        "reference": "Evaluate new technology for use within Enterra. Work with other Enterra and client personnel to administer and operate client-specific instances of the Enterra solution offerings"
    },
    {
        "skill": "Data Architecture",
        "reference": "Developing and maintaining data architectures, acquisition/contract management, information technology, analytics"
    },
    {
        "skill": "Knowledge Management",
        "reference": "Collaboration with product design and engineering teams for data analysis"
    },
    {
        "skill": "Technology Research",
        "reference": "Recommend tools and capabilities based on research of environment and new technology"
    },
    {
        "skill": "Data Engineer",
        "reference": "Proven work experience as an ETL or data engineer (3+ years of experience preferred)"
    },
    {
        "skill": "ETL Experience",
        "reference": "Proven work experience as an ETL or data engineer (3+ years of experience preferred)"
    },
    {
        "skill": "Analyze Data",
        "reference": "Proven experience being able to analyze data and data sources"
    },
    {
        "skill": "Pattern Recognition",
        "reference": "Proven experience being able to analyze data and data sources"
    },
    {
        "skill": "Troubleshooting",
        "reference": "Experience developing and maintaining code in the ETL processes, Using stored procedures, T-SQL, and SSIS, against a variety of data sources such as PostgreSQL, Microsoft SQL Server, flat files, etc."
    },
    {
        "skill": "Data Consistency",
        "reference": "Experience developing and maintaining code in the ETL processes, Using stored procedures, T-SQL, and SSIS, against a variety of data sources such as PostgreSQL, Microsoft SQL Server, flat files, etc."
    },
    {
        "skill": "SQL Experience",
        "reference": "2+ Years of experience with SQL Server, including creating tables, stored procedures, views, setting up jobs, and troubleshooting code and data issues"
    },
    {
        "skill": "ETL Processes",
        "reference": "2+ Years of experience with SQL Server, including creating tables, stored procedures, views, setting up jobs, and troubleshooting code and data issues"
    },
    {
        "skill": "SSIS Packages",
        "reference": "Working experience developing SSIS packages (1+ years of experience preferred), Working experience designing and developing data warehouse dimensional modeling"
    },
    {
        "skill": "Data Warehouse Dimensional Modeling",
        "reference": "Working experience developing SSIS packages (1+ years of experience preferred), Working experience designing and developing data warehouse dimensional modeling"
    },
    {
        "skill": "Business Rules Documentation",
        "reference": "Experience documenting business rules and cases to be applied to the ETL processes"
    },
    {
        "skill": "ETL Processes",
        "reference": "Experience documenting business rules and cases to be applied to the ETL processes"
    },
    {
        "skill": "Quality Automated Testing",
        "reference": "Implementing quality automated testing for data completion, accuracy, integrity, and consistency"
    },
    {
        "skill": "Data Accuracy, Integrity",
        "reference": "Implementing quality automated testing for data completion, accuracy, integrity, and consistency"
    },
    {
        "skill": "Insurance Experience",
        "reference": "Desired Skills\u2014Plusses: Insurance industry experience"
    },
    {
        "skill": "Data Engineering",
        "reference": "Staff Data Engineer at [Company], you will play a pivotal role in designing, building, and maintaining our data infrastructure."
    },
    {
        "skill": "Leadership",
        "reference": "Strong leadership mindset with a focus on accountability. Results-oriented and passionate about helping team members grow and improve."
    },
    {
        "skill": "Technical Proficiency",
        "reference": "Proven track record of implementing AWS cloud services, expertise in Python and SQL, experience with data orchestration tools, and knowledge of big data platforms."
    },
    {
        "skill": "Data Analysis, Modeling, Statistical Programs",
        "reference": "Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python."
    },
    {
        "skill": "SQL, Javascript, XML, JSON, HTML, Quick Learning",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML. Ability to learn new methods quickly and work under deadlines."
    },
    {
        "skill": "Teamwork, Communication, Analytical Skills",
        "reference": "Excellent teamwork and communication skills. Strong analytical and problem-solving abilities."
    },
    {
        "skill": "ETL development",
        "reference": "Minimum of 5 years of experience in ETL development, working with diverse and extensive data sources."
    },
    {
        "skill": "Programming languages",
        "reference": "Proficiency in programming languages like Java, Scala, Python, R, and familiarity with JSON Schema."
    },
    {
        "skill": "Data Warehouse/Business Intelligence experience",
        "reference": "Demonstrated experience in Data Warehouse/Data Lake and Business Intelligence environments."
    },
    {
        "skill": "Agile development practices",
        "reference": "Experience with Agile development practices, including Scrum and Kanban, as well as management tools such as Jira and Confluence."
    },
    {
        "skill": "AWS Cloud environment experience",
        "reference": "Proficiency in engineering/DevOps tools, such as Jenkins. Experience with AWS Database Migration Service (DMS), Databricks/Apache Spark, and/or Kafka."
    },
    {
        "skill": "Databricks, Python, Azure Development",
        "reference": "MUST HAVE: 3 years' experience with Databricks, Python and Azure Development (all 3)"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Databricks data engineer"
    },
    {
        "skill": "Azure Cloud Environment",
        "reference": "configuring the data lake (ADLS Gen2), create and optimizing data pipelines, and closely monitoring them to ensure data quality and scalability"
    },
    {
        "skill": "Python programming",
        "reference": "The ideal candidate will have a strong background in Python programming, Data Warehousing (DWH), proficient in PL/SQL programming, expertise in optimizing stored procedures, and knowledge in Test Automation using Python. As a Data Engineer Lead, you will play a pivotal role in designing, developing, and maintaining robust data pipelines."
    },
    {
        "skill": "Data Warehousing (DWH)",
        "reference": "The ideal candidate will have a strong background in Python programming, Data Warehousing (DWH), proficient in PL/SQL programming, expertise in optimizing stored procedures, and knowledge in Test Automation using Python. As a Data Engineer Lead, you will play a pivotal role in designing, developing, and maintaining robust data pipelines."
    },
    {
        "skill": "PL/SQL programming",
        "reference": "The ideal candidate will have a strong background in Python programming, Data Warehousing (DWH), proficient in PL/SQL programming, expertise in optimizing stored procedures, and knowledge in Test Automation using Python. As a Data Engineer Lead, you will play a pivotal role in designing, developing, and maintaining robust data pipelines."
    },
    {
        "skill": "Stored procedures optimization",
        "reference": "The ideal candidate will have a strong background in Python programming, Data Warehousing (DWH), proficient in PL/SQL programming, expertise in optimizing stored procedures, and knowledge in Test Automation using Python. As a Data Engineer Lead, you will play a pivotal role in designing, developing, and maintaining robust data pipelines."
    },
    {
        "skill": "Test Automation using Python",
        "reference": "The ideal candidate will have a strong background in Python programming, Data Warehousing (DWH), proficient in PL/SQL programming, expertise in optimizing stored procedures, and knowledge in Test Automation using Python. As a Data Engineer Lead, you will play a pivotal role in designing, developing, and maintaining robust data pipelines."
    },
    {
        "skill": "Data Engineer Lead",
        "reference": "The ideal candidate will have a strong background in Python programming, Data Warehousing (DWH), proficient in PL/SQL programming, expertise in optimizing stored procedures, and knowledge in Test Automation using Python. As a Data Engineer Lead, you will play a pivotal role in designing, developing, and maintaining robust data pipelines."
    },
    {
        "skill": "Python programming",
        "reference": "Proficiency in Python programming for data manipulation, automation, and testing, Strong expertise in PL/SQL programming and optimizing stored procedures, Extensive knowledge of Data Warehousing concepts and best practices, Experience with ETL tools and processes, Hands-on experience with database technologies such as SQL Server, Oracle, or PostgreSQL, Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and associated data services, Excellent problem-solving skills and attention to detail."
    },
    {
        "skill": "ETL tools and processes",
        "reference": "Proficiency in Python programming for data manipulation, automation, and testing, Strong expertise in PL/SQL programming and optimizing stored procedures, Extensive knowledge of Data Warehousing concepts and best practices, Experience with ETL tools and processes, Hands-on experience with database technologies such as SQL Server, Oracle, or PostgreSQL, Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and associated data services, Excellent problem-solving skills and attention to detail."
    },
    {
        "skill": "Database technologies",
        "reference": "Proficiency in Python programming for data manipulation, automation, and testing, Strong expertise in PL/SQL programming and optimizing stored procedures, Extensive knowledge of Data Warehousing concepts and best practices, Experience with ETL tools and processes, Hands-on experience with database technologies such as SQL Server, Oracle, or PostgreSQL, Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and associated data services, Excellent problem-solving skills and attention to detail."
    },
    {
        "skill": "Cloud platforms (e.g., AWS, Azure, GCP)",
        "reference": "Proficiency in Python programming for data manipulation, automation, and testing, Strong expertise in PL/SQL programming and optimizing stored procedures, Extensive knowledge of Data Warehousing concepts and best practices, Experience with ETL tools and processes, Hands-on experience with database technologies such as SQL Server, Oracle, or PostgreSQL, Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and associated data services, Excellent problem-solving skills and attention to detail."
    },
    {
        "skill": "Problem-solving skills",
        "reference": "Proficiency in Python programming for data manipulation, automation, and testing, Strong expertise in PL/SQL programming and optimizing stored procedures, Extensive knowledge of Data Warehousing concepts and best practices, Experience with ETL tools and processes, Hands-on experience with database technologies such as SQL Server, Oracle, or PostgreSQL, Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and associated data services, Excellent problem-solving skills and attention to detail."
    },
    {
        "skill": "Attention to detail",
        "reference": "Proficiency in Python programming for data manipulation, automation, and testing, Strong expertise in PL/SQL programming and optimizing stored procedures, Extensive knowledge of Data Warehousing concepts and best practices, Experience with ETL tools and processes, Hands-on experience with database technologies such as SQL Server, Oracle, or PostgreSQL, Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and associated data services, Excellent problem-solving skills and attention to detail."
    },
    {
        "skill": "Leadership",
        "reference": "Strong communication and teamwork skills, with the ability to lead and mentor a team of Data Engineers."
    },
    {
        "skill": "Teamwork",
        "reference": "Strong communication and teamwork skills, with the ability to lead and mentor a team of Data Engineers."
    },
    {
        "skill": "Mentorship",
        "reference": "Strong communication and teamwork skills, with the ability to lead and mentor a team of Data Engineers."
    },
    {
        "skill": "Data Engineering",
        "reference": "Develop and maintain scalable data pipelines, enhance data models"
    },
    {
        "skill": "Cloud-based Data Warehousing ETL",
        "reference": "Developing best practices, reusable code, libraries, frameworks for cloud-based data warehousing and ETL"
    },
    {
        "skill": "Multi-cloud Programming Languages",
        "reference": "Uses multi-cloud, programming languages like Java, Scala, Python, RDBMS, NoSQL databases"
    },
    {
        "skill": "Data Engineering, Big Data, Databricks",
        "reference": "B.S. in Computer Science or equivalent, 4-6 years' experience in data engineering and big data."
    },
    {
        "skill": "Client-facing Projects, Engagement Leadership, Stakeholder Management",
        "reference": "As a Tech Lead, often as an engagement lead on client engagements related to data warehousing."
    },
    {
        "skill": "Cloud Platforms (AWS, Azure), Data Integrations, ETL Development, Data Modeling",
        "reference": "Utilize consulting and technical skills to work in a client-facing project environment, contributing to real-time data processing solutions."
    },
    {
        "skill": "Data Engineering",
        "reference": "Are you passionate about leveraging data to make a positive impact on government initiatives? Join our team at a leading consulting firm."
    },
    {
        "skill": "Government Initiatives",
        "reference": "Are you passionate about leveraging data to make a positive impact on government initiatives? Join our team at a leading consulting firm."
    },
    {
        "skill": "Agile Development",
        "reference": "Collaborate with client stakeholders and technical teams to optimize data collection, storage, and usage, maximizing the value of information within the organization."
    },
    {
        "skill": "Team Collaboration",
        "reference": "Collaborate with client stakeholders and technical teams to optimize data collection, storage, and usage, maximizing the value of information within the organization."
    },
    {
        "skill": "Data Management",
        "reference": "Experience working within an Agile development environment, DevOps, and using version control platforms (e.g., GitHub)."
    },
    {
        "skill": "Cloud-Data Pipeline Solutions",
        "reference": "Experience working within an Agile development environment, DevOps, and using version control platforms (e.g., GitHub)."
    },
    {
        "skill": "Data Engineering, Analytics, SQL",
        "reference": "Maintain data architectures; focus on analytics; experience with relational and NoSQL databases; strong proficiency with SQL, data modeling, and data engineering."
    },
    {
        "skill": "Cloud Platforms, Data Management",
        "reference": "Experience with Cloud Platforms (GCP); knowledge of Cloud Data Platforms (Snowflake); understand best practices for data governance, management and remote teams."
    },
    {
        "skill": "Problem-Solving, Communication",
        "reference": "Strong problem-solving skills; excellent communication and collaboration abilities; ability to explain complex insights to non-technical stakeholders."
    },
    {
        "skill": "GCP Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "GCP Experience",
        "reference": "About the job"
    },
    {
        "skill": "Banking and Payments experience",
        "reference": "About the job"
    },
    {
        "skill": "Must have",
        "reference": ""
    },
    {
        "skill": "Good communication",
        "reference": ""
    },
    {
        "skill": "Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "ETL Development",
        "reference": "About the job"
    },
    {
        "skill": "SQL",
        "reference": "About the job"
    },
    {
        "skill": "UNIX/Linux scripting",
        "reference": "About the job"
    },
    {
        "skill": "Big Data distributed systems",
        "reference": "About the job"
    },
    {
        "skill": "IBM DataStage",
        "reference": "About the job"
    },
    {
        "skill": "Bachelor's in Computer Science, Computer Engineering or related field",
        "reference": "Qualifications"
    },
    {
        "skill": "Experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems",
        "reference": "Qualifications"
    },
    {
        "skill": "Prefer experience with IBM DataStage",
        "reference": "Qualifications"
    },
    {
        "skill": "Various programming languages like Java and Python, orchestration tools and processes or other directly related experience",
        "reference": "Qualifications"
    },
    {
        "skill": "Excellent analytical, organizational, and problem-solving skills",
        "reference": "Qualifications"
    },
    {
        "skill": "Ability to work independently and collaborate with others at all levels of technical understanding",
        "reference": "Qualifications"
    },
    {
        "skill": "Able to meet deadlines",
        "reference": "Qualifications"
    },
    {
        "skill": "Good judgment and project management skills",
        "reference": "Qualifications"
    },
    {
        "skill": "Ability to communicate verbally and in writing with both technical and non-technical staff",
        "reference": "Qualifications"
    },
    {
        "skill": "Ability to adapt to changing technology and priorities",
        "reference": "Qualifications"
    },
    {
        "skill": "Goal of supporting Production environment",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Maintaining enterprise-grade platforms that enable data-driven solutions",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Search for ways to automate and maintain scalable infrastructure",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Ensure delivery of highly available and scalable systems",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Monitor all systems and applications and ensure optimal performance",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Analyze and design technical solutions to address production problems",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Participate in troubleshooting applications and systems issues",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Identify, investigate, and propose solutions to technical problems",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Develop, test, and modify software to improve efficiency of data platforms and applications",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Monitor system performance to maintain consistent up time",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Prepare and maintain necessary documentation",
        "reference": "As a Data Engineer, you\u2019ll provide your talents"
    },
    {
        "skill": "Coordinate with data operations teams to deploy changes into production",
        "reference": "Participate in daily standups, team backlog grooming, and iteration retrospectives. Coordinate with data operations teams to deploy changes into production. Highest level may function as a lead. Other duties as assigned."
    },
    {
        "skill": "Highest level may function as a lead",
        "reference": "Participate in daily standups, team backlog grooming, and iteration retrospectives. Coordinate with data operations teams to deploy changes into production. Highest level may function as a lead. Other duties as assigned."
    },
    {
        "skill": "Other duties as assigned",
        "reference": "Participate in daily standups, team backlog grooming, and iteration retrospectives. Coordinate with data operations teams to deploy changes into production. Highest level may function as a lead. Other duties as assigned."
    },
    {
        "skill": "Excellent analytical, organizational, and problem-solving skills",
        "reference": "Qualifications"
    },
    {
        "skill": "Ability to work independently and collaborate with others at all levels of technical understanding",
        "reference": "Qualifications"
    },
    {
        "skill": "Able to meet deadlines",
        "reference": "Qualifications"
    },
    {
        "skill": "Good judgment and project management skills",
        "reference": "Qualifications"
    },
    {
        "skill": "Ability to communicate both verbally and in writing with both technical and non-technical staff",
        "reference": "Qualifications"
    },
    {
        "skill": "data engineering",
        "reference": "As a contract Data Engineer at Bezos Academy, you will be a critical member of our data engineering team."
    },
    {
        "skill": "autonomous work",
        "reference": "As a contract Data Engineer at Bezos Academy, you will be a critical member of our data engineering team."
    },
    {
        "skill": "problem solving",
        "reference": "As a contract Data Engineer at Bezos Academy, you will be a critical member of our data engineering team."
    },
    {
        "skill": "experienced data engineer",
        "reference": "You are an experienced data engineer who operates autonomously, loves to dig into data and learn new domains,"
    },
    {
        "skill": "dig into data",
        "reference": "You are an experienced data engineer who operates autonomously, loves to dig into data and learn new domains,"
    },
    {
        "skill": "love learning",
        "reference": "You are an experienced data engineer who operates autonomously, loves to dig into data and learn new domains,"
    },
    {
        "skill": "data infrastructure",
        "reference": "Help build and maintain the data infrastructure that is essential to Bezos Academy having a positive impact across the country and will help us make decisions that will shape our organization for years to come."
    },
    {
        "skill": "impact nationwide",
        "reference": "Help build and maintain the data infrastructure that is essential to Bezos Academy having a positive impact across the country and will help us make decisions that will shape our organization for years to come."
    },
    {
        "skill": "make decisions",
        "reference": "Help build and maintain the data infrastructure that is essential to Bezos Academy having a positive impact across the country and will help us make decisions that will shape our organization for years to come."
    },
    {
        "skill": "ETL Data Engineer",
        "reference": "Role: ETL Data EngineerLocation: Philadelphia, PA- Remote Opportunity"
    },
    {
        "skill": "Philadelphia, PA- Remote Opportunity",
        "reference": "Role: ETL Data EngineerLocation: Philadelphia, PA- Remote Opportunity"
    },
    {
        "skill": "Data Activities Management",
        "reference": "Manages data activities such as data requirements gathering, data analysis/modeling, and data issues resolution using standard approved technology Manages standardization, migration, transformation, validation, and quality assurance of data within multi-database platforms"
    },
    {
        "skill": "Database Platforms Management",
        "reference": "Manages data activities such as data requirements gathering, data analysis/modeling, and data issues resolution using standard approved technology Manages standardization, migration, transformation, validation, and quality assurance of data within multi-database platforms"
    },
    {
        "skill": "Data Processing & Publishing",
        "reference": "Identifies complex issues proactively and is responsible to see them through resolution, including identifying trends through data analysis and manipulation"
    },
    {
        "skill": "Data Analysis & Trend Identification",
        "reference": "Identifies complex issues proactively and is responsible to see them through resolution, including identifying trends through data analysis and manipulation"
    },
    {
        "skill": "Career Development",
        "reference": "Provide optional support and resources for job seekers in their career endeavors."
    },
    {
        "skill": "High-Growth Industries",
        "reference": "Inviting professionals in high-growth industries to join the talent pool."
    },
    {
        "skill": "Predictive Analytics",
        "reference": "Backed by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance."
    },
    {
        "skill": "Data Engineer GCP",
        "reference": "Actualmente nos encontramos en b\u00fasqueda de nuevos horizontes y ampliar nuestra red de profesionales en Data Engineer GCP"
    },
    {
        "skill": "Doer Community",
        "reference": "Los Doers somos la comunidad dentro de I2B Technologies"
    },
    {
        "skill": "Design, Develop & Maintain",
        "reference": "Funciones del cargo El Data Engineer estar\u00eda encargado..."
    },
    {
        "skill": "Sr Engineer, Python, CI/CD, Cloud experiences, GCP",
        "reference": "Responsibilities: Sr engineer- development experiences - Python (5 ~ 7 years)CI/CD (>2 years)Cloud experiences, GCP preferred (>2 years)Data engineering pipeline development experiences with following preferredDataflow (Apache beam),Cloud function + cloud composerPub/Sub (streaming)SQL development (>3 years)Cloud data warehousing/BigQuery (>2 years)Data analysis (>2 years)"
    },
    {
        "skill": "ApTask Recruitment, Workforce solutions and Talent Acquisition",
        "reference": "Join ApTask, a global leader in workforce solutions and talent acquisition services, as we shape the future of work."
    },
    {
        "skill": "GCP Data Engineer",
        "reference": "Job Title: GCP Data Engineer"
    },
    {
        "skill": "Remote Work",
        "reference": "Location: Remote"
    },
    {
        "skill": "1 Year Contract",
        "reference": "Duration: 1 year contract"
    },
    {
        "skill": "7+ Years Overall Experience",
        "reference": "Must Have 7+ years of overall data engineer experience"
    },
    {
        "skill": "Google Cloud",
        "reference": "Google Cloud"
    },
    {
        "skill": "Dataproc",
        "reference": "Google CloudDataproc"
    },
    {
        "skill": "Bigquery",
        "reference": "Google CloudDataprocBigquery"
    },
    {
        "skill": "DB2 (optional)",
        "reference": "Airflow experience is nice to have."
    },
    {
        "skill": "Airflow (optional)",
        "reference": "Airflow experience is nice to have."
    },
    {
        "skill": "AWS Services",
        "reference": "Required Skills Must have lead exp"
    },
    {
        "skill": "Application Integration",
        "reference": "Candidates will be dealing with confidential information"
    },
    {
        "skill": "Data Analysis",
        "reference": "Candidates will be dealing with confidential information"
    },
    {
        "skill": "AWS Analytics",
        "reference": ""
    },
    {
        "skill": "AWS Compute",
        "reference": ""
    },
    {
        "skill": "Database Management",
        "reference": ""
    },
    {
        "skill": "SQL",
        "reference": ""
    },
    {
        "skill": "Python",
        "reference": ""
    },
    {
        "skill": "Project Management",
        "reference": ""
    },
    {
        "skill": "JIRA",
        "reference": ""
    },
    {
        "skill": "Application Integration (CI/CD)",
        "reference": "Experience using JIRA"
    },
    {
        "skill": "Cleaning and Transforming Data",
        "reference": "Experience in the process of analyzing data"
    },
    {
        "skill": "Data Analysis",
        "reference": ""
    },
    {
        "skill": "Data Manipulation",
        "reference": ""
    },
    {
        "skill": "Data Validation",
        "reference": ""
    },
    {
        "skill": "Data Visualization",
        "reference": ""
    },
    {
        "skill": "Dbt",
        "reference": ""
    },
    {
        "skill": "Product Development Skills",
        "reference": ""
    },
    {
        "skill": "Design Requirements Translation",
        "reference": ""
    },
    {
        "skill": "The group of skills related to Product Development",
        "reference": ""
    },
    {
        "skill": "Software Development",
        "reference": ""
    },
    {
        "skill": "Product Testing Skills",
        "reference": ""
    },
    {
        "skill": "Product Testing",
        "reference": ""
    },
    {
        "skill": "Usability Testing",
        "reference": ""
    },
    {
        "skill": "UAT",
        "reference": ""
    },
    {
        "skill": "Using relevant product testing technology",
        "reference": ""
    },
    {
        "skill": "Software Evaluation and Testing",
        "reference": ""
    },
    {
        "skill": "Comparing products or systems components",
        "reference": ""
    },
    {
        "skill": "Assessing design, performance, supportability",
        "reference": ""
    },
    {
        "skill": "Programming",
        "reference": ""
    },
    {
        "skill": "Coding",
        "reference": ""
    },
    {
        "skill": "Debugging",
        "reference": ""
    },
    {
        "skill": "Using relevant programming languages",
        "reference": ""
    },
    {
        "skill": "Risk Mitigation",
        "reference": ""
    },
    {
        "skill": "Senior Data/ETL Engineer (Pentaho)",
        "reference": "Position Title: Senior Data Engineer to join our team in Washington, DC."
    },
    {
        "skill": "Technical Leadership",
        "reference": "Lead the development of business intelligence solutions."
    },
    {
        "skill": "Collaboration",
        "reference": "Coordinate with scrum masters and project managers to optimize scheduling, work allocations, and story assignments."
    },
    {
        "skill": "Big Data Engineer",
        "reference": "Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to their team."
    },
    {
        "skill": "Mid level",
        "reference": "Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to their team."
    },
    {
        "skill": "Data Services",
        "reference": "Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to their team."
    },
    {
        "skill": "Analyze system requirements",
        "reference": "Responsibilities: Analyze system requirements and design responsive algorithms and solutions. Use big data and cloud technologies to produce production quality code."
    },
    {
        "skill": "Design algorithms and solutions",
        "reference": "Responsibilities: Analyze system requirements and design responsive algorithms and solutions. Use big data and cloud technologies to produce production quality code."
    },
    {
        "skill": "Big data technologies",
        "reference": "Responsibilities: Analyze system requirements and design responsive algorithms and solutions. Use big data and cloud technologies to produce production quality code."
    },
    {
        "skill": "Cloud technologies",
        "reference": "Responsibilities: Analyze system requirements and design responsive algorithms and solutions. Use big data and cloud technologies to produce production quality code."
    },
    {
        "skill": "Production quality code",
        "reference": "Responsibilities: Analyze system requirements and design responsive algorithms and solutions. Use big data and cloud technologies to produce production quality code."
    },
    {
        "skill": "Performance tuning",
        "reference": "Responsibilities: Engage in performance tuning and scalability engineering. Work with team, peers and management to identify objectives and set priorities."
    },
    {
        "skill": "Scalability engineering",
        "reference": "Responsibilities: Engage in performance tuning and scalability engineering. Work with team, peers and management to identify objectives and set priorities."
    },
    {
        "skill": "SDLC engineering activities",
        "reference": "Responsibilities: Engage in performance tuning and scalability engineering. Work with team, peers and management to identify objectives and set priorities."
    },
    {
        "skill": "Small agile teams",
        "reference": "Responsibilities: Engage in performance tuning and scalability engineering. Work with team, peers and management to identify objectives and set priorities."
    },
    {
        "skill": "Creative solutions",
        "reference": "Responsibilities: Engage in performance tuning and scalability engineering. Work with team, peers and management to identify objectives and set priorities."
    },
    {
        "skill": "Work effectively",
        "reference": "Responsibilities: Perform related SDLC engineering activities like sprint planning and estimation. Work effectively in small agile teams."
    },
    {
        "skill": "Identify opportunities for improvement",
        "reference": "Responsibilities: Perform related SDLC engineering activities like sprint planning and estimation. Work effectively in small agile teams."
    },
    {
        "skill": "Execute",
        "reference": "Responsibilities: Perform related SDLC engineering activities like sprint planning and estimation. Work effectively in small agile teams."
    },
    {
        "skill": "Proven professional experience",
        "reference": "Requirements: Minimum 5 years of proven professional experience working in the IT industry. Degree in Computer Science or related domains."
    },
    {
        "skill": "IT industry",
        "reference": "Requirements: Minimum 5 years of proven professional experience working in the IT industry. Degree in Computer Science or related domains."
    },
    {
        "skill": "Computer Science degree",
        "reference": "Requirements: Minimum 5 years of proven professional experience working in the IT industry. Degree in Computer Science or related domains."
    },
    {
        "skill": "AWS experience",
        "reference": "Requirements: Minimum 5 years of proven professional experience working in the IT industry. Degree in Computer Science or related domains."
    },
    {
        "skill": "Cloud based Big Data technologies",
        "reference": "Requirements: Minimum 5 years of proven professional experience working in the IT industry. Degree in Computer Science or related domains."
    },
    {
        "skill": "Experience with big data technologies",
        "reference": "Requirements: Experience with cloud based Big Data technologies, Experience with big data technologies like Hadoop, Spark and Hive."
    },
    {
        "skill": "Hadoop",
        "reference": "Requirements: Experience with cloud based Big Data technologies, Experience with big data technologies like Hadoop, Spark and Hive."
    },
    {
        "skill": "Spark",
        "reference": "Requirements: Experience with cloud based Big Data technologies, Experience with big data technologies like Hadoop, Spark and Hive."
    },
    {
        "skill": "Hive",
        "reference": "Requirements: Experience with cloud based Big Data technologies, Experience with big data technologies like Hadoop, Spark and Hive."
    },
    {
        "skill": "Kubernetes",
        "reference": "Requirements: Experience with cloud based Big Data technologies, Experience with big data technologies like Hadoop, Spark and Hive."
    },
    {
        "skill": "AWS EKS",
        "reference": "Requirements: Experience with cloud based Big Data technologies, Experience with big data technologies like Hadoop, Spark and Hive."
    },
    {
        "skill": "Programming languages",
        "reference": "Requirements: Experience with one or more programming languages like Scala & Python & Java. Ability to push the frontier of technology and independently pursue better alternatives."
    },
    {
        "skill": "Scala",
        "reference": "Requirements: Experience with one or more programming languages like Scala & Python & Java. Ability to push the frontier of technology and independently pursue better alternatives."
    },
    {
        "skill": "Python",
        "reference": "Requirements: Experience with one or more programming languages like Scala & Python & Java. Ability to push the frontier of technology and independently pursue better alternatives."
    },
    {
        "skill": "Java",
        "reference": "Requirements: Experience with one or more programming languages like Scala & Python & Java. Ability to push the frontier of technology and independently pursue better alternatives."
    },
    {
        "skill": "Push the frontier of technology",
        "reference": "Requirements: Experience with one or more programming languages like Scala & Python & Java. Ability to push the frontier of technology and independently pursue better alternatives."
    },
    {
        "skill": "Python",
        "reference": "Data Science Engineer High Jr. to mid level with python, tensorflow, generative AICan have post doc academic background"
    },
    {
        "skill": "Tensorflow",
        "reference": "Data Science Engineer High Jr. to mid level with python, tensorflow, generative AICan have post doc academic background"
    },
    {
        "skill": "Generative AI",
        "reference": "Data Science Engineer High Jr. to mid level with python, tensorflow, generative AICan have post doc academic background"
    },
    {
        "skill": "AI and ML Strategy",
        "reference": "Responsible for Advance Analytics, Power BI, Cloud Migration, Cloud Architecture."
    },
    {
        "skill": "Advance Analytics",
        "reference": "Responsible for Advance Analytics, Power BI, Cloud Migration, Cloud Architecture."
    },
    {
        "skill": "Cloud Migration",
        "reference": "Responsible for Advance Analytics, Power BI, Cloud Migration, Cloud Architecture."
    },
    {
        "skill": "Agile Methodology",
        "reference": "They are working in Agile (but really more at a task/use case level).Natural Language Processing would be help."
    },
    {
        "skill": "Natural Language Processing",
        "reference": "They are working in Agile (but really more at a task/use case level).Natural Language Processing would be help."
    },
    {
        "skill": "Data Analysis",
        "reference": "Strong knowledge of R or Python for data analysis and modeling."
    },
    {
        "skill": "Modeling",
        "reference": "Strong knowledge of R or Python for data analysis and modeling."
    },
    {
        "skill": "Statistical Programs",
        "reference": "Strong knowledge of R or Python for data analysis and modeling."
    },
    {
        "skill": "Data Analytics Tools",
        "reference": "Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology)."
    },
    {
        "skill": "Database Applications",
        "reference": "Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology)."
    },
    {
        "skill": "Spreadsheets (VBA)",
        "reference": "Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology)."
    },
    {
        "skill": "SQL",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "Javascript",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "XML",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "JSON",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "HTML",
        "reference": "Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "Quick Learning",
        "reference": "Ability to learn new methods quickly and work under deadlines. Excellent teamwork and communication skills."
    },
    {
        "skill": "Deadline-oriented Work",
        "reference": "Ability to learn new methods quickly and work under deadlines. Excellent teamwork and communication skills."
    },
    {
        "skill": "Teamwork",
        "reference": "Ability to learn new methods quickly and work under deadlines. Excellent teamwork and communication skills."
    },
    {
        "skill": "Analytical Skills",
        "reference": "Strong analytical and problem-solving abilities. Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "Problem-solving Abilities",
        "reference": "Strong analytical and problem-solving abilities. Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "SQL, VBA Knowledge",
        "reference": "Strong analytical and problem-solving abilities. Basic understanding of SQL, Javascript, XML, JSON, and HTML."
    },
    {
        "skill": "Kafka",
        "reference": "Must-Haves Experience working with Kafka, Spark Streams, Snowflake"
    },
    {
        "skill": "Spark Streams",
        "reference": "Must-Haves Experience working with Kafka, Spark Streams, Snowflake"
    },
    {
        "skill": "Snowflake",
        "reference": "Must-Haves Experience working with Kafka, Spark Streams, Snowflake"
    },
    {
        "skill": "Grafana",
        "reference": "Experience working with HL7 dataExperience coding with Python/JavaStrong SQL, Data Warehousing, and Data Lake fundamentalsHands-on experience with Linux"
    },
    {
        "skill": "Prometheus",
        "reference": "Experience working with HL7 dataExperience coding with Python/JavaStrong SQL, Data Warehousing, and Data Lake fundamentalsHands-on experience with Linux"
    },
    {
        "skill": "AWS services (S3, Kinesis, TimeStream, Redshift, CloudWatch, EKS)",
        "reference": "Experience working with HL7 dataExperience coding with Python/JavaStrong SQL, Data Warehousing, and Data Lake fundamentalsHands-on experience with Linux"
    },
    {
        "skill": "Agile methodology",
        "reference": "Experience utilizing Agile methodology for developmentExperience evaluating and implementing technologies in a production capacityExperience building a streaming data platform from ground upAbility to manage one's tasks and work with stakeholders"
    },
    {
        "skill": "API integration",
        "reference": "Experience utilizing Agile methodology for developmentExperience evaluating and implementing technologies in a production capacityExperience building a streaming data platform from ground upAbility to manage one's tasks and work with stakeholders"
    },
    {
        "skill": "Technology implementation",
        "reference": "Experience utilizing Agile methodology for developmentExperience evaluating and implementing technologies in a production capacityExperience building a streaming data platform from ground upAbility to manage one's tasks and work with stakeholders"
    },
    {
        "skill": "Data Warehousing",
        "reference": "Strong SQL, Data Warehousing, and Data Lake fundamentals"
    },
    {
        "skill": "Data Lake fundamentals",
        "reference": "Strong SQL, Data Warehousing, and Data Lake fundamentals"
    },
    {
        "skill": "Database modeling, data pipelines design, cleansing and performance tuning",
        "reference": "Create database models and components, Assist in the design of data pipelines, data models, profiling/cleansing the data, and performance tuning"
    },
    {
        "skill": "Developing and maintaining data pipelines, workflows, ETL scripts, tests and results",
        "reference": "Develop and maintain data pipelines, data workflows, ETL/ELT scripts or packages, Conducts tests Tests data pipelines"
    },
    {
        "skill": "Full lifecycle of services/solution delivery for projects",
        "reference": "Implements full lifecycle of services/solution delivery for projects"
    },
    {
        "skill": "Data infrastructure, Snowflake, DBT, Fivetran, AWS, SQL/PSQL, Python, R",
        "reference": "Required Skills/Abilities Extensive experience building and supporting data infrastructure. Direct experience working with Snowflake, DBT and Fivetran. Strong experience with AWS services such as Data Migration Services, RDS, Lambda, API Gateway, S3, etc. Expert knowledge of SQL / PSQL."
    },
    {
        "skill": "High availability systems management",
        "reference": "Strong experience with maintaining high availability systems"
    },
    {
        "skill": "Complex problem solving, innovation mindset",
        "reference": "Initiative to solve complex problems; takes an outside in perspective to identify innovative solutions."
    },
    {
        "skill": "Data Engineer, Enterprise Data Warehouse/Datamart, ETL",
        "reference": "Required Experience Data Engineer with Enterprise Data warehouse/Datamart and ETL background."
    },
    {
        "skill": "P&C Insurance Domain Expertise",
        "reference": "Experience in Property & Casualty (P&C) Insurance domain is required."
    },
    {
        "skill": "Strong Python Experience",
        "reference": "Required Strong Python experience (DataFrame, APIs, Batch processing, Data pipelines)"
    },
    {
        "skill": "Azure Platform Knowledge",
        "reference": "Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake, Data Factory, Azure SQL Database."
    },
    {
        "skill": "Data Analysis and Predictive Modeling",
        "reference": "Experience in data analysis and predictive modeling to support decision-making processes."
    },
    {
        "skill": "Excellent SQL, Excel Skills",
        "reference": "Strong SQL experience with excellent Excel skills (i.e. Pivot Tables)."
    },
    {
        "skill": "Data Modeling for Enterprise Data Warehouse Ecosystems",
        "reference": "Understanding of Data modeling for Enterprise Data Warehouse ecosystems."
    },
    {
        "skill": "Spark, PySpark Familiarity",
        "reference": "Knowledge in Spark, PySpark preferable."
    },
    {
        "skill": "Data Lake Experience",
        "reference": "Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues."
    },
    {
        "skill": "File Formats, Data Encryption & Masking Knowledge",
        "reference": "Working knowledge on different file formats such as JSON, Parquet, CSV, etc. Familiarity with data encryption, data maskingDatabase experience in SQL Server."
    },
    {
        "skill": "5+ years working with Advanced SQL & PL/SQL",
        "reference": "Need To Have Excellent Communication Skills 5+ years working with Advanced Structured Query Language (SQL) & PL/SQL"
    },
    {
        "skill": "Data Engineering Experience",
        "reference": "Minimum of 5-10 years of IT/IS experience Need to have excellent communication skills"
    },
    {
        "skill": "Agile Methodologies Knowledge",
        "reference": "Experience working in an Agile environment"
    },
    {
        "skill": "SQL",
        "reference": "A passion for technology 5+ years of experience with SQL and Python"
    },
    {
        "skill": "Python",
        "reference": "A passion for technology 5+ years of experience with SQL and Python"
    },
    {
        "skill": "Data Eng/Science",
        "reference": "5+ years of experience in the Data Eng/Science space"
    },
    {
        "skill": "ELT/ETL",
        "reference": "5+ years of experience in the Data Eng/Science space"
    },
    {
        "skill": "Databases",
        "reference": "5+ years of experience in the Data Eng/Science space"
    },
    {
        "skill": "Cross-functional",
        "reference": "Experience working cross functionally to ground business strategy and process in data"
    },
    {
        "skill": "Business Strategy",
        "reference": "Experience working cross functionally to ground business strategy and process in data"
    },
    {
        "skill": "Data Engineer",
        "reference": "Requirements Data Engineer with Integration (ETL/Informatica), Database (SQL Server/Oracle) and Automation (API, Python scripting etc.) experience"
    },
    {
        "skill": "ETL/Informatica",
        "reference": "Requirements Data Engineer with Integration (ETL/Informatica), Database (SQL Server/Oracle) and Automation (API, Python scripting etc.) experience"
    },
    {
        "skill": "Database",
        "reference": "Requirements Data Engineer with Integration (ETL/Informatica), Database (SQL Server/Oracle) and Automation (API, Python scripting etc.) experience"
    },
    {
        "skill": "Cloud/On Prem Data warehouse",
        "reference": "Experienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data science"
    },
    {
        "skill": "Data Lake",
        "reference": "Experienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data science"
    },
    {
        "skill": "Data science",
        "reference": "Experienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data science"
    },
    {
        "skill": "Multi-year, large-scale projects",
        "reference": "Experience with multi-year, large-scale projects"
    },
    {
        "skill": "Extensive experience with data migration and transformation testing",
        "reference": "Experience with multi-year, large-scale projects"
    },
    {
        "skill": "DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase",
        "reference": "Extensive experience with both data migration and data transformation testing"
    },
    {
        "skill": "SQL/Unix/Linux",
        "reference": "Extensive experience using Python scripting and Cloud Technologies"
    },
    {
        "skill": "Python scripting",
        "reference": "Extensive experience using Python scripting and Cloud Technologies"
    },
    {
        "skill": "Cloud Technologies",
        "reference": "Extensive experience using Python scripting and Cloud Technologies"
    },
    {
        "skill": "API/RESTAssured automation",
        "reference": "API/RESTAssured automation, building reusable frameworks, and good technical expertise/acumen"
    },
    {
        "skill": "Building reusable frameworks",
        "reference": "API/RESTAssured automation, building reusable frameworks, and good technical expertise/acumen"
    },
    {
        "skill": "Good technical expertise",
        "reference": "API/RESTAssured automation, building reusable frameworks, and good technical expertise/acumen"
    },
    {
        "skill": "Java/Java Script",
        "reference": "Java/Java Script - Implement core Java, Integration, Core Java and API"
    },
    {
        "skill": "Implement core Java",
        "reference": "Java/Java Script - Implement core Java, Integration, Core Java and API"
    },
    {
        "skill": "Integration",
        "reference": "Java/Java Script - Implement core Java, Integration, Core Java and API"
    },
    {
        "skill": "Core Java and API",
        "reference": "Java/Java Script - Implement core Java, Integration, Core Java and API"
    },
    {
        "skill": "Functional/UI/Selenium",
        "reference": "Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress"
    },
    {
        "skill": "BDD/Cucumber",
        "reference": "Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress"
    },
    {
        "skill": "Specflow",
        "reference": "Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress"
    },
    {
        "skill": "Data Validation/Kafka",
        "reference": "Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress"
    },
    {
        "skill": "Big Data",
        "reference": "Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress"
    },
    {
        "skill": "Automation experience using Cypress",
        "reference": "Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress"
    },
    {
        "skill": "API/Rest API",
        "reference": "API/Rest API - Rest API and Micro Services using JSON, SoapUI"
    },
    {
        "skill": "JSON",
        "reference": "API/Rest API - Rest API and Micro Services using JSON, SoapUI"
    },
    {
        "skill": "SoapUI",
        "reference": "API/Rest API - Rest API and Micro Services using JSON, SoapUI"
    },
    {
        "skill": "Micro Services",
        "reference": "API/Rest API - Rest API and Micro Services using JSON, SoapUI"
    },
    {
        "skill": "Map reduce",
        "reference": "Experience in map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R"
    },
    {
        "skill": "Tools like Hadoop, Hive, Pig, Kafka, S4, Map R",
        "reference": "Experience in map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R"
    },
    {
        "skill": "Storage tools",
        "reference": "Experience with one or more industry-standard defect or Test Case management Tools"
    },
    {
        "skill": "S3, HDFS",
        "reference": "Experience with one or more industry-standard defect or Test Case management Tools"
    },
    {
        "skill": "Good communication skills",
        "reference": "Great communication skills (regularly interacts with cross functional team members)"
    },
    {
        "skill": "Cross functional team members",
        "reference": "Great communication skills (regularly interacts with cross functional team members)"
    },
    {
        "skill": "Azure Data Engineering",
        "reference": "Analytica seeks a remote Azure Data Engineer to join its growing Analytics practice in support of federal clients."
    },
    {
        "skill": "Data Pipeline Architecture",
        "reference": "Lead initiatives to create and maintain optimal data pipeline architecture."
    },
    {
        "skill": "Apache Spark on Databricks",
        "reference": "Design, build, and maintain large, complex data processing pipelines using Apache Spark on Databricks in Azure for enterprise requirements."
    },
    {
        "skill": "Data Analytics Engineer, Python, Tableau, Cloud",
        "reference": "We need a senior (10+ Years) Data Analytics engineer with heavy Python and data visualization (Tableau or Power BI) and working with Large Cloud Data (BigQuery) sets as well as heavy ETL experience. Matlab is a big plus."
    },
    {
        "skill": "ETL Development",
        "reference": "You will develop ETL/ELT processes that comply with the computational demands, accuracy, and reliability of the relevant processes at various stages of production."
    },
    {
        "skill": "Data Visualization",
        "reference": "You will partner with analysts and business leaders to create compelling visualization environments."
    },
    {
        "skill": "Azure, Senior Data Engineer, Consulting Experience",
        "reference": "Blueprint is looking for an Azure, Senior Data Engineer to join us as we build cutting-edge technology solutions!"
    },
    {
        "skill": "Data Architecture Solutions, Databricks and Lakehouse, Optimization & Troubleshooting",
        "reference": "Develop and implement effective data architecture solutions using Databricks and Lakehouse; Optimize and tune data pipelines for performance and scalability"
    },
    {
        "skill": "Data Ingestion, SQL Development, Data Engineering, Data Warehousing, Best Practices",
        "reference": "At least 5+ -years of experience as a data engineer; Data Ingestion experience from inception to Gold Medallion; Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices"
    },
    {
        "skill": "Data Engineering, Data Pipeline, Hybrid Cloud",
        "reference": "Establish and operate data pipeline for USMC ground vehicle CAN bus data to SIL hybrid cloud environment"
    },
    {
        "skill": "Data Transformation Scripts, Data Integrity, Security",
        "reference": "Utilize data engineering tools to transform raw CAN bus data into processed data ready for ingestion and ensure data integrity, security, and analysis"
    },
    {
        "skill": "AI/ML Models, DOD Data Systems, Relational Databases, NoSQL",
        "reference": "Experience with data systems and demonstrated experience in DOD data systems, relational databases, and NoSQL databases for AI/ML models"
    },
    {
        "skill": "Agile software development testing, SQL queries, data validation",
        "reference": "Participate in Agile ceremonies, help analyze requirements, develop testing strategies, and write manual and automated test scripts for new and existing functionality. Monitor all development cycles, prepare test data, design and execute test plans, and evaluate test results"
    },
    {
        "skill": "Database testing, relational databases, data warehousing",
        "reference": "4+ years of experience with agile software development testing, reviewing user stories, acceptance criteria, and other available information in order to develop test plans and test scenarios, both manual and automated. 4+ years of experience in database testing for relational databases(preferred Snowflake and MS SQL Server), ETL/ELT data solutions, and reporting and analytics tools"
    },
    {
        "skill": "API testing, automation frameworks, cloud services",
        "reference": "Experience in writing complex SQL queries, ability to determine the types of testing that must be conducted (i.e., data validation, regression, etc.), including evaluating the testability of requirements and create a comprehensive test plan that supports the business and technological solutions being delivered. 3+ years of experience creating automated scripts using pytest for data validation Expertise building test architecture and framework from ground up 1+ years of experience with API testing; manual (using tools like Postman/swagger) & automated (REST Assured/CURL). Experience working with Cloud services and CI/CD tools like Jenkins"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Data systems, data pipelines, data lakes, lakehouses",
        "reference": "As a Senior Data Engineer on the data platform team"
    },
    {
        "skill": "Collaborate with stakeholders, Spark and Pyspark libraries, load data, lakehouse models, build scalable data pipelines, cloud infrastructure services, documentation, automation, monitoring and optimizing",
        "reference": "Your areas of focus will include:"
    },
    {
        "skill": "IT managed AWS account, VPC, data platform development, staging, production environments, code as code",
        "reference": "Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments"
    },
    {
        "skill": "Data pipelines, cloud infrastructure, standard operating procedures, documentation, automation load scaling performance testing, data platform cloud infrastructure services configuration",
        "reference": "Documentation of data pipelines, cloud infrastructure, and standard operating procedures"
    },
    {
        "skill": "Express data platform cloud infrastructure, services and configuration as code, load, scale, performance tuning, ensure appropriate data privacy and security, continuous upgrades testing",
        "reference": "Automate load, scaling, and performance testing of data platform pipelines and infrastructure; Monitor, operate, optimize data pipelines and distributed applications"
    },
    {
        "skill": "Build data pipeline unit integration quality performance tests, participate in peer code reviews, identify opportunities for improvement, experience developing Spark and streaming technologies",
        "reference": "Participate in peer code reviews, code approvals, and pull requests; Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance"
    },
    {
        "skill": "Bonus skills\u2014lakehouse technologies, Flink Presto Dremio Kubernetes infrastructure as code, zero trust security framework, CI/CD pipelines for automated testing deployment",
        "reference": "Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta; Experience with Flink, Presto, Dremio, Databricks, or Kubernetes"
    },
    {
        "skill": "GenRocket",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "Scripting",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "PostGres/DB ability",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "API work",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "Agile Mindset",
        "reference": "Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "Azure Cloud",
        "reference": "Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "ADO experience",
        "reference": "Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "Sr. Data Engineer, ETL Pipelines, Enterprise Data Initiatives",
        "reference": "Overview Otter Products is currently recruiting for a Sr. Data Engineer."
    },
    {
        "skill": "Data Pipeline Design, Optimization and Production",
        "reference": "The Sr. Data Engineer's work would be in designing, managing and optimizing data pipelines."
    },
    {
        "skill": "Collaboration with Other Data Professionals",
        "reference": "The Sr. Data Engineers will also be expected to collaborate with other professionals like data scientists, data architects, etc."
    },
    {
        "skill": "Data Security and Compliance Management",
        "reference": "Guarantee compliance with data governance and security requirements while creating data pipelines."
    },
    {
        "skill": "Azure Data Catalog, Business Glossary Application Management",
        "reference": "Manage Azure Data Catalog and Business Glossary Application ensuring linage and data sources are mapped within the application."
    },
    {
        "skill": "Data Pipelines, Snowflake, DevSecOps",
        "reference": "Develops and operationalizes data pipelines"
    },
    {
        "skill": "Data Processing Solutions, Automation",
        "reference": "Designs and implements standardized data management procedures"
    },
    {
        "skill": "Business Intelligence Analytics, Real-Time Data Pipelines",
        "reference": "Designs, develops, and maintains real-time processing applications"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments."
    },
    {
        "skill": "Consulting Experience",
        "reference": "Blueprint is looking for a Principal Data Engineer to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates."
    },
    {
        "skill": "Mentoring & Training",
        "reference": "Mentor and train other data engineers on best practices for data engineering and Databricks usage; provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally."
    },
    {
        "skill": "Strong programming skills",
        "reference": "Experience with Apache Spark and SQL Experience with cloud computing platforms"
    },
    {
        "skill": "Python and Scala Experience",
        "reference": "Experience with Apache Spark and SQL Experience with cloud computing platforms"
    },
    {
        "skill": "Apache Spark, SQL",
        "reference": "Experience with Apache Spark and SQL Experience with cloud computing platforms"
    },
    {
        "skill": "Databricks Certified Data Engineer Associate",
        "reference": "Additional Qualifications"
    },
    {
        "skill": "Data Engineering best practices",
        "reference": "Additional Qualifications"
    },
    {
        "skill": "Designing data pipelines",
        "reference": "Responsibilities"
    },
    {
        "skill": "Event-driven sources",
        "reference": "Responsibilities"
    },
    {
        "skill": "Pipeline orchestration tools",
        "reference": "Responsibilities"
    },
    {
        "skill": "Data Engineer Experience",
        "reference": "Previous experience as a data engineer or in a similar role"
    },
    {
        "skill": "Data Modeling & Techniques",
        "reference": "Technical expertise with data models, data mining, and segmentation techniques"
    },
    {
        "skill": "Programming Languages",
        "reference": "Knowledge of programming languages (e.g. Java and Python)"
    },
    {
        "skill": "Data Engineering",
        "reference": "8+ years of data engineering experience, Previous experience building out data warehousing, data pipelines, and internal analytics"
    },
    {
        "skill": "Scalable Solutions Architecture",
        "reference": "Strong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, Elasticsearch, The ability to communicate cross-functionally"
    },
    {
        "skill": "Data Warehousing and Infrastructure Design",
        "reference": "Architect and build out the future of data warehousing at EvenUp, Ensure our organization can scale with consistent, standardized access to data stores"
    },
    {
        "skill": "Data Engineering",
        "reference": "Design, develop, and maintain data infrastructure for decision-making processes"
    },
    {
        "skill": "Master Data Management (MDM)",
        "reference": "Implement MDM processes, manage master data repositories, ensure data quality, and consistency across organization"
    },
    {
        "skill": "Azure Data Factory, Databricks, Python",
        "reference": "Utilize expertise in these tools to optimize data pipelines, store datasets, and manage data repositories"
    },
    {
        "skill": "Software Development",
        "reference": "6+ years of experience as a software developer in an ETL environment, Experienced with ETL tools (Informatica, Talend, etc.), Strong proficiency with Python, Google Go (Golang), or similar language"
    },
    {
        "skill": "Data Management",
        "reference": "Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Glue, lambda, etc., Experience with relational SQL and NoSQL databases such as Postgres or Cassandra"
    },
    {
        "skill": "Collaboration",
        "reference": "Participate in internal reviews of code, software components, and systems and make data-driven decisions on how they should evolve, Work on any task and help solve problems when needed, Communicate effectively and participate with team members in an Agile environment"
    },
    {
        "skill": "Data Software Engineer",
        "reference": "Working at Atlassian - Apply for the Principal Data Software Engineer role"
    },
    {
        "skill": "Principal Data Software Engineer",
        "reference": "Apply for the Principal Data Software Engineer role"
    },
    {
        "skill": "Database Technology",
        "reference": "Considering multiple database technologies to improve storage and retrieval"
    },
    {
        "skill": "Data Architecture Leadership",
        "reference": "Refine and govern our logical data models for enterprise decision making tools"
    },
    {
        "skill": "Customer Reporting Requirements",
        "reference": "Work with design and product management team to translate customer reporting requirements"
    },
    {
        "skill": "System Design and Maintenance",
        "reference": "Proficiency in designing and maintenance of systems with multiple dependencies"
    },
    {
        "skill": "Data Ingestion and Translation Tools",
        "reference": "Experience in data ingestion and translation tools for security and privacy"
    },
    {
        "skill": "Data Modelling and API Design",
        "reference": "Data modeling and API design experience"
    },
    {
        "skill": "Cross-Team Collaboration",
        "reference": "Previous experience in cross-team collaboration in large-scale projects"
    },
    {
        "skill": "AWS Native services, AWS Glue, Lambda functions, Step functions",
        "reference": "Primary skills will be AWS Native services , AWS Glue , Lambda functions , Step functions  and also on ETL and Data bricks"
    },
    {
        "skill": "Data Engineering, AWS Native application services",
        "reference": "Mandatory Skill DataBricks - Data Engineering Additional Skill AWS Native application services"
    },
    {
        "skill": "Diverse workforce, equal employment opportunity",
        "reference": "All applicants will be evaluated solely on their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "We are looking for a Senior Data Engineer to help evolve our data pipelines, warehouse, and workflow systems"
    },
    {
        "skill": "Core Data Team",
        "reference": "The Position: We are looking for a Senior Data Engineer to help evolve our data pipelines, warehouse, and workflow systems to be more resilient, extensible, and maintainable. This role sits on the Core Data Team, which is focused on delivering high-quality data and tooling on a reliable and scalable platform."
    },
    {
        "skill": "High-Quality Data",
        "reference": "So if you love sports and their community building potential, or building cool products is your sport, GameChanger is the team for you. We are a remote first, dynamic tech company based in New York City, and we are solving some of the biggest challenges in youth sports today."
    },
    {
        "skill": "Data Governance",
        "reference": "Collibra Data Governance Engineer"
    },
    {
        "skill": "Remote Work",
        "reference": "6 month CTH that is remote"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Data Engineer, you'll provide your talents in contributing to the success of the client's team by delivering various responsibilities."
    },
    {
        "skill": "Enterprise-grade Platforms",
        "reference": "As a Data Engineer, you'll provide your talents in contributing to the success of the client's team by delivering various responsibilities."
    },
    {
        "skill": "ETL Development",
        "reference": "Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems."
    },
    {
        "skill": "SQL",
        "reference": "Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems."
    },
    {
        "skill": "UNIX/Linux Scripting",
        "reference": "Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems."
    },
    {
        "skill": "Big Data Distributed Systems",
        "reference": "Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems."
    },
    {
        "skill": "DataStage (Prefer)",
        "reference": "Experience with IBM DataStage. Various programming languages like Java and Python, orchestration tools and processes or other directly related experience."
    },
    {
        "skill": "IBM",
        "reference": "Experience with IBM DataStage. Various programming languages like Java and Python, orchestration tools and processes or other directly related experience."
    },
    {
        "skill": "Programming Languages (Java, Python)",
        "reference": "Experience with IBM DataStage. Various programming languages like Java and Python, orchestration tools and processes or other directly related experience."
    },
    {
        "skill": "Orchestration Tools",
        "reference": "Experience with IBM DataStage. Various programming languages like Java and Python, orchestration tools and processes or other directly related experience."
    },
    {
        "skill": "Data engineering experience",
        "reference": "8+ years of data engineering experience"
    },
    {
        "skill": "Building out data infrastructure",
        "reference": "Previous experience building out data warehousing, data pipelines, and internal analytics"
    },
    {
        "skill": "Strong communication skills",
        "reference": "The ability to communicate cross-functionally with various stakeholders"
    },
    {
        "skill": "Data Engineering",
        "reference": "About Our Team And What We'll Build Together"
    },
    {
        "skill": "Collaboration",
        "reference": "Work alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature"
    },
    {
        "skill": "Data Warehouse Strategy",
        "reference": "Establish theSkimm's data warehousing strategy (ex. Kimball, Data Vault, etc.)"
    },
    {
        "skill": "Collibra Data Governance Engineer",
        "reference": "Position: Collibra Data Governance Engineer Remote Position"
    },
    {
        "skill": "Remote Work",
        "reference": "Remote Poistion"
    },
    {
        "skill": "Suitable Resume Submission",
        "reference": "Send resume to sowmya.g@ivytechsol.us OR CALL: 2243488595"
    },
    {
        "skill": "Azure, Data Engineering, Azure Synapse Analytics",
        "reference": "As a Principal Data Engineer (Azure), you would have hands-on experience working on Azure as cloud, Databricks and some exposure/experience on Data Modelling."
    },
    {
        "skill": "Data Lake, Pipeline Design, Big Data Technologies",
        "reference": "You will build and learn about a variety of analytics solutions & platforms, data lakes, modern data platforms, data fabric solutions, etc. using different Open Source, Big Data, and Cloud technologies on Microsoft Azure."
    },
    {
        "skill": "Data Processing, Collaboration, Problem Solving",
        "reference": "Design and build scalable & metadata-driven data ingestion pipelines (For Batch and Streaming Datasets), Conceptualize and execute high-performance data processing for structured and unstructured data, and data harmonization."
    },
    {
        "skill": "SQL",
        "reference": "Skills: 3 main requirements are SQL, Azure Data Factory, Azure(very heavy Azure role), SQL stored procedures."
    },
    {
        "skill": "Azure Data Factory",
        "reference": "Skills: 3 main requirements are SQL, Azure Data Factory, Azure(very heavy Azure role), SQL stored procedures."
    },
    {
        "skill": "Azure",
        "reference": "Skills: 3 main requirements are SQL, Azure Data Factory, Azure(very heavy Azure role), SQL stored procedures."
    },
    {
        "skill": "Communication",
        "reference": "Skills: 3 main requirements are SQL, Azure Data Factory, Azure(very heavy Azure role), SQL stored procedures."
    },
    {
        "skill": "Functional Requirement Documents",
        "reference": "Must have experience creating User stories. Will be working with 2 Data Engineers Ideal candidate will have experience as a Data Engineer Azure space."
    },
    {
        "skill": "User Stories",
        "reference": "Must have experience creating User stories. Will be working with 2 Data Engineers Ideal candidate will have experience as a Data Engineer Azure space."
    },
    {
        "skill": "Requirements Gathering",
        "reference": "Must have experience creating User stories. Will be working with 2 Data Engineers Ideal candidate will have experience as a Data Engineer Azure space."
    },
    {
        "skill": "Agile Environment",
        "reference": "Must have experience creating User stories. Will be working with 2 Data Engineers Ideal candidate will have experience as a Data Engineer Azure space."
    },
    {
        "skill": "Data-Focused",
        "reference": "Heavy data focused and heavy integration focused. Must work in an Agile environment Must write BRD's, Must write FRD's, Must have experience in creating user stories, Must have experience in data mapping and workflow diagrams."
    },
    {
        "skill": "Integration Focused",
        "reference": "Heavy data focused and heavy integration focused. Must work in an Agile environment Must write BRD's, Must write FRD's, Must have experience in creating user stories, Must have experience in data mapping and workflow diagrams."
    },
    {
        "skill": "Business Units Collaboration",
        "reference": "Heavy data focused and heavy integration focused. Must work in an Agile environment Must write BRD's, Must write FRD's, Must have experience in creating user stories, Must have experience in data mapping and workflow diagrams."
    },
    {
        "skill": "Data Engineering",
        "reference": "A solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates."
    },
    {
        "skill": "Databricks and Lakehouse",
        "reference": "Develop and implement effective data architecture solutions using Databricks and Lakehouse"
    },
    {
        "skill": "Mentoring",
        "reference": "Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally"
    },
    {
        "skill": "LinkedIn, Senior Consultants, DBT, GCP Data Engineer, Remote, Est Timezone",
        "reference": "About the job"
    },
    {
        "skill": "Design, develop, optimize data pipelines, architectures, and data sets, Google Cloud, BigQuery, DBT, SQL, DBT experience, Python, Automate data pipeline processes, Data analysts, Data scientists, Data governance, Data integrity, Quality, Industry trends, Continuous learning",
        "reference": "What You'll Do"
    },
    {
        "skill": "Google Cloud Certification (Professional Data Engineer), DBT expertise, Advanced concepts like incremental models, custom schemas, use of variables, DBT Cloud, SQL, Complex queries, Optimization techniques, Data warehousing, Google Cloud data services (BigQuery, Pub/Sub, Dataflow, Dataproc), Designing and implementing architectures, Best practices for security, performance, reliability, Python, Data governance, Security practices, Communication skills, Problem-solving, Attention to detail",
        "reference": "What You Have"
    },
    {
        "skill": "Data Engineering",
        "reference": "Responsibilities Develop, construct, test and maintain architectures"
    },
    {
        "skill": "Development",
        "reference": "Responsibilities Develop, construct, test and maintain architectures"
    },
    {
        "skill": "Maintenance",
        "reference": "Responsibilities Develop, construct, test and maintain architectures"
    },
    {
        "skill": "ETL Tools",
        "reference": "Data acquisition and ingestion \u2013 Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx"
    },
    {
        "skill": "Pipelines",
        "reference": "Data acquisition and ingestion \u2013 Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx"
    },
    {
        "skill": "Prototypes",
        "reference": "Data acquisition and ingestion \u2013 Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx"
    },
    {
        "skill": "Data Architecture",
        "reference": "Evaluate business needs and objectives; define the data architecture framework, standards and principles, including modeling, metadata, security and reference data"
    },
    {
        "skill": "Standards",
        "reference": "Evaluate business needs and objectives; define the data architecture framework, standards and principles, including modeling, metadata, security and reference data"
    },
    {
        "skill": "Principles",
        "reference": "Evaluate business needs and objectives; define the data architecture framework, standards and principles, including modeling, metadata, security and reference data"
    },
    {
        "skill": "Data Modeling",
        "reference": "Create and optimize data models to support various business applications"
    },
    {
        "skill": "Optimization",
        "reference": "Create and optimize data models to support various business applications"
    },
    {
        "skill": "Business Applications",
        "reference": "Create and optimize data models to support various business applications"
    },
    {
        "skill": "Workflows",
        "reference": "Automate and support workflows to ensure timely delivery"
    },
    {
        "skill": "Delivery",
        "reference": "Automate and support workflows to ensure timely delivery"
    },
    {
        "skill": "Automation",
        "reference": "Automate and support workflows to ensure timely delivery"
    },
    {
        "skill": "Data Engineering Experience",
        "reference": "5+ years data engineering experience; Proficient programming capability, Python preferred"
    },
    {
        "skill": "Proficiency",
        "reference": "5+ years data engineering experience; Proficient programming capability, Python preferred"
    },
    {
        "skill": "Python",
        "reference": "5+ years data engineering experience; Proficient programming capability, Python preferred"
    },
    {
        "skill": "Experience",
        "reference": "Experience working within financial services industry and/or familiarity of different asset classes is preferred"
    },
    {
        "skill": "Financial Services",
        "reference": "Experience working within financial services industry and/or familiarity of different asset classes is preferred"
    },
    {
        "skill": "Asset Classes",
        "reference": "Experience working within financial services industry and/or familiarity of different asset classes is preferred"
    },
    {
        "skill": "Data Models",
        "reference": "Experience designing data models and data warehouses and using SQL database management systems along with data processing using traditional and distributed systems"
    },
    {
        "skill": "Database Management Systems",
        "reference": "Experience designing data models and data warehouses and using SQL database management systems along with data processing using traditional and distributed systems"
    },
    {
        "skill": "Distributed Systems",
        "reference": "Experience designing data models and data warehouses and using SQL database management systems along with data processing using traditional and distributed systems"
    },
    {
        "skill": "ETL Pipelines",
        "reference": "Experience with data modeling, data warehousing, and building ETL pipelines for the financial services industry and various asset classes"
    },
    {
        "skill": "Asset Classes",
        "reference": "Experience with data modeling, data warehousing, and building ETL pipelines for the financial services industry and various asset classes"
    },
    {
        "skill": "Financial Services",
        "reference": "Experience with data modeling, data warehousing, and building ETL pipelines for the financial services industry and various asset classes"
    },
    {
        "skill": "Salary Range",
        "reference": "Salary Range"
    },
    {
        "skill": "$150,000-$250,000",
        "reference": "Salary Range"
    },
    {
        "skill": "Scala/Spark",
        "reference": "Data Engineer (scala/spark and AWS).Remote and long-term.Must work est time.Data Engineer Requirements 3+ years of experience building scalable data pipelines with Scala and Spark"
    },
    {
        "skill": "AWS",
        "reference": "Data Engineer (scala/spark and AWS).Remote and long-term.Must work est time.Data Engineer Requirements 3+ years of experience building scalable data pipelines with Scala and Spark"
    },
    {
        "skill": "Data pipelines",
        "reference": "Data Engineer (scala/spark and AWS).Remote and long-term.Must work est time.Data Engineer Requirements 3+ years of experience building scalable data pipelines with Scala and Spark"
    },
    {
        "skill": "Scala programming",
        "reference": "Data Engineer Requirements Strong Scala programming skills and knowledge of functional programming Experience with Spark Scala, DataFrames, Datasets, and Hadoop Filesystem"
    },
    {
        "skill": "Functional programming",
        "reference": "Data Engineer Requirements Strong Scala programming skills and knowledge of functional programming Experience with Spark Scala, DataFrames, Datasets, and Hadoop Filesystem"
    },
    {
        "skill": "Spark Scala",
        "reference": "Data Engineer Requirements Strong Scala programming skills and knowledge of functional programming Experience with Spark Scala, DataFrames, Datasets, and Hadoop Filesystem"
    },
    {
        "skill": "Datasets",
        "reference": "Data Engineer Requirements Strong Scala programming skills and knowledge of functional programming Experience with Spark Scala, DataFrames, Datasets, and Hadoop Filesystem"
    },
    {
        "skill": "Hadoop Filesystem",
        "reference": "Data Engineer Requirements Strong Scala programming skills and knowledge of functional programming Experience with Spark Scala, DataFrames, Datasets, and Hadoop Filesystem"
    },
    {
        "skill": "AWS services",
        "reference": "Familiarity with CI/CD best practices and experience with Jenkins, Git, and Azure DevOps Ability to optimize Spark jobs for performance and cost efficiency"
    },
    {
        "skill": "CI/CD best practices",
        "reference": "Familiarity with CI/CD best practices and experience with Jenkins, Git, and Azure DevOps Ability to optimize Spark jobs for performance and cost efficiency"
    },
    {
        "skill": "Jenkins",
        "reference": "Familiarity with CI/CD best practices and experience with Jenkins, Git, and Azure DevOps Ability to optimize Spark jobs for performance and cost efficiency"
    },
    {
        "skill": "Git",
        "reference": "Familiarity with CI/CD best practices and experience with Jenkins, Git, and Azure DevOps Ability to optimize Spark jobs for performance and cost efficiency"
    },
    {
        "skill": "Azure DevOps",
        "reference": "Familiarity with CI/CD best practices and experience with Jenkins, Git, and Azure DevOps Ability to optimize Spark jobs for performance and cost efficiency"
    },
    {
        "skill": "Supporting production data pipelines",
        "reference": "Ability to support production data pipelines and ETL processes Excellent communication skills and ability to quickly ramp up on our tech stack"
    },
    {
        "skill": "ETL processes",
        "reference": "Ability to support production data pipelines and ETL processes Excellent communication skills and ability to quickly ramp up on our tech stack"
    },
    {
        "skill": "Excellent communication skills",
        "reference": "Ability to support production data pipelines and ETL processes Excellent communication skills and ability to quickly ramp up on our tech stack"
    },
    {
        "skill": "Data Engineering, Architecting Data Warehousing, Data Tooling",
        "reference": "8+ years of data engineering experience; Previous experience building out data warehousing, data pipelines, and internal analytics"
    },
    {
        "skill": "Communication, Stakeholder Collaboration, Cross-Functional Problem Solving",
        "reference": "Strong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, Elasticsearch; The ability to communicate cross-functionally with various stakeholders to derive requirements"
    },
    {
        "skill": "System Design, Code Contribution, Product Feature Ownership",
        "reference": "75% doing system design and contributing code, starting with shipping code within 2 weeks; Leverage a self-starter mindset by taking a product concept and building the feature end to end (whether it\u2019s a component of the system or a significant piece of functionality)"
    },
    {
        "skill": "Data Engineering, Analytics",
        "reference": "Responsibilities include developing and supporting new and existing quantitative products and datasets."
    },
    {
        "skill": "Python/SQL/NoSQL",
        "reference": "Strong analytical, data and programming skills (Python/SQL/NoSQL/JavaScript) required."
    },
    {
        "skill": "Cloud Computing, AWS",
        "reference": "1+ year of experience with cloud computing services, AWS preferred."
    },
    {
        "skill": "Data Integration",
        "reference": "The Data Integration Engineer is responsible for building and maintaining core data integration platforms, primarily involving cleaning, parsing, storing, enriching, and validating data."
    },
    {
        "skill": "Parsing",
        "reference": "The Data Integration Engineer is responsible for building and maintaining core data integration platforms, primarily involving cleaning, parsing, storing, enriching, and validating data."
    },
    {
        "skill": "Database Management",
        "reference": "The Data Integration Engineer is responsible for building and maintaining core data integration platforms, primarily involving cleaning, parsing, storing, enriching, and validating data."
    },
    {
        "skill": "Problem Solving",
        "reference": "The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills, and display a passion for learning new methods and technologies."
    },
    {
        "skill": "Critical Thinking",
        "reference": "The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills, and display a passion for learning new methods and technologies."
    },
    {
        "skill": "Learning New Technologies",
        "reference": "The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills, and display a passion for learning new methods and technologies."
    },
    {
        "skill": "Collaboration",
        "reference": "Tasks will vary in complexity and scope, with examples ranging from service endpoint integrations to collaborating on larger team projects designed for Machine Learning and Analytics consumption."
    },
    {
        "skill": "Machine Learning",
        "reference": "Tasks will vary in complexity and scope, with examples ranging from service endpoint integrations to collaborating on larger team projects designed for Machine Learning and Analytics consumption."
    },
    {
        "skill": "Analytics",
        "reference": "Tasks will vary in complexity and scope, with examples ranging from service endpoint integrations to collaborating on larger team projects designed for Machine Learning and Analytics consumption."
    },
    {
        "skill": "GenRocket",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work"
    },
    {
        "skill": "scripting",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work"
    },
    {
        "skill": "PostGres/DB ability",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work"
    },
    {
        "skill": "API work",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform"
    },
    {
        "skill": "Determine best practice",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform"
    },
    {
        "skill": "Agile Mindset",
        "reference": "Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "Azure Cloud ADO experience",
        "reference": "Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "python or powershell scripting",
        "reference": "Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "Azure Data Factory, SQL, Stored Procedures, Python, Databricks, Snowflake, Azure Synapse Analytics, Power BI",
        "reference": "Position Overview: As a Data Engineer at Tail Wind you will be responsible for designing, developing, and maintaining data pipelines and systems on the Azure cloud platform."
    },
    {
        "skill": "Data pipelines, ETL processes, Data transformation, Data orchestration, Data warehousing",
        "reference": "Key Responsibilities: Design, build, and maintain data pipelines and ETL processes using Azure Data Factory. Develop and optimize SQL queries, stored procedures, and scripts for data transformation and extraction."
    },
    {
        "skill": "Data quality checks, Validation, Reporting, Dashboard creation",
        "reference": "Implement data quality checks and data validation processes to ensure data accuracy and consistency. Utilize Databricks for advanced data processing, transformation, and analytics. Create interactive reports and dashboards using Power BI for data visualization and insights."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About Us"
    },
    {
        "skill": "Data Platform Team",
        "reference": "About Us"
    },
    {
        "skill": "Develop, deploy and support data systems",
        "reference": "What You'll Do"
    },
    {
        "skill": "Automate and performance tune",
        "reference": "What You'll Do"
    },
    {
        "skill": "Build scalable data pipelines",
        "reference": "Your initial areas of focus will include"
    },
    {
        "skill": "Work within AWS account and VPC",
        "reference": "Your initial areas of focus will include"
    },
    {
        "skill": "Documentation of data pipelines",
        "reference": "What You Need to Get the Job Done"
    },
    {
        "skill": "Automate load, scaling, and performance testing",
        "reference": "Your initial areas of focus will include"
    },
    {
        "skill": "Monitor, operate, and optimize data platforms",
        "reference": "What You Need to Get the Job Done"
    },
    {
        "skill": "Data privacy and security",
        "reference": "What You Need to Get the Job Done"
    },
    {
        "skill": "Selenium, Databricks",
        "reference": "Candidate must have Selenium and Databricks experience to be considered."
    },
    {
        "skill": "Data Quality Engineer (Databricks)",
        "reference": "Job: Data Quality Engineer (Databricks)"
    },
    {
        "skill": "8+ months, Remote, 100%",
        "reference": "Location: Remote 100%, Term: 8+ months. Likely to extend to 12+ months."
    },
    {
        "skill": "Scala ML",
        "reference": "Writing and maintaining Scala ML code, with some additional Python"
    },
    {
        "skill": "Data Science Knowledge",
        "reference": "Extensive knowledge of Scala in a production environment"
    },
    {
        "skill": "AI Solution Implementation",
        "reference": "Proven history of implementing AI solutions for customers, with quantifiable results"
    },
    {
        "skill": "Cloudera Data Platform, Migration Expertise",
        "reference": "We need a Cloudera expert who will help us with the migration so Cloudera CDP expertise & hands-on experience is a must."
    },
    {
        "skill": "Architecting Data Platforms, Scalable Code",
        "reference": "This position is responsible for architecting and building a next generation data platform using reusable, scalable code with the ability to scale very large data volumes."
    },
    {
        "skill": "Assess Challenges, Offer Solutions",
        "reference": "The position requires the ability to assess the current project's challenges and offer solutions leading to the launch of the next generation product."
    },
    {
        "skill": "Data Integration, Design, Development, Testing, Deployment",
        "reference": "JOB DUTIES/RESPONSIBILITIES: Designs, develops, tests, and deploys data integration solutions with a focus on financial data accuracy and integrity"
    },
    {
        "skill": "Collaboration, Cross-functional Teamwork",
        "reference": "Works with stakeholders and cross-functional teams in the Finance Business Unit to understand financial data requirements and develops financial data models that can be used for further analysis"
    },
    {
        "skill": "Evaluation, Implementation of Tools, Technologies",
        "reference": "Evaluates, selects, and implements appropriate data integration tools and technologies, with a focus on financial data security and efficiency"
    },
    {
        "skill": "Scala/Spark, Cloud Data Platforms (Databricks), Azure",
        "reference": "Strong technical experience in Scala and Spark for designing , creating and maintaining applications. Experience with Cloud Data Platforms like Databricks. Experience with Azure cloud"
    },
    {
        "skill": "ETL Routines, SQL, Large Database (100M+ datasets)",
        "reference": "Experience implementing ETL routines with 100M+ datasets in a performant and scalable manner; Strong SQL skills with commensurate experience in a large database platform"
    },
    {
        "skill": "Relational Databases, SDLC, Agile Methodology",
        "reference": "Experience with relational database design (MySQL, Postgres etc); Experience in complete SDLC process and Agile Methodology; Strong oral and written communication skills"
    },
    {
        "skill": "Data Engineering, Architecture, and Scalability",
        "reference": "EvenUp needs to scale out data warehousing, data tooling and internal analytics. We need engineering leaders to help drive our vision. 10x pipeline processing throughput in 12 months, rethinking & rebuilding data ingestion."
    },
    {
        "skill": "Strong Communication and Collaboration",
        "reference": "The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions. Enjoying owning a project from start to finish and driving projects across the line."
    },
    {
        "skill": "Cross-Functional Teamwork and Mentorship",
        "reference": "Collaborating with stakeholders, mentoring, lunch & learns, and more. Leverage a self-starter mindset by taking a product concept end to end."
    },
    {
        "skill": "Data Integration, Parsing, Validating",
        "reference": "The Data Integration Engineer is primarily responsible for building and maintaining core data integration platforms."
    },
    {
        "skill": "Problem Solving, Critical Thinking",
        "reference": "The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills, and display a passion for learning new methods and technologies."
    },
    {
        "skill": "Collaboration, Cross-Departmental Work",
        "reference": "Collaborate with other technical resources and cross-department staff on various projects"
    },
    {
        "skill": "Data Engineering",
        "reference": "Sr. Data Engineer to build cutting-edge technology solutions"
    },
    {
        "skill": "Consulting Experience",
        "reference": "Demonstrated experience leading clients through the process of building modern data estates"
    },
    {
        "skill": "Leadership and Mentorship",
        "reference": "Responsible for overseeing and mentoring junior developers within the organization"
    },
    {
        "skill": "Data Engineering, Agile Methodology, Technical Design",
        "reference": "Title: Data Engineer IILocation: Pittsburgh, PADuration: 6+ MonthThis role will also be charged with understanding and interpreting requirements to contribute to the technical architecture and the associated design documents."
    },
    {
        "skill": "Coding, Testing, Debugging",
        "reference": "Primary Duties And ResponsibilitiesWriting, debugging, unit testing, and performance test code in the data access layer in accordance with Client's standards.As an agile team member, participate in code reviews, design reviews, etc."
    },
    {
        "skill": "Familiarity with Linux, Software Development Methodologies, SQL",
        "reference": "Strong understanding and familiarity working in the Linux operating environment.Familiarity and experience executing several software development methodologies and life cycles preferred.5+ years of SQL"
    },
    {
        "skill": "SQL",
        "reference": "Strong knowledge of SQL to aid in data visualization"
    },
    {
        "skill": "Data Visualization",
        "reference": "Strong knowledge of SQL to aid in data visualization"
    },
    {
        "skill": "Hadoop tools",
        "reference": "Working knowledge of Hadoop tools, such as Spark, Impala, Hue, and Kafka, to create, query, and manage ETL data flows"
    },
    {
        "skill": "ETL data flows",
        "reference": "Working knowledge of Hadoop tools, such as Spark, Impala, Hue, and Kafka, to create, query, and manage ETL data flows"
    },
    {
        "skill": "Data connectors",
        "reference": "Knowledge of Tableau Server and/or Microsoft PowerBIPython experience is a bonus"
    },
    {
        "skill": "Tableau Server",
        "reference": "Knowledge of Tableau Server and/or Microsoft PowerBIPython experience is a bonus"
    },
    {
        "skill": "PowerBI",
        "reference": "Knowledge of Tableau Server and/or Microsoft PowerBIPython experience is a bonus"
    },
    {
        "skill": "SharePoint",
        "reference": "Knowledge of Tableau Server and/or Microsoft PowerBIPython experience is a bonus"
    },
    {
        "skill": "PL/SQL Development, Oracle 19, Airflow, Git, Python Coding, REST API",
        "reference": "Expert In PL/SQL Development Work Experience With Oracle 19, Must have working experience with tools like Airflow, Git"
    },
    {
        "skill": "Accounting and Financial Data",
        "reference": "Plus Would Be Experience With Accounting And Financial Data"
    },
    {
        "skill": "Self-managing tasks, learning new technologies",
        "reference": "Candidates need to have the drive to learn new technologies, and can pick up new technologies rapidly, able to self-manage a growing list of tasks and priorities."
    },
    {
        "skill": "Data Engineering",
        "reference": "Title: Data Engineer"
    },
    {
        "skill": "8+ Years Experience",
        "reference": "Experience: 8+ years"
    },
    {
        "skill": "USC Only Visa",
        "reference": "Visa: USC Only"
    },
    {
        "skill": "IT/IS Experience",
        "reference": "Minimum of 5-10 years of IT/IS experience"
    },
    {
        "skill": "Communication Skills",
        "reference": "Need to have excellent communication skills"
    },
    {
        "skill": "SQL/PLSQL",
        "reference": "5+ years working with Advanced Structured Query Language (SQL) & PL/SQL"
    },
    {
        "skill": "Oracle Relational Database Experience",
        "reference": "5+ years experience with at least Oracle relational database"
    },
    {
        "skill": "Software Development Lifecycle",
        "reference": "5+ years experience in software development lifecycle activities"
    },
    {
        "skill": "Data Loading (ETL/ELT)",
        "reference": "5+ years experience with data loading (ETL, ELT)"
    },
    {
        "skill": "Working with Data at Scale",
        "reference": "5+ years working with data at scale 50+ TB"
    },
    {
        "skill": "Data Warehouses, ODS, Hubs",
        "reference": "Experience with data warehouses, operational data stores, data hubs"
    },
    {
        "skill": "Near-Real Time & Batch Data Pipelines Design",
        "reference": "Experience in end-to-end design of near-real-time and batch data pipelines"
    },
    {
        "skill": "Agile Environment Experience",
        "reference": "Experience working in an Agile environment"
    },
    {
        "skill": "Systems Design & Test Plan",
        "reference": "Experience developing detailed systems design and written test plans"
    },
    {
        "skill": "Installation Procedures Coordination",
        "reference": "Experience preparing installation instructions and coordinating installation procedures"
    },
    {
        "skill": "Data Audit, Archiving, Restoration Processes",
        "reference": "Experience documenting data audits, archiving, and restoration processes"
    },
    {
        "skill": "Version Control Git Experience",
        "reference": "Experience with version control systems Git"
    },
    {
        "skill": "Tracking & Ticket Software Familiarity",
        "reference": "Experience with tracking and ticket software (Jira, Confluence, etc.)"
    },
    {
        "skill": "Data Architecture, Integration Knowledge",
        "reference": "Familiarity with data architecture, data integration, data governance, and data lineage concepts"
    },
    {
        "skill": "Leadership & Health Care Experience (Nice to Have)",
        "reference": "NICE TO HAVE Leading a team / Health care experience / Professional certifications / Other Programming Language Experience / Experience in Machine learning or Artificial Intelligence / Familiarity with microservices / Familiarity with DevSecOps / 2+ years working with one NoSQL database (Hadoop) / Experience with MongoDB, Redshift, Synapse, or others is a plus. / Experience with containerization (Docker, Kubernetes, etc.) / Familiarity with agile and lean methodologies / Familiarity with JSON, XML"
    },
    {
        "skill": "Personal Qualities",
        "reference": "Welcomes new approaches and innovative thinking / Self-organized and responsible with experience in a distributed team / Able to multitask and be responsive/flexible to support customers / Ability to work with others from diverse skill sets and backgrounds / Takes ownership of a situation and sees it through the completion / Able to switch context and complete work processes"
    },
    {
        "skill": "Data Warehousing",
        "reference": "Design and implement enterprise data warehouse models (e.g., data vault, data mart), advance data platform architecture, and reliable data pipelines."
    },
    {
        "skill": "Cloud Technologies",
        "reference": "Work hands-on with multiple cloud technologies and tools such as Python, PySpark, AWS, GCP, Databricks."
    },
    {
        "skill": "Data Architecture",
        "reference": "5+ years of experience in data architecture and engineering, expertise on Spark (Databricks) and Data Platform Architecture in AWS and/or GCP."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "As a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses."
    },
    {
        "skill": "Data Platform Development, Deployment, Support",
        "reference": "Your ability to automate, performance tune, and scale the data platform will be key to your success."
    },
    {
        "skill": "Collaboration, Stakeholder Engagement",
        "reference": "Collaborate with stakeholders to make effective use of core data assets"
    },
    {
        "skill": "Data Engineer",
        "reference": "4.Data Engineer: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, and data modeling."
    },
    {
        "skill": "Proficiency in data warehousing, ETL (Extract, Transform, Load)",
        "reference": "4.Data Engineer: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, and data modeling."
    },
    {
        "skill": "Data modeling",
        "reference": "4.Data Engineer: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, and data modeling."
    },
    {
        "skill": "Strong knowledge of databases",
        "reference": "4.Data Engineer: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, and data modeling."
    },
    {
        "skill": "SQL and NoSQL",
        "reference": "4.Data Engineer: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, and data modeling."
    },
    {
        "skill": "Experience with big data technologies like Hadoop and Spark",
        "reference": "4.Data Engineer: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, and data modeling."
    },
    {
        "skill": "SQL",
        "reference": "A passion for technology 5+ years of experience with SQL and Python, 5+ years of experience in the Data Eng/Science space"
    },
    {
        "skill": "Python",
        "reference": "A passion for technology 5+ years of experience with SQL and Python, 5+ years of experience in the Data Eng/Science space"
    },
    {
        "skill": "5+ years of experience in Data Eng/Science space",
        "reference": "A passion for technology 5+ years of experience with SQL and Python, 5+ years of experience in the Data Eng/Science space"
    },
    {
        "skill": "BS in Computer Science, Math or equivalent",
        "reference": "BS in Computer Science, Math, a related technical field or equivalent experience, Strong foundations in databases, warehousing, data infrastructure, ELT/ETL"
    },
    {
        "skill": "Strong foundations in databases, warehousing, data infrastructure",
        "reference": "BS in Computer Science, Math, a related technical field or equivalent experience, Strong foundations in databases, warehousing, data infrastructure, ELT/ETL"
    },
    {
        "skill": "Experience with orchestration platforms",
        "reference": "Bonus Points: Experience with orchestration platforms, Experience with metric and features stores, Expertise working with cloud-based data warehouse solutions (BigQuery, Snowflake), Knowledge and experience in the GTM and Finance space for B2B products"
    },
    {
        "skill": "Working cross-functionally",
        "reference": "Bonus Points: Experience with orchestration platforms, Experience with metric and features stores, Expertise working with cloud-based data warehouse solutions (BigQuery, Snowflake), Knowledge and experience in the GTM and Finance space for B2B products"
    },
    {
        "skill": "Knowledge and experience in the GTM & Finance space",
        "reference": "Bonus Points: Experience with orchestration platforms, Experience with metric and features stores, Expertise working with cloud-based data warehouse solutions (BigQuery, Snowflake), Knowledge and experience in the GTM and Finance space for B2B products"
    },
    {
        "skill": "Data pipeline architecture",
        "reference": "Create and maintain optimal data pipeline architecture"
    },
    {
        "skill": "Internal process improvements",
        "reference": "Identify, design, and implement internal process improvements"
    },
    {
        "skill": "Optimizing data delivery",
        "reference": "Help build the infrastructure required for optimal extraction, transformation, and loading of data from various sources"
    },
    {
        "skill": "Data analytics",
        "reference": "Build analytics tools that utilize data pipeline for insights into business performance metrics"
    },
    {
        "skill": "Support stakeholders",
        "reference": "Assist with stakeholder technical issues and support their infrastructure needs"
    },
    {
        "skill": "Relevant work experience",
        "reference": "A minimum of 2 years of relevant work experience"
    },
    {
        "skill": "Advanced SQL, MySQL development",
        "reference": "Advanced working SQL, and MySQL development experience"
    },
    {
        "skill": "PowerBi, Domo, or related BI products",
        "reference": "Experience with PowerBi, Domo, or related BI products"
    },
    {
        "skill": "BI/Data visualization tools",
        "reference": "Working experience with Azure preferred and/or AWS. Experience in data warehousing and business intelligence"
    },
    {
        "skill": "Excellent computer science fundamentals",
        "reference": "Excellent computer science fundamentals and problem-solving skills"
    },
    {
        "skill": "Creating logical and physical models",
        "reference": "Experience creating logical and physical data models"
    },
    {
        "skill": "Root cause analysis",
        "reference": "Perform root cause analysis on internal and external data to answer business questions and identify opportunities"
    },
    {
        "skill": "Multiple programming languages",
        "reference": "Experience with multiple programming languages"
    },
    {
        "skill": "Analytical skills for unstructured datasets",
        "reference": "Strong analytic skills related to working with unstructured datasets"
    },
    {
        "skill": "IT Expertise",
        "reference": "Technical expertise with data models, data mining, and segmentation techniques"
    },
    {
        "skill": "Analytical",
        "reference": "Great numerical and analytical skills"
    },
    {
        "skill": "Results Driven - Competitive Awareness",
        "reference": "Stays informed of industry trends and technology of the competitive landscape."
    },
    {
        "skill": "Communication - Listening Ability",
        "reference": "Listening Ability to accurately receive and interpret messages."
    },
    {
        "skill": "Interpersonal Skills - Build Trust",
        "reference": "Build a common identity with others that aligns with company goals, fosters mutual respect and integrity."
    },
    {
        "skill": "Results Driven - Time Utilization",
        "reference": "Uses time effectively while concentrating on more important priorities."
    },
    {
        "skill": "Takes Measured Risks",
        "reference": "Willing to make difficult decisions, understanding when to gain consensus and assess potential outcomes."
    },
    {
        "skill": "Infirmatica MDM",
        "reference": "Core skill set: Infirmatica MDM and Informatic ETL tool"
    },
    {
        "skill": "Informatic ETL tool",
        "reference": "Core skill set: Infirmatica MDM and Informatic ETL tool"
    },
    {
        "skill": "ETL tools with strong SQL skills",
        "reference": "Experience in ETL tools with strong SQL skills, minimum 5+ years"
    },
    {
        "skill": "Data warehouse",
        "reference": "Create and maintain data model standards, including master data management (MDM) and experience in creating pipelines for Data warehouse"
    },
    {
        "skill": "Snowflake",
        "reference": "Create and maintain data model standards, including master data management (MDM) and experience in creating pipelines for Data warehouse"
    },
    {
        "skill": "MDM",
        "reference": "Create and maintain data model standards, including master data management (MDM) and experience in creating pipelines for Data warehouse"
    },
    {
        "skill": "Metadata",
        "reference": "Capture, validate, and publish metadata by enterprise data governance policies and MDM taxonomies"
    },
    {
        "skill": "Enterprise data governance policies",
        "reference": "Capture, validate, and publish metadata by enterprise data governance policies and MDM taxonomies"
    },
    {
        "skill": "MDM taxonomies",
        "reference": "Capture, validate, and publish metadata by enterprise data governance policies and MDM taxonomies"
    },
    {
        "skill": "Backend programming",
        "reference": "Experience in backend programming including schema and table design, stored procedures, Triggers, Views, and Indexes"
    },
    {
        "skill": "Schema design",
        "reference": "Experience in backend programming including schema and table design, stored procedures, Triggers, Views, and Indexes"
    },
    {
        "skill": "Table design",
        "reference": "Experience in backend programming including schema and table design, stored procedures, Triggers, Views, and Indexes"
    },
    {
        "skill": "Stored procedures",
        "reference": "Experience in backend programming including schema and table design, stored procedures, Triggers, Views, and Indexes"
    },
    {
        "skill": "Triggers",
        "reference": "Experience in backend programming including schema and table design, stored procedures, Triggers, Views, and Indexes"
    },
    {
        "skill": "Views",
        "reference": "Experience in backend programming including schema and table design, stored procedures, Triggers, Views, and Indexes"
    },
    {
        "skill": "Indexes",
        "reference": "Experience in backend programming including schema and table design, stored procedures, Triggers, Views, and Indexes"
    },
    {
        "skill": "Data analysis",
        "reference": "Conduct data analysis, mapping transformation, data modeling and data-warehouse concepts"
    },
    {
        "skill": "Mapping transformation",
        "reference": "Conduct data analysis, mapping transformation, data modeling and data-warehouse concepts"
    },
    {
        "skill": "Data modeling",
        "reference": "Conduct data analysis, mapping transformation, data modeling and data-warehouse concepts"
    },
    {
        "skill": "Data-warehouse concepts",
        "reference": "Conduct data analysis, mapping transformation, data modeling and data-warehouse concepts"
    },
    {
        "skill": "Agile",
        "reference": "Strong working Experience with Agile, Scrum, Kanban, and Waterfall methodologies"
    },
    {
        "skill": "Scrum",
        "reference": "Strong working Experience with Agile, Scrum, Kanban, and Waterfall methodologies"
    },
    {
        "skill": "Kanban",
        "reference": "Strong working Experience with Agile, Scrum, Kanban, and Waterfall methodologies"
    },
    {
        "skill": "Waterfall methodologies",
        "reference": "Strong working Experience with Agile, Scrum, Kanban, and Waterfall methodologies"
    },
    {
        "skill": "Data Extraction, Transformation and Loading",
        "reference": "Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources."
    },
    {
        "skill": "Creating Data Collection Frameworks",
        "reference": "Creates data collection frameworks for structured and unstructured data."
    },
    {
        "skill": "Infrastructure Systems Development",
        "reference": "Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs."
    },
    {
        "skill": "ETL, SQL, Object-Oriented Programming",
        "reference": "Designs and implements software & support solutions as a member of an agile squad. Software in scope are 3rd party software applications which support finance and regulatory reporting business functions."
    },
    {
        "skill": "Agile Software Development, Compliance & Governance",
        "reference": "General FunctionDesigns and implements software & support solutions as a member of an agile squad. Following best practices and standards and participates in communities of practice to continuously refine and document these standards, following any required compliance & governance requirements."
    },
    {
        "skill": "Kafka, Apache NIFI, Snowflake",
        "reference": "The individual will be responsible for building mechanisms to receive data from an external partner (file batch and real time events via kafka) and route to our downstream teams for reporting and operational processing. General FunctionDesigns and implements software & support solutions as a member of an agile squad."
    },
    {
        "skill": "Azure, Facet integration, Relational database engineering, Azure Data Factory, On-premises migration",
        "reference": "Job Description: Azure Claims Processing and Database Engineering"
    },
    {
        "skill": "Claims processing experience (Payer claim), Designing data pipelines, Relational Database Management",
        "reference": "About the job"
    },
    {
        "skill": "Facet Integration with Azure, Relational Database Engineering, On-premises migration to Data Warehouse using Azure, Performance Optimization",
        "reference": "Responsibilities and Requirements of the Job Description"
    },
    {
        "skill": "Data Engineering",
        "reference": "The Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making."
    },
    {
        "skill": "Business Management",
        "reference": "The Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making."
    },
    {
        "skill": "Strategy and Transformation Management",
        "reference": "Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management."
    },
    {
        "skill": "Acquisition Management",
        "reference": "Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management."
    },
    {
        "skill": "Data Architecture",
        "reference": "By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Data Engineer will significantly contribute to the optimization and innovation of organizational processes."
    },
    {
        "skill": "IT Expertise",
        "reference": "By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Data Engineer will significantly contribute to the optimization and innovation of organizational processes."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "Skiltrek is looking for a Senior Data Engineer to join one of the largest digital credential service companies in the nation."
    },
    {
        "skill": "Digital Credential Service Companies",
        "reference": "Skiltrek is looking for a Senior Data Engineer to join one of the largest digital credential service companies in the nation."
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "Skiltrek is looking for a Senior Data Engineer to join one of the largest digital credential service companies in the nation."
    },
    {
        "skill": "Data Focused",
        "reference": "Data has become a primary focus at this organization."
    },
    {
        "skill": "Key Role",
        "reference": "Data has become a primary focus at this organization."
    },
    {
        "skill": "Maintain and Develop Data Pipelines",
        "reference": "Data has become a primary focus at this organization."
    },
    {
        "skill": "Highly Motivated Data Engineer",
        "reference": "They're seeking a highly motivated data engineer for a key role in maintaining and developing data pipelines for data products and analytics."
    },
    {
        "skill": "Reporting to Data Analytics and Science Manager",
        "reference": "They're seeking a highly motivated data engineer for a key role in maintaining and developing data pipelines for data products and analytics."
    },
    {
        "skill": "Remote role",
        "reference": "About the job"
    },
    {
        "skill": "Need photo ID",
        "reference": "About the job"
    },
    {
        "skill": "Valid LinkedIn",
        "reference": "Remote role, Need Valid LinkedIn, Need photo ID"
    },
    {
        "skill": "Clearance",
        "reference": "Remote role, Need Valid LinkedIn, Need photo ID"
    },
    {
        "skill": "Development experience",
        "reference": "Remote role, Need Valid LinkedIn, Need photo ID"
    },
    {
        "skill": "7+ years",
        "reference": "Must be able to obtain a clearance, 7+ years of development experience building data pipelines using Cloud technologies."
    },
    {
        "skill": "Data pipelines",
        "reference": "Must be able to obtain a clearance, 7+ years of development experience building data pipelines using Cloud technologies."
    },
    {
        "skill": "Cloud technologies",
        "reference": "Must be able to obtain a clearance, 7+ years of development experience building data pipelines using Cloud technologies."
    },
    {
        "skill": "5+ years",
        "reference": "5+ years of experience in architecture of modern data warehousing"
    },
    {
        "skill": "Modern data warehousing",
        "reference": "5+ years of experience in architecture of modern data warehousing"
    },
    {
        "skill": "Platforms",
        "reference": "5+ years of experience in architecture of modern data warehousing"
    },
    {
        "skill": "Snowflake or Redshift",
        "reference": "5+ platforms using technologies such as Snowflake or Redshift, Cloud experience \u2013 S3, step functions, Glue, Step functions and Airflow."
    },
    {
        "skill": "Cloud experience",
        "reference": "5+ platforms using technologies such as Snowflake or Redshift, Cloud experience \u2013 S3, step functions, Glue, Step functions and Airflow."
    },
    {
        "skill": "Python development",
        "reference": "Good Python development for data transfers and extractions (ELT and ETL)."
    },
    {
        "skill": "Data transfers",
        "reference": "Good Python development for data transfers and extractions (ELT and ETL)."
    },
    {
        "skill": "Extractions (ELT & ETL)",
        "reference": "Good Python development for data transfers and extractions (ELT and ETL)."
    },
    {
        "skill": "BigQuery, Composer, Python, GCP Fundamentals",
        "reference": "Mandatory Skills - BigQuery ,Composer, Python, GCP Fundamentals"
    },
    {
        "skill": "Snowflake, DLP, Pub/Sub, Dataflow, Shell Scripting, SQL, Security(Platform & Data) concepts",
        "reference": "Secondary Skills - Snowflake, DLP, Pub/Sub, Dataflow, Shell Scripting, SQL, Security(Platform & Data) concepts"
    },
    {
        "skill": "Knowledge of ETL Migration from On-Premises to GCP CloudSQL Performance Tuning Batch/Streaming Data Processing Fundamentals of Kafka, Pub/Sub",
        "reference": "Fundamental skills - Knowledge of ETL Migration from On-Premises to GCP CloudSQL Performance Tuning Batch/Streaming Data Processing Fundamentals of Kafka, Pub/Sub"
    },
    {
        "skill": "Certifications in any of the following: GCP Professional Cloud Architect, GCP Professional Data Engineer",
        "reference": "Good To Have - Certifications in any of the following: GCP Professional Cloud Architect, GCP Professional Data Engineer"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Data Engineer at Treez, you will be responsible for designing the data architecture for our organization."
    },
    {
        "skill": "Architecture",
        "reference": "As a Data Engineer at Treez, you will be responsible for designing the data architecture for our organization."
    },
    {
        "skill": "Collaboration",
        "reference": "Collaborate with cross-functional teams, including engineers, analysts, and business stakeholders to understand data requirements."
    },
    {
        "skill": "Mentorship",
        "reference": "Collaborate with cross-functional teams, including engineers, analysts, and business stakeholders to understand data requirements."
    },
    {
        "skill": "Communication",
        "reference": "Ability to work independently and manage multiple priorities in a fast-paced environment."
    },
    {
        "skill": "Critical Thinking",
        "reference": "Ability to work independently and manage multiple priorities in a fast-paced environment."
    },
    {
        "skill": "Cloud Engineering, Python, Software Engineering",
        "reference": "2+ years of experience with Cloud Engineering / Services 3+ years of work experience as a backend software engineer in Python with exceptional software engineering knowledge."
    },
    {
        "skill": "ML Workflow Orchestration Tools, DevOps Experience, ML Frameworks",
        "reference": "Experience in DevOps: Jenkins/Tekton etc. Experience with ML workflow orchestration tools: Airflow, Kubeflow etc. Experience with cloud services, preferably GCP Services like Vertex AI, Cloud Function, BigQuery etc."
    },
    {
        "skill": "GCP Experience, Infrastructure as Code, Container Management",
        "reference": "Experience in container management solution: Kubernetes, Docker Experience in scripting language: Bash, PowerShell etc. Experience with Infrastructure as code: Terraform etc. Experience with Google Cloud platform(GCP) specifically Google Kubernetes engine, Terraform, and infrastructure."
    },
    {
        "skill": "Big Data Engineer",
        "reference": "Our client is looking for a true Mid-level Big Data Engineer to contribute."
    },
    {
        "skill": "Data Analysis & Algorithms Design",
        "reference": "Analyze system requirements and design responsive algorithms and solutions."
    },
    {
        "skill": "Cloud Technologies Expertise",
        "reference": "Use big data and cloud technologies to produce production quality code."
    },
    {
        "skill": "Data Pipelines, ETL Processes",
        "reference": "Design and develop robust data pipelines and ETL processes"
    },
    {
        "skill": "Data Infrastructure Optimization",
        "reference": "Optimize data infrastructure for performance, reliability, and scalability"
    },
    {
        "skill": "Data Quality & Integrity",
        "reference": "Ensure data quality, integrity, and security across the data lake"
    },
    {
        "skill": "SQL, R, Python",
        "reference": "Required Skills/Abilities SQL.R, Python.Collibra or other Governance / Metadata management tool Tableau, and one other BI analytics tool. Strong data analysis background."
    },
    {
        "skill": "Data Analysis",
        "reference": "Required Skills/Abilities SQL.R, Python.Collibra or other Governance / Metadata management tool Tableau, and one other BI analytics tool. Strong data analysis background."
    },
    {
        "skill": "Data Quality Management",
        "reference": "This role is the \u2018first line of defense\u2019 protecting the quality of data and serves as a liaison between the business and technical stakeholders to ensure functional data needs are supported by a technical quality framework."
    },
    {
        "skill": "Claims Data Engineer",
        "reference": "Experienced, solution-focused Claims Data Engineer to support our data insights and analytics efforts"
    },
    {
        "skill": "Data Pipeline Design",
        "reference": "Expand capabilities for ingesting and organizing medical and pharmacy claims data"
    },
    {
        "skill": "Collaboration",
        "reference": "A team player, empathetic, security-oriented, comfortable with a distributed workforce"
    },
    {
        "skill": "data engineering",
        "reference": "Applicant must have 2 years of relevant experience with the following: Hands-on experience with data engineering design and implementation"
    },
    {
        "skill": "experience",
        "reference": "Applicant must have 2 years of relevant experience with the following: Hands-on experience with data engineering design and implementation"
    },
    {
        "skill": "relevant",
        "reference": "Applicant must have 2 years of relevant experience with the following: Hands-on experience with data engineering design and implementation"
    },
    {
        "skill": "data modeling",
        "reference": "Hands-on industry experience programming in SQL on relational database platforms (T-SQL and PL/SQL preferred)"
    },
    {
        "skill": "experience",
        "reference": "Hands-on industry experience programming in SQL on relational database platforms (T-SQL and PL/SQL preferred)"
    },
    {
        "skill": "implementation",
        "reference": "Hands-on industry experience programming in SQL on relational database platforms (T-SQL and PL/SQL preferred)"
    },
    {
        "skill": "ETL/ELT tools",
        "reference": "Experience with data modeling design and implementation"
    },
    {
        "skill": "experience",
        "reference": "Experience with data modeling design and implementation"
    },
    {
        "skill": "Azure Data Factory, Databricks preferred",
        "reference": "Experience with data modeling design and implementation"
    },
    {
        "skill": "modern languages",
        "reference": "Hands-on industry experience working with enterprise ETL/ELT tools (Azure Data Factory and Databricks preferred)"
    },
    {
        "skill": "hands-on experience",
        "reference": "Hands-on industry experience working with enterprise ETL/ELT tools (Azure Data Factory and Databricks preferred)"
    },
    {
        "skill": "Python preferred",
        "reference": "Hands-on industry experience working with enterprise ETL/ELT tools (Azure Data Factory and Databricks preferred)"
    },
    {
        "skill": "cloud platforms",
        "reference": "Hands-on experience with Azure, AWS, and/or GCP cloud platforms (Azure preferred)"
    },
    {
        "skill": "experience",
        "reference": "Hands-on experience with Azure, AWS, and/or GCP cloud platforms (Azure preferred)"
    },
    {
        "skill": "preferred\u2014Azure, AWS, GCP",
        "reference": "Hands-on experience with Azure, AWS, and/or GCP cloud platforms (Azure preferred)"
    },
    {
        "skill": "Bachelor's degree",
        "reference": "Progressive mindset particularly around deployment models and emerging technologies"
    },
    {
        "skill": "Computer Science",
        "reference": "Progressive mindset particularly around deployment models and emerging technologies"
    },
    {
        "skill": "similar field",
        "reference": "Progressive mindset particularly around deployment models and emerging technologies"
    },
    {
        "skill": "collaborative team player",
        "reference": "Collaborative team player who is detailed oriented, focused on solution quality and execution"
    },
    {
        "skill": "detail-oriented",
        "reference": "Collaborative team player who is detailed oriented, focused on solution quality and execution"
    },
    {
        "skill": "focused on solution quality and execution",
        "reference": "Collaborative team player who is detailed oriented, focused on solution quality and execution"
    },
    {
        "skill": "Docker",
        "reference": "Experience with Docker for containerization and Kubernetes for orchestration"
    },
    {
        "skill": "experience",
        "reference": "Experience with Docker for containerization and Kubernetes for orchestration"
    },
    {
        "skill": "Kubernetes",
        "reference": "Experience with Docker for containerization and Kubernetes for orchestration"
    },
    {
        "skill": "Data Engineering",
        "reference": "Build and support Cloud/On-Premises enterprise data infrastructure, design scalable solutions, data pipeline frameworks, and automation for data ingestion, processing, and delivery."
    },
    {
        "skill": "DevOps, Continuous Integration",
        "reference": "Experience with DevOps methodologies and continuous integration/continuous delivery, proficiency in Linux operations and development, basic commands, and shell scripting."
    },
    {
        "skill": "AWS Cloud Service, Python, SQL",
        "reference": "Proficiency in AWS Cloud Service, Python, and Sql for data profiling, analysis, and extraction. Experience with data visualization tools to describe data."
    },
    {
        "skill": "Data Platform Creation",
        "reference": "Support the creation of an impactful data platform, shaping the future of the business"
    },
    {
        "skill": "Advanced Technology Work",
        "reference": "Work with a mix of advanced technologies in a hybrid data estate"
    },
    {
        "skill": "Continuous Learning",
        "reference": "encourages continuous learning with substantial training time and support for skill enhancement"
    },
    {
        "skill": "Impactful Data Engineer Role",
        "reference": "Perfect for those who want to make their mark in a well-established yet forward-thinking business"
    },
    {
        "skill": "Data Pipeline Construction",
        "reference": "Help build and develop the new customer platform"
    },
    {
        "skill": "Problem Solving",
        "reference": "Proven ability in commercial data analytics development, coupled with a knack for problem-solving and outstanding communication skills"
    },
    {
        "skill": "Data Management Experience",
        "reference": "Hands-on experience with source data, including operational relational data models, business files and log streams"
    },
    {
        "skill": "Cloud Data Warehouses/Data Lakes Knowledge",
        "reference": "Experience in managing scheduled ETL tools, and a history of working with Cloud Data Warehouses/Data Lakes in AWS"
    },
    {
        "skill": "Salary Benefits",
        "reference": "Competitive salary of up to \u00a370,000 and an array of unique benefits that truly set them apart"
    },
    {
        "skill": "Flexible Working Arrangements",
        "reference": "Support flexible working arrangements to promote a healthy work-life balance"
    },
    {
        "skill": "Personal and Professional Development",
        "reference": "Committed to personal and professional development, offering mentorship and learning opportunities to fuel your growth too"
    },
    {
        "skill": "Data Infrastructure, Data Architecture",
        "reference": "Strong data infrastructure and data architecture skills"
    },
    {
        "skill": "Health Plan Data Experience",
        "reference": "Proven track record of technical leadership and experience with health plan data"
    },
    {
        "skill": "Project Management, Visionary",
        "reference": "Strong project management skills and a vision for how data can be an enabler"
    },
    {
        "skill": "Scalable Software Development",
        "reference": "Must-Haves: 10+ years of experience developing and delivering scalable, customer-facing enterprise software."
    },
    {
        "skill": "Customer-Facing Enterprise Software",
        "reference": "Must-Haves: 10+ years of experience developing and delivering scalable, customer-facing enterprise software."
    },
    {
        "skill": "Breadth of Coding Experience",
        "reference": "Not married to any technology (has breadth of coding experience & enjoys honing their craft) but also understands how to leverage latest tech to solve problems in a better way."
    },
    {
        "skill": "Latest Tech Leveraging",
        "reference": "Not married to any technology (has breadth of coding experience & enjoys honing their craft) but also understands how to leverage latest tech to solve problems in a better way."
    },
    {
        "skill": "Advanced Architecture Experience",
        "reference": "5+ years of experience designing, implementing and managing advanced architectures in a SaaS application domain or platform. Track record of delivering timely, high-quality features and functionality within an agile sprint environment."
    },
    {
        "skill": "Scaling Challenges Solving",
        "reference": "5+ years of experience designing, implementing and managing advanced architectures in a SaaS application domain or platform. Track record of delivering timely, high-quality features and functionality within an agile sprint environment."
    },
    {
        "skill": "Good Energy & Communication",
        "reference": "1 Good energy and communication skills, 2 A leadership mindset."
    },
    {
        "skill": "Leadership Mindset",
        "reference": "1 Good energy and communication skills, 2 A leadership mindset."
    },
    {
        "skill": "Comfort in Ambiguous Environments",
        "reference": "3 Comfort in ambiguous environments"
    },
    {
        "skill": "Data Engineer",
        "reference": "Required Skills Hands-on Data Engineer to code ETL ingestion pipelines using AWS tools-for ETL they are using API calls-Glue, Lambda"
    },
    {
        "skill": "Graph Databases Experience",
        "reference": "Required Skills Hands-on Data Engineer to code ETL ingestion pipelines using AWS tools-for ETL they are using API calls-Glue, Lambda"
    },
    {
        "skill": "Engineering functions",
        "reference": "Experience with Engineering functions like estimating, procurement, construction"
    },
    {
        "skill": "ETL ingestion pipelines",
        "reference": "They are using API calls-Glue, Lambda"
    },
    {
        "skill": "AWS tools",
        "reference": "They are using API calls-Glue, Lambda"
    },
    {
        "skill": "Graph databases",
        "reference": "Big plus is experience with Graph databases such as Neptune"
    },
    {
        "skill": "Neptune",
        "reference": "Big plus is experience with Graph databases such as Neptune"
    },
    {
        "skill": "Data Engineering Experience",
        "reference": "8+ years of data engineering experience"
    },
    {
        "skill": "Data Warehousing, Pipelines, Internal Analytics",
        "reference": "Previous experience building out data warehousing, data pipelines, and internal analytics"
    },
    {
        "skill": "Cross-Functional Communication",
        "reference": "The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions"
    },
    {
        "skill": "Data Science",
        "reference": "Have at least 4 years of experience in a Data Science or Data Engineering role, with a focus on instrumenting data collection, building data pipelines, data modelling and driving insights from complex data"
    },
    {
        "skill": "Data Engineering",
        "reference": "Have at least 4 years of experience in a Data Science or Data Engineering role, with a focus on instrumenting data collection, building data pipelines, data modelling and driving insights from complex data"
    },
    {
        "skill": "Data Analytics",
        "reference": "Identify the data needs of the engineering, product, and business teams, understand their specific requirements for metrics and analysis, then build efficient, scalable, accurate, and complete data pipelines to enable data-informed decisions across the company"
    },
    {
        "skill": "SQL",
        "reference": "Experience writing and maintaining frontend client applications, Angular preferred"
    },
    {
        "skill": "Azure Data Factory",
        "reference": "Experience writing and maintaining frontend client applications, Angular preferred"
    },
    {
        "skill": "ETL work",
        "reference": "Proven ability to complete projects in a timely manner while clearly measuring progress"
    },
    {
        "skill": "Timely Project Completion",
        "reference": "Proven ability to complete projects in a timely manner while clearly measuring progress"
    },
    {
        "skill": "Software Engineering Fundamentals",
        "reference": "Strong software engineering fundamentals and experience with at least one popular programming language"
    },
    {
        "skill": "Programming Language (.NET or Java)",
        "reference": "Strong software engineering fundamentals and experience with at least one popular programming language"
    },
    {
        "skill": "Frontend Client Applications",
        "reference": "Experience writing and maintaining frontend client applications, Angular preferred"
    },
    {
        "skill": "Angular",
        "reference": "Experience writing and maintaining frontend client applications, Angular preferred"
    },
    {
        "skill": "Revision Control",
        "reference": "Experience with revision control (Git)"
    },
    {
        "skill": "Git",
        "reference": "Experience with revision control (Git)"
    },
    {
        "skill": "Cloud-based Systems",
        "reference": "Experience with cloud-based systems (Azure / AWS / GCP)."
    },
    {
        "skill": "Azure/AWS/GCP",
        "reference": "Experience with cloud-based systems (Azure / AWS / GCP)."
    },
    {
        "skill": "Big Data Design",
        "reference": "High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns"
    },
    {
        "skill": "Data Normalization Patterns",
        "reference": "High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns"
    },
    {
        "skill": "Data Lake, Mesh, Warehouse",
        "reference": "High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns"
    },
    {
        "skill": "Queueing Technologies",
        "reference": "Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)"
    },
    {
        "skill": "Kafka/SNS/RabbitMQ",
        "reference": "Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)"
    },
    {
        "skill": "Metrics, Logging, Monitoring, Alerting",
        "reference": "Experience with Metrics, Logging, Monitoring and Alerting tools. Strong experience with use of RESTful APIs"
    },
    {
        "skill": "RESTful APIs",
        "reference": "Experience with Metrics, Logging, Monitoring and Alerting tools. Strong experience with use of RESTful APIs"
    },
    {
        "skill": "Strong Communication",
        "reference": "Strong communication skills. High level understanding of HL7 V2.x / FHIR based interface messages. Strong experience with use of RESTful APIs. High level understanding of system deployment tasks and technologies"
    },
    {
        "skill": "High Level Understanding",
        "reference": "Strong communication skills. High level understanding of HL7 V2.x / FHIR based interface messages. Strong experience with use of RESTful APIs. High level understanding of system deployment tasks and technologies"
    },
    {
        "skill": "System Deployment Tasks & Technologies",
        "reference": "Strong communication skills. High level understanding of HL7 V2.x / FHIR based interface messages. Strong experience with use of RESTful APIs. High level understanding of system deployment tasks and technologies"
    },
    {
        "skill": "CI/CD Pipeline",
        "reference": "Strong communication skills. High level understanding of HL7 V2.x / FHIR based interface messages. Strong experience with use of RESTful APIs. High level understanding of system deployment tasks and technologies"
    },
    {
        "skill": "Design",
        "reference": "Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Development",
        "reference": "Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Code Deployment",
        "reference": "Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Monitoring",
        "reference": "Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Develop, deploy and support data systems",
        "reference": "What You'll Do"
    },
    {
        "skill": "Performance tune and scale data platform",
        "reference": "Your ability to automate, performance tune, and scale the data platform will be key to your success."
    },
    {
        "skill": "Collaborate with stakeholders",
        "reference": "Collaborate with stakeholders to make effective use of core data assets"
    },
    {
        "skill": "Leverage various tools, engines, libraries and code for scalable pipelines",
        "reference": "Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assets. With Spark and Pyspark libraries, load both streaming and batched data."
    },
    {
        "skill": "Work within IT managed AWS account",
        "reference": "Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments"
    },
    {
        "skill": "Documentation of pipelines and infrastructure",
        "reference": "Documentation of data pipelines, cloud infrastructure, and standard operating procedures"
    },
    {
        "skill": "Express data platform as code",
        "reference": "Automate load, scaling, and performance testing of data platform pipelines and infrastructure"
    },
    {
        "skill": "Monitor and optimize data pipelines",
        "reference": "Help ensure appropriate data privacy and security. Automate continuous upgrades and testing of data platform infrastructure and services."
    },
    {
        "skill": "Data pipeline unit, integration, quality and performance tests",
        "reference": "Build data pipeline unit, integration, quality, and performance tests"
    },
    {
        "skill": "Peer code reviews, approvals and pull requests",
        "reference": "Participate in peer code reviews, code approvals, and pull requests. Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance."
    },
    {
        "skill": "Experience with various data storage formats",
        "reference": "Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and Kinesis. Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum"
    },
    {
        "skill": "Cloud infrastructure experience",
        "reference": "Experience working with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets manager. Understanding of security around cloud infrastructure and data systems"
    },
    {
        "skill": "Git-based team coding workflows",
        "reference": "Understanding of Git-based team coding workflows. Beyond the technical skills, we're looking for individuals who are clear communicators."
    },
    {
        "skill": "Lead Data Engineer",
        "reference": "As a lead data engineer"
    },
    {
        "skill": "ETL, Multi-model DB, SQL",
        "reference": "Creating and maintaining a scalable ETL data pipeline,"
    },
    {
        "skill": "AWS, PostgreSQL, Graph DBs",
        "reference": "Working with Data Science team to enable ML Ops"
    },
    {
        "skill": "Data Engineering",
        "reference": "In this role you will be responsible for building the data and reporting infrastructure to power our systems"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS."
    },
    {
        "skill": "ETL/ELT and Data Warehousing",
        "reference": "Experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran"
    },
    {
        "skill": "Programming Languages",
        "reference": "Expertise in one or more programming languages (ideally Python)"
    },
    {
        "skill": "SQL Expertise",
        "reference": "Strong SQL experience, with expert level skill in Postgres, Snowflake and/or AWS Redshift."
    },
    {
        "skill": "Data Pipeline Management",
        "reference": "Supporting self-service data pipeline management (ETL) and self-directed learning process"
    },
    {
        "skill": "Agile Practices",
        "reference": "Ability to implement, advocate, and teach agile practices, and adapt them to your people and circumstances"
    },
    {
        "skill": "Data Quality Tracking",
        "reference": "Implement systems for tracking data quality and consistency"
    },
    {
        "skill": "Developing Data Models & Schema",
        "reference": "Evolve data models & data schema based on business and engineering needs"
    },
    {
        "skill": "Data Engineering, Design & Build",
        "reference": "As a Data Engineer at Vevo, you will extend and maintain the data pipelines that feed our ever growing data lake."
    },
    {
        "skill": "Teamwork, Communication & Problem-solving",
        "reference": "This Describes You You believe in values like effectiveness over efficiency and quality over quantity. You desire to continuously improve your team, your product and yourself. You have pragmatic communication and problem-solving skills."
    },
    {
        "skill": "Data Analytics & Python",
        "reference": "You love different types of data. i.e. content metadata, viewership metrics, etc. You love to solve difficult and interesting problems using data from various systems. You have experience developing and maintaining software in Python."
    },
    {
        "skill": "AWS Data Integration Services",
        "reference": "In-depth knowledge of AWS Data Integration Services such as Glue, and experience with Microsoft SQL Server."
    },
    {
        "skill": "ETL and Analytical Reporting",
        "reference": "This position requires in-depth knowledge of AWS Data Integration Services such as Glue, as well as experience with Microsoft SQL Server."
    },
    {
        "skill": "Microsoft SQL Server, SSIS, MySQL",
        "reference": "Experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL."
    },
    {
        "skill": "Dimensional data modeling experience",
        "reference": "Experience in Dimensional Data Modeling Development for Healthcare Data Architecture Project"
    },
    {
        "skill": "Caboodle development experience",
        "reference": "Development Experience with Caboodle"
    },
    {
        "skill": "Azure Data Factory, SSIS or Data Lake development preferred",
        "reference": "SSIS, Azure Data Factory, or Data Lake Development Preferred"
    },
    {
        "skill": "Senior Data Engineer, Overseeing, Department's Data Integration",
        "reference": "Job Description"
    },
    {
        "skill": "Python, SQL, OLAP, OLTP, Datalakes, Data Pipelining, Cloud Development (AWS)",
        "reference": "Minimum Requirements"
    },
    {
        "skill": "Agile or Scrum Experience",
        "reference": "Minimum Requirements"
    },
    {
        "skill": "3-5 years Data Engineering, BA in Computer Science",
        "reference": "Relevant Experience"
    },
    {
        "skill": "Asset Management",
        "reference": "Asset Management domain exp needed, must have strong and extensive exp with DBT"
    },
    {
        "skill": "DBT",
        "reference": "Asset Management domain exp needed, must have strong and extensive exp with DBT"
    },
    {
        "skill": "Data Integration",
        "reference": "Asset Management domain exp needed, must have strong and extensive exp with DBT"
    },
    {
        "skill": "Snowflake",
        "reference": "Experience in Snowflake data modelling, ELT using snowpipe, implementing stored procedures and standard DWH and ETL concepts. Strong experience in DBT (data build tool), Snowflake and Python"
    },
    {
        "skill": "Python",
        "reference": "Experience in Snowflake data modelling, ELT using snowpipe, implementing stored procedures and standard DWH and ETL concepts. Strong experience in DBT (data build tool), Snowflake and Python"
    },
    {
        "skill": "Etl",
        "reference": "Experience in Snowflake data modelling, ELT using snowpipe, implementing stored procedures and standard DWH and ETL concepts. Strong experience in DBT (data build tool), Snowflake and Python"
    },
    {
        "skill": "Data Migration",
        "reference": "Experience with enterprise cloud economics, Understanding of enterprise data management concepts and Experience in Data Migration from RDBMS to Snowflake cloud data warehouse"
    },
    {
        "skill": "Snowflake cloud",
        "reference": "Experience with enterprise cloud economics, Understanding of enterprise data management concepts and Experience in Data Migration from RDBMS to Snowflake cloud data warehouse"
    },
    {
        "skill": "Enterprise Cloud",
        "reference": "Experience with enterprise cloud economics, Understanding of enterprise data management concepts and Experience in Data Migration from RDBMS to Snowflake cloud data warehouse"
    },
    {
        "skill": "Azure",
        "reference": "Must have skills: Azure, Data Vault, Data Modeling, Data Analysis, SQL, Data Warehouse"
    },
    {
        "skill": "Data Vault",
        "reference": "Must have skills: Azure, Data Vault, Data Modeling, Data Analysis, SQL, Data Warehouse"
    },
    {
        "skill": "Data Modeling",
        "reference": "Must have skills: Azure, Data Vault, Data Modeling, Data Analysis, SQL, Data Warehouse"
    },
    {
        "skill": "SQL",
        "reference": "Notes: LinkedIn Profile is a mustRole Description..."
    },
    {
        "skill": "Data Analysis",
        "reference": "Notes: LinkedIn Profile is a mustRole Description..."
    },
    {
        "skill": "Data Warehouse",
        "reference": "Notes: LinkedIn Profile is a mustRole Description..."
    },
    {
        "skill": "Azure Databricks",
        "reference": "Experience in preparing data for and building pipelines and architecture"
    },
    {
        "skill": "Data Pipelines",
        "reference": "Experience in preparing data for and building pipelines and architecture"
    },
    {
        "skill": "System Integration",
        "reference": "Experience in preparing data for and building pipelines and architecture"
    },
    {
        "skill": "Data Quality",
        "reference": "Ensure adherence to company data policies and procedures for enhanced data quality and reduced discrepancies."
    },
    {
        "skill": "Adherence to Policies",
        "reference": "Ensure adherence to company data policies and procedures for enhanced data quality and reduced discrepancies."
    },
    {
        "skill": "Data Access Approval",
        "reference": "Ensure adherence to company data policies and procedures for enhanced data quality and reduced discrepancies."
    },
    {
        "skill": "Scalable Automation",
        "reference": "Develop scalable automated ETL/ELT jobs and ensure their maintenance."
    },
    {
        "skill": "ETL/ELT Jobs",
        "reference": "Develop scalable automated ETL/ELT jobs and ensure their maintenance."
    },
    {
        "skill": "Maintenance",
        "reference": "Develop scalable automated ETL/ELT jobs and ensure their maintenance."
    },
    {
        "skill": "Data Integrity",
        "reference": "Identify and address data integrity issues, performing deep dives to determine root causes and utilizing PL/SQL queries, Python, and API calls for master data set management."
    },
    {
        "skill": "Root Cause Analysis",
        "reference": "Identify and address data integrity issues, performing deep dives to determine root causes and utilizing PL/SQL queries, Python, and API calls for master data set management."
    },
    {
        "skill": "Data Merging",
        "reference": "Identify and address data integrity issues, performing deep dives to determine root causes and utilizing PL/SQL queries, Python, and API calls for master data set management."
    },
    {
        "skill": "PL/SQL",
        "reference": "Utilize PL/SQL queries, Python, and API calls to establish master data sets and merge datasets across different systems."
    },
    {
        "skill": "Python",
        "reference": "Utilize PL/SQL queries, Python, and API calls to establish master data sets and merge datasets across different systems."
    },
    {
        "skill": "API",
        "reference": "Utilize PL/SQL queries, Python, and API calls to establish master data sets and merge datasets across different systems."
    },
    {
        "skill": "Training",
        "reference": "Provide training for analysts and data scientists on available data sources."
    },
    {
        "skill": "Analysts",
        "reference": "Provide training for analysts and data scientists on available data sources."
    },
    {
        "skill": "Data Scientists",
        "reference": "Provide training for analysts and data scientists on available data sources."
    },
    {
        "skill": "Large Database",
        "reference": "Optimize the operation of very large databases and compute clusters."
    },
    {
        "skill": "Compute Clusters Optimization",
        "reference": "Optimize the operation of very large databases and compute clusters."
    },
    {
        "skill": "Database Structures",
        "reference": "Implement and maintain database structures and governance."
    },
    {
        "skill": "Governance",
        "reference": "Implement and maintain database structures and governance."
    },
    {
        "skill": "Documentation",
        "reference": "Develop and maintain documentation on databases and production tables."
    },
    {
        "skill": "Production Tables",
        "reference": "Develop and maintain documentation on databases and production tables."
    },
    {
        "skill": "Collaboration",
        "reference": "Collaborate with multiple data teams across the company to meet analytics deliverables."
    },
    {
        "skill": "Data Teams",
        "reference": "Collaborate with multiple data teams across the company to meet analytics deliverables."
    },
    {
        "skill": "Expertise in Consulting",
        "reference": "5 years of experience in consulting, business intelligence, analytics, or an equivalent analyst position, with expertise in PL/SQL and an additional object-oriented programming language (e.g., Python, Java, JavaScript)."
    },
    {
        "skill": "Business Intelligence Analytics",
        "reference": "5 years of experience in consulting, business intelligence, analytics, or an equivalent analyst position, with expertise in PL/SQL and an additional object-oriented programming language (e.g., Python, Java, JavaScript)."
    },
    {
        "skill": "Effective Problem Solving",
        "reference": "Effective problem-solving and analytical skills, managing multiple projects and reporting simultaneously across different stakeholders."
    },
    {
        "skill": "Analytical Skills",
        "reference": "Effective problem-solving and analytical skills, managing multiple projects and reporting simultaneously across different stakeholders."
    },
    {
        "skill": "Structured Thinking",
        "reference": "Structured thinking with the ability to break down ambiguous problems and propose impactful data modeling designs."
    },
    {
        "skill": "Impactful Data Modeling Designs",
        "reference": "Structured thinking with the ability to break down ambiguous problems and propose impactful data modeling designs."
    },
    {
        "skill": "Passion for Analyzing Large Complex Data Sets",
        "reference": "Passion for analyzing large and complex data sets to derive information driving business decisions."
    },
    {
        "skill": "Driving Business Decisions",
        "reference": "Passion for analyzing large and complex data sets to derive information driving business decisions."
    },
    {
        "skill": "Attention to Detail",
        "reference": "Attention to detail, organizational skills, and effective verbal/written communication skills."
    },
    {
        "skill": "Organization Skills",
        "reference": "Attention to detail, organizational skills, and effective verbal/written communication skills."
    },
    {
        "skill": "Communication Skills",
        "reference": "Attention to detail, organizational skills, and effective verbal/written communication skills."
    },
    {
        "skill": "Big Data Instances Experience",
        "reference": "Experience in big data instances, such as Cloudera, Azure, Snowflake, etc."
    },
    {
        "skill": "Cloudera, Azure, Snowflake",
        "reference": "Experience in big data instances, such as Cloudera, Azure, Snowflake, etc."
    },
    {
        "skill": "Self-Service Analytics Solutions Tableau Server Ask Data",
        "reference": "A proven track record in rolling out self-service analytics solutions (e.g., Tableau Server Ask Data)."
    },
    {
        "skill": "Negotiation",
        "reference": "Solid decision-making, negotiation, and persuasion skills, often in ambiguous situations."
    },
    {
        "skill": "Persuasion Skills",
        "reference": "Solid decision-making, negotiation, and persuasion skills, often in ambiguous situations."
    },
    {
        "skill": "Ambiguous Situations",
        "reference": "Solid decision-making, negotiation, and persuasion skills, often in ambiguous situations."
    },
    {
        "skill": "Fast Paced Environment Adaptability",
        "reference": "Ability to work in a fast-paced environment and adapt to changing requirements."
    },
    {
        "skill": "Changing Requirements",
        "reference": "Ability to work in a fast-paced environment and adapt to changing requirements."
    },
    {
        "skill": "Technology Development Projects Understanding",
        "reference": "Understanding of technology development projects and the full technology development lifecycle."
    },
    {
        "skill": "Full Lifecycle Experience",
        "reference": "Understanding of technology development projects and the full technology development lifecycle."
    },
    {
        "skill": "Informatica",
        "reference": "Required Skills Informatica - Axon, EDC, IDQ, MDM"
    },
    {
        "skill": "Linux",
        "reference": "5+ Linux experience"
    },
    {
        "skill": "Data Integration",
        "reference": "5+ years of experience supporting data integration, data quality management and data governance solutions"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Data Engineer, you will be responsible for designing, building, and maintaining data pipelines, data integration processes, and data infrastructure using Dataiku"
    },
    {
        "skill": "Data Integration",
        "reference": "As a Data Engineer, you will be responsible for designing, building, and maintaining data pipelines, data integration processes, and data infrastructure using Dataiku"
    },
    {
        "skill": "Pharmaceutical Domain Knowledge",
        "reference": "Strong understanding of the pharmaceutical domain, including knowledge of clinical data, drug development processes, regulatory requirements, and healthcare data"
    },
    {
        "skill": "Clinical Data",
        "reference": "Strong understanding of the pharmaceutical domain, including knowledge of clinical data, drug development processes, regulatory requirements, and healthcare data"
    },
    {
        "skill": "Data Engineering Technologies",
        "reference": "Proficiency in data engineering technologies and tools, such as SQL, Python, ETL frameworks, data integration platforms, and data warehousing solutions"
    },
    {
        "skill": "ETL Frameworks",
        "reference": "Proficiency in data engineering technologies and tools, such as SQL, Python, ETL frameworks, data integration platforms, and data warehousing solutions"
    },
    {
        "skill": "Data Warehousing Solutions",
        "reference": "Proficiency in data engineering technologies and tools, such as SQL, Python, ETL frameworks, data integration platforms, and data warehousing solutions"
    },
    {
        "skill": "Cloud Engineering, Python, Machine Learning, Backend Software Engineer",
        "reference": "2+ years of experience with Cloud Engineering / Services, 3+ years of work experience as a backend software engineer in Python, with exceptional software engineering knowledge"
    },
    {
        "skill": "ML Workflow Orchestration Tools, Google Cloud Platform, DevOps, GCP Services, Container Management Solution",
        "reference": "Experience with ML workflow orchestration tools like Airflow and Kubeflow, advanced working knowledge of object-oriented programming languages, experience with cloud services on GCP, familiarity with container management solutions"
    },
    {
        "skill": "Kubernetes, Docker, Infrastructure as Code, Terraform",
        "reference": "Experience in container management solution like Kubernetes and Docker, understanding of infrastructure as code, familiarity with tools such as Terraform"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a member of the Data Visualization team, you will report to the Data Visualization Leader. You will work with data solutions including data warehouses and data lakes to support data analytics, dashboard development, and data science efforts."
    },
    {
        "skill": "Database Management",
        "reference": "Develop integrated database solutions to store and retrieve company information for BI reporting and other analytical needs. Migrate data from legacy systems to new solutions. Install and configure information systems to ensure functionality."
    },
    {
        "skill": "Technology Experience",
        "reference": "Required certifications: Microsoft Azure administration, Microsoft Windows desktop and Server certifications, VMware VCP-DTM, and/or VCP-DCV (Horizon and VCenter). Strong knowledge of MS Windows 10/11 desktop, Microsoft 365, and VMware vSphere and Horizon is required. Knowledge of various software applications."
    },
    {
        "skill": "Databricks Developer",
        "reference": "Job: Databricks Developer (Data Engineer\u2014Databricks/ Azure)"
    },
    {
        "skill": "Core Framework Design",
        "reference": "Need to have hands-on design experience developing a core framework for data quality"
    },
    {
        "skill": "Dynamic Problem Solving",
        "reference": "Most candidate will not have done this before on DataBricks but need to be dynamic enough to get it done"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "As a Senior Data Engineer joining our growing Data team"
    },
    {
        "skill": "Data Engineering Leadership",
        "reference": "serve as a leader for data engineering"
    },
    {
        "skill": "ETLs & Internal App Development",
        "reference": "drive improvements and optimization in ETLs and internal apps"
    },
    {
        "skill": "Data Engineering",
        "reference": "Job Title: Data Engineer (MarTech, AdTech, Data Profiling)"
    },
    {
        "skill": "MarTech, AdTech, Data Profiling",
        "reference": "Job Title: Data Engineer (MarTech, AdTech, Data Profiling)"
    },
    {
        "skill": "Hands On Experience with MarTech and/or AdTech",
        "reference": "Interview : Zoom"
    },
    {
        "skill": "IT Resume",
        "reference": "Interview : Zoom"
    },
    {
        "skill": "Solid Programming Skills (J2EE, Python)",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Statistics Knowledge",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Analytical Skills",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Data Warehousing & ETL",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Data Architecture & Pipelining",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Understanding Salesforce and Adobe Marketing Stacks",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Experience with Third-Party Databases, Libraries, Interfaces, Internet Protocols",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Knowledge of Linux, J2EE, Relational and Document Databases, JSON, Shell Scripting, Automation",
        "reference": "Requirements: Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies"
    },
    {
        "skill": "Data Engineering, Azure, SQL, PySpark",
        "reference": "Requirements Bachelors Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM) 5+ years proven ability of professional Data Development experience 3+ years proven ability to develop with Azure and SQL"
    },
    {
        "skill": "ETL Concepts, Distributed Systems",
        "reference": "Requirements Bachelors Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM) 5+ years proven ability of professional Data Development experience 3+ years proven ability to develop with Azure and SQL"
    },
    {
        "skill": "Azure Data Factory, Azure Databricks",
        "reference": "Requirements Bachelors Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM) 5+ years proven ability of professional Data Development experience 3+ years proven ability to develop with Azure and SQL"
    },
    {
        "skill": "Agile Principles, Scrum",
        "reference": "Requirements Bachelors Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM) 5+ years proven ability of professional Data Development experience 3+ years proven ability to develop with Azure and SQL"
    },
    {
        "skill": "Relational Data Modeling",
        "reference": "Desirable Skills Proficient with Relational Data Modeling Understanding of Data Mesh Principles Experience with Python Library Development Experience with Structured Streaming (Spark or otherwise) Experience with Kafka and/or Azure Event Hub"
    },
    {
        "skill": "CI/CD",
        "reference": "Desirable Skills Proficient with Relational Data Modeling Understanding of Data Mesh Principles Experience with CI/CD - Continuous Integration/Continuous Delivery Experience with Python Library Development Experience with Structured Streaming (Spark or otherwise) Experience with Kafka and/or Azure Event Hub"
    },
    {
        "skill": "Agile Methodologies, Scrum",
        "reference": "Desirable Skills Proficient with Relational Data Modeling Understanding of Data Mesh Principles Experience with CI/CD - Continuous Integration/Continuous Delivery Experience with Python Library Development Experience with Structured Streaming (Spark or otherwise) Experience with Kafka and/or Azure Event Hub"
    },
    {
        "skill": "Data Visualization",
        "reference": "Transform raw data into visually compelling and informative dashboards, reports, and visualizations"
    },
    {
        "skill": "Collaboration",
        "reference": "Work with cross-functional teams to understand business requirements"
    },
    {
        "skill": "Interactive Dashboards",
        "reference": "Create dynamic and interactive visualizations with tools like Power-BI/Tableau"
    },
    {
        "skill": "Big Data",
        "reference": "Responsibilities Be responsible for leveraging existing data assets and platform components to build a flexible and reliable service layer for business systems."
    },
    {
        "skill": "Data Assets",
        "reference": "Responsibilities Be responsible for leveraging existing data assets and platform components to build a flexible and reliable service layer for business systems."
    },
    {
        "skill": "Backend Development",
        "reference": "After joining our team, you need to get familiar with our core business logic, and then use the big data tool stack to develop services and applications to satisfy product requirements."
    },
    {
        "skill": "SQL",
        "reference": "After joining our team, you need to get familiar with our core business logic, and then use the big data tool stack to develop services and applications to satisfy product requirements."
    },
    {
        "skill": "Java/Scala",
        "reference": "Requirements Bachelor\u2019s degree or higher in Computer Science, Software Engineering or a related field, or equivalent functional experience in the area. 2+ years experience in a big data related area."
    },
    {
        "skill": "Big Data Tools",
        "reference": "Requirements Bachelor\u2019s degree or higher in Computer Science, Software Engineering or a related field, or equivalent functional experience in the area. 2+ years experience in a big data related area."
    },
    {
        "skill": "Data Management",
        "reference": "Responsible for modernizing and transforming data and reporting capabilities across products"
    },
    {
        "skill": "Modernizing Architecture",
        "reference": "Responsible for modernizing and transforming data and reporting capabilities across products"
    },
    {
        "skill": "Cloud Data Engineer",
        "reference": "Part of the Data Management team responsible for data collection, transportation, maintenance/curation, and access to the corporate data asset"
    },
    {
        "skill": "Day-to-day responsibilities",
        "reference": "Part of the Data Management team responsible for data collection, transportation, maintenance/curation, and access to the corporate data asset"
    },
    {
        "skill": "Cross-functional Collaboration",
        "reference": "Works cross-functionally across the enterprise centralizing data and standardizing it for use by business reporting, machine learning, data science or other stakeholders"
    },
    {
        "skill": "Centralizing Data Access",
        "reference": "Works cross-functionally across the enterprise centralizing data and standardizing it for use by business reporting, machine learning, data science or other stakeholders"
    },
    {
        "skill": "Data Engineering",
        "reference": "Crossover\u2019s Data Engineer is responsible for managing and developing data sources for analytics at scale."
    },
    {
        "skill": "Analytics",
        "reference": "Crossover\u2019s Data Engineer is responsible for managing and developing data sources for analytics at scale."
    },
    {
        "skill": "Healthcare Data",
        "reference": "The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data."
    },
    {
        "skill": "Clinical Healthcare Data",
        "reference": "The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data."
    },
    {
        "skill": "Health Informatics",
        "reference": "The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data."
    },
    {
        "skill": "Data Architecture",
        "reference": "Assist with recommendations for data architecture, data storage, data integration, data quality, and data models."
    },
    {
        "skill": "Data Storage",
        "reference": "Assist with recommendations for data architecture, data storage, data integration, data quality, and data models."
    },
    {
        "skill": "Data Integration",
        "reference": "Assist with recommendations for data architecture, data storage, data integration, data quality, and data models."
    },
    {
        "skill": "AWS Data Integration Services",
        "reference": "This position requires in-depth knowledge of AWS Data Integration Services, such as Glue"
    },
    {
        "skill": "SQL Server, SQL Server Integration Services, MySQL",
        "reference": "The successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services"
    },
    {
        "skill": "ETL, Analytical Reporting, Data Pipelines",
        "reference": "This position is for an AWS Data Engineer with ETL and Analytical Reporting experience"
    },
    {
        "skill": "AWS Services (Glue, Lambda, Spark, RDS, S3, Redshift)",
        "reference": "The role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo)"
    },
    {
        "skill": "Data Models, Schemas, ETL Processes",
        "reference": "Design and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark"
    },
    {
        "skill": "Collaboration, Data Warehousing, Analytics Needs",
        "reference": "Collaborate with stakeholders to understand business requirements and translate them into technical data solutions"
    },
    {
        "skill": "Data Ingestion, Performance Tuning, Data Security",
        "reference": "Implement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift; Optimize data pipelines for performance, scalability, and cost-efficiency"
    },
    {
        "skill": "Data Governance, Validation Rules, Access Controls",
        "reference": "Ensure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls"
    },
    {
        "skill": "Monitoring, Troubleshooting, Performance Bottlenecks",
        "reference": "Monitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks"
    },
    {
        "skill": "Data Science, Analyst Collaboration, Support Initiatives",
        "reference": "Collaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure"
    },
    {
        "skill": "Continuous Learning, Improvement Opportunities",
        "reference": "Stay updated with the latest AWS data engineering trends, best practices, and technologies, proactively identify opportunities for improvement; Mentor and provide guidance to junior members of the data engineering team"
    },
    {
        "skill": "Data Engineering, Data Warehousing, Business Requirements",
        "reference": "Senior Data Engineer - Full-time"
    },
    {
        "skill": "ETL/ELT, Database Design, Optimization & Scheduling",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Cloud Platforms, ETL Experience",
        "reference": "Qualifications & Education and Experience"
    },
    {
        "skill": "Distributed systems",
        "reference": "Previous experience with distributed systems at scale."
    },
    {
        "skill": "Data infrastructure",
        "reference": "Experience building and maintaining custom ingestion pipelines"
    },
    {
        "skill": "Optimize and automate",
        "reference": "To be excited by the opportunity to optimize and automate our pipelines."
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Senior Data Engineer on the data platform team, you will develop and support data systems, pipelines, lakes, lakehouses."
    },
    {
        "skill": "Cloud Infrastructure Management",
        "reference": "Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments."
    },
    {
        "skill": "Security and Data Privacy",
        "reference": "Experience with a zero trust security framework, ensuring appropriate privacy and security measures."
    },
    {
        "skill": "Data Engineering",
        "reference": "Design and build data ingestion pipeline, Design and build complex data processing pipelines"
    },
    {
        "skill": "ETL/Data Architecture",
        "reference": "Work with relevant stakeholders to assist with data-related technical issues and support their data needs, Build programs for data quality checks, Provide operational support. Work with data architecture, data governance and data analytics teams."
    },
    {
        "skill": "System Testing, UAT, Code Deployment",
        "reference": "Involve in System Testing, UAT, code deployment activities, Coordinate with offshore team on regular basis"
    },
    {
        "skill": "Dollar Universe Consultant",
        "reference": "Role: Dollar Universe Consultant"
    },
    {
        "skill": "On-going contract potential hire or renewal",
        "reference": "6+ months on-going contract to potential hire or renewal"
    },
    {
        "skill": "Dollar Universe Administration",
        "reference": "ResponsibilitiesBasic Qualifications Dollar Universe Administration"
    },
    {
        "skill": "Own the Dollar Universe Automation tool for the team",
        "reference": "ResponsibilitiesBasic Qualifications Dollar Universe Administration"
    },
    {
        "skill": "Hands on experience with BMC / Control M",
        "reference": "Basic QualificationsDollar Universe AdministrationHands on experience with BMC / Control M"
    },
    {
        "skill": "Big Data",
        "reference": "About the job"
    },
    {
        "skill": "Applications",
        "reference": "About the job"
    },
    {
        "skill": "Business Strategies",
        "reference": "About the job"
    },
    {
        "skill": "Data Engineering Life Cycle",
        "reference": ""
    },
    {
        "skill": "Independent/Collaborative Work",
        "reference": ""
    },
    {
        "skill": "SQL",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "Python/Scala",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "Spark",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "Kafka",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "Sqoop",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "Hive",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "Kudu",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "HBASE",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "Impala",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "S3",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "HDFS",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "AWS, GCP, Azure",
        "reference": "Building data ingestion pipelines"
    },
    {
        "skill": "ETL Data Engineering",
        "reference": "Design, develop, and maintain scaled ETL process for data insights"
    },
    {
        "skill": "Data Lake Development",
        "reference": "Work as part of a team to build out and support Data Lake infrastructure"
    },
    {
        "skill": "Data Pipeline Automation",
        "reference": "Implement solutions using Python to process structured and unstructured data"
    },
    {
        "skill": "Collaboration & Communication",
        "reference": "Partner with business, architects, engineers, and manage project objectives"
    },
    {
        "skill": "Reporting Solutions",
        "reference": "Create and maintain report specifications & documentation for data deliverables"
    },
    {
        "skill": "Multitasking & Prioritization",
        "reference": "Manage an evolving workload in a fast-paced environment"
    },
    {
        "skill": "AWS Data Engineering, Senior, 10+ Years",
        "reference": "Role: AWS Data Engineer(W2 Only) Candidates must have all of the AWS data services below and be senior (minimum 10+ years) with strong communication"
    },
    {
        "skill": "Strong in AWS technologies, Python, PySpark, Database & SQL, Python, AWS experience, Data engineering, Serverless experience, Containerization",
        "reference": "Required Skills  Data Engineering Very strong in AWS technologies (Lambda, STEP Functions, SQS/SNS, S3, CloudWatch, EventBridge, EMR, EC@, Redshift, Airflow, Appflow etc.).Expert in Python, PySpark.Very Strong Database and SQL skills.Strong Python, Strong working AWS experience, Data engineering (EMR, PySpark, Redshift, Glue), Serverless experience (Lambda, step functions), containerization (ECS with Fargate)."
    },
    {
        "skill": "Good to have: SAS knowledge, DevOps knowledge, Testing Automation",
        "reference": "Good to have: SAS knowledge, DevOps knowledge(Jenkins, Bitbucket, Terraform/UCD/CloudFormation), Testing Automation."
    },
    {
        "skill": "AWS Services: Glue, Lambda, AWS Batch.Storage: S3.Database: Redshift, Aurora, RDS. Strong SAS/R/Python Scripting Experience. Familiarity with Domino Data Lab, Sagemaker",
        "reference": "AWS Services Compute: Glue, Lambda, AWS Batch.Storage: S3.Database: Redshift, Aurora, RDS.Strong SAS/R/Python Scripting Experience.Familiarity Domino Data Lab.Sagemaker."
    },
    {
        "skill": "Data Pipelines, ETL Processes",
        "reference": "Design and maintain data pipelines using AWS, Databricks, Python, and SQL technologies for ETL processes."
    },
    {
        "skill": "Collaboration, Problem Solving",
        "reference": "Collaborate with various teams to grasp data requirements, implement solutions aligning with business needs, enhance and troubleshoot existing pipelines, improve performance, ensure data quality, identify process improvements, and provide infrastructure support for data initiatives."
    },
    {
        "skill": "Data Engineering, Process Enhancement",
        "reference": "Migration Expertise: Proven experience in executing a data migration. Maintain scalability by automating manual processes."
    },
    {
        "skill": "Data Engineering, DBA",
        "reference": "We are looking for a Senior-level DBA who can perform Data Engineering work."
    },
    {
        "skill": "DBA with SQL Server and Azure SQL experience, Performance Tuning",
        "reference": "Someone with both SQL Server on-prem and Azure SQL experience with performance tuning."
    },
    {
        "skill": "Data Infrastructure Administration, Monitoring, Optimization",
        "reference": "Experience administrating and managing the organization's data infrastructure. Experience monitoring performance and optimize databases to ensure efficiency."
    },
    {
        "skill": "Python",
        "reference": "Experience with Python"
    },
    {
        "skill": "Web crawling",
        "reference": "Experience managing web crawling at scale, any framework, Scrapy is a plus"
    },
    {
        "skill": "Kubernetes",
        "reference": "Experience working with Kubernetes"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "The Senior Data Engineer is responsible for expanding, troubleshooting, and updating the organization's data pipeline architecture."
    },
    {
        "skill": "Data Pipeline Builder",
        "reference": "This individual will play a key role in re-designing the company's data architecture to support our digital transformation data initiatives."
    },
    {
        "skill": "Optimize Data Systems",
        "reference": "The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up."
    },
    {
        "skill": "Data Engineering, Mortgage Banking",
        "reference": "Job Overview: Are you a skilled Data Engineer with a passion for innovation and solid background in mortgage banking?"
    },
    {
        "skill": "Design and Construction, Business Alignment, Innovation",
        "reference": "Responsibilities  Design and Construct, Business Alignment, Innovation"
    },
    {
        "skill": "Translate Functional Requirements, Integration, Tool Development",
        "reference": "Responsibilities  Translation: Translate complex functional and technical requirements into detailed architecture, design, and high-performing software. Integration: Integrate new data management technologies and software engineering tools."
    },
    {
        "skill": "Collaboration, Security and Compliance",
        "reference": "Collaboration: Work closely with data and analytics experts to enhance functionality in our data systems. Security and Compliance: Maintain a secure and compliant data processing environment."
    },
    {
        "skill": "MS Azure",
        "reference": "Primary Skills: MS Azure, Databricks deployment, and in-depth technical knowledge."
    },
    {
        "skill": "Databricks Deployment",
        "reference": "Primary Skills: MS Azure, Databricks deployment, and in-depth technical knowledge."
    },
    {
        "skill": "DevOps CI/CD Tools",
        "reference": "Primary Skills: MS Azure, Databricks deployment, and in-depth technical knowledge."
    },
    {
        "skill": "Data Platforms",
        "reference": "As a Data Platform/DevOps Engineer, you'll play a crucial role with 5+ years of experience."
    },
    {
        "skill": "Azure Ecosystem Integration",
        "reference": "As a Data Platform/DevOps Engineer, you'll play a crucial role with 5+ years of experience."
    },
    {
        "skill": "Databricks",
        "reference": "As a Data Platform/DevOps Engineer, you'll play a crucial role with 5+ years of experience."
    },
    {
        "skill": "Big Data Use Cases",
        "reference": "Working knowledge of DevOps tools and monitoring tools like Dynatrace."
    },
    {
        "skill": "Large Clusters Management",
        "reference": "Working knowledge of DevOps tools and monitoring tools like Dynatrace."
    },
    {
        "skill": "Data Monitoring Tools",
        "reference": "Working knowledge of DevOps tools and monitoring tools like Dynatrace."
    },
    {
        "skill": "Informatica, Python, SQL, Redshift",
        "reference": "Need Strong experience in below skill sets. Informatica, Python, SQL, Redshift"
    },
    {
        "skill": "Experience in Informatica, Python, SQL, Redshift",
        "reference": "Job Title: Data Engineer(Redshift), Job Location: Los Angeles, CA - Onsite, Duration : Fulltime / 12 Months Contract"
    },
    {
        "skill": "Data Engineering, 6+ Years Experience",
        "reference": "Leverage your expertise with a minimum of 6 years in data engineering."
    },
    {
        "skill": "SQL Across Multiple Platforms, Tools like Snowflake & Spark SQL",
        "reference": "Showcase Your Strong SQL Skills Across Multiple Database Platforms. Work with tools such as Snowflake, Databricks, Spark SQL, PySpark, and Python."
    },
    {
        "skill": "ETL Pipelines, Continuous Integration/Delivery",
        "reference": "Develop and maintain ETL pipelines for seamless data flow. Apply your knowledge of database design principles. Engage in data modeling, schema development, and comprehensive data-centric documentation. Experience in integrating data from diverse sources."
    },
    {
        "skill": "Database Design Principles, Data Modeling",
        "reference": "Apply your knowledge of database design principles. Expertise in data modeling, schema development, and data-centric documentation."
    },
    {
        "skill": "Data Performance, Code Optimization, CI/CD",
        "reference": "Enhance code performance and optimize queries for efficiency. Implement Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environment."
    },
    {
        "skill": "Problem Solving, Communication Skills",
        "reference": "Demonstrate outstanding problem-solving skills. Communicate effectively with a diverse group, including executives, managers, and subject matter experts."
    },
    {
        "skill": "Data Engineering",
        "reference": "Title Data Engineer (Streaming Platforms) Duration 9+ Months Location Remote customer is doing near real-time data integration. They do streaming analytics and they emphasize on Live Table Structured streaming, Kafka, Event Hub, Apache Spark."
    },
    {
        "skill": "Streaming Platforms",
        "reference": "Title Data Engineer (Streaming Platforms) Duration 9+ Months Location Remote customer is doing near real-time data integration. They do streaming analytics and they emphasize on Live Table Structured streaming, Kafka, Event Hub, Apache Spark."
    },
    {
        "skill": "Apache Spark",
        "reference": "Streaming Platforms Mandatory BS, MS, or PhD in Computer Science, Information Technology, Management Information Systems (MIS), Data Science or related field 2+ years of experience in Apache Spark (PySpark / Spark SQL) 2+ years of experience in Python Experience with large scale streaming platforms (e.g. Kafka, Event Hub, Databricks Live Table / Structured Streaming), processing frameworks (e.g. Spark, Databricks) and storage engines stream analytics."
    },
    {
        "skill": "Python",
        "reference": "Streaming Platforms Mandatory BS, MS, or PhD in Computer Science, Information Technology, Management Information Systems (MIS), Data Science or related field 2+ years of experience in Apache Spark (PySpark / Spark SQL) 2+ years of experience in Python Experience with large scale streaming platforms (e.g. Kafka, Event Hub, Databricks Live Table / Structured Streaming), processing frameworks (e.g. Spark, Databricks) and storage engines stream analytics."
    },
    {
        "skill": "Data Engineering",
        "reference": "3+ years of experience in data engineering, data integration, data modeling, data architecture, and ETL/ELT processes to provide quality data and analytics solutions"
    },
    {
        "skill": "Data Integration",
        "reference": "3+ years of experience in data engineering, data integration, data modeling, data architecture, and ETL/ELT processes to provide quality data and analytics solutions"
    },
    {
        "skill": "Data Modeling",
        "reference": "3+ years of experience in data engineering, data integration, data modeling, data architecture, and ETL/ELT processes to provide quality data and analytics solutions"
    },
    {
        "skill": "SQL",
        "reference": "Highly proficient working in Azure cloud environment (e.g. Blob / ADLS, Databricks, Azure Data Factory, Event Hub) Excellent communication skills ability to communicate technical concepts to both technical and non-technical audiences Experience in Regular Expression, CI/CD technology, Terraform and Git"
    },
    {
        "skill": "Complex Data Schemas",
        "reference": "Highly proficient working in Azure cloud environment (e.g. Blob / ADLS, Databricks, Azure Data Factory, Event Hub) Excellent communication skills ability to communicate technical concepts to both technical and non-technical audiences Experience in Regular Expression, CI/CD technology, Terraform and Git"
    },
    {
        "skill": "Query Optimization",
        "reference": "Highly proficient working in Azure cloud environment (e.g. Blob / ADLS, Databricks, Azure Data Factory, Event Hub) Excellent communication skills ability to communicate technical concepts to both technical and non-technical audiences Experience in Regular Expression, CI/CD technology, Terraform and Git"
    },
    {
        "skill": "Data Engineer",
        "reference": "Job Title: Data Engineer"
    },
    {
        "skill": "Programming skills, statistics knowledge, analytics",
        "reference": "Requirements"
    },
    {
        "skill": "Big data technologies, Data warehousing & ETL, Data architecture & pipelining",
        "reference": "Experience with J2EE, Python, statistics knowledge, analytical skills, understanding of big data technologies"
    },
    {
        "skill": "GCP DevOps Engineer and Database Administrator",
        "reference": "Our client is seeking a skilled GCP DevOps / DBA Engineer to join their team."
    },
    {
        "skill": "GCP, Terraform, Bamboo (or other CI/CD tools)",
        "reference": "Direct experience with GCP, Terraform, and Bamboo (can have experience with other CI/CD tools instead)."
    },
    {
        "skill": "PostgreSQL, Snowflake",
        "reference": "They are looking for a resource with extensive Postgres and Snowflake experience."
    },
    {
        "skill": "Senior Data Engineer, Expertise across multiple disciplines, Develop data systems",
        "reference": "About Us"
    },
    {
        "skill": "Build scalable data pipelines, Automation, Performance tuning",
        "reference": "What You'll Do"
    },
    {
        "skill": "Data lake, Lakehouse, Lakehouse models, Spark and Pyspark libraries",
        "reference": "Initial areas of focus will include"
    },
    {
        "skill": "Collaborate with stakeholders, Leverage tools, Engines, Libraries",
        "reference": "About Your Initial Areas of Focus"
    },
    {
        "skill": "Demonstrated experience in various areas",
        "reference": "What You Need to Get the Job Done (if you don't have all, apply anyway!)"
    },
    {
        "skill": "Security around cloud infrastructure and data systems",
        "reference": "Understanding of security around cloud infrastructure and data systems"
    },
    {
        "skill": "Bonus Skills: Lakehouse technologies, Flink, Presto, Dremio, Kubernetes, Terraform, Zero trust, CI/CD pipelines, QA and test automation, Tableau",
        "reference": "Beyond the technical skills, we're looking for individuals who are"
    },
    {
        "skill": "Clear communicators, Analytical, Creative coding, Detail-oriented, Productive in dynamic settings",
        "reference": "If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree"
    },
    {
        "skill": "Infrastructure engineer",
        "reference": "Must haves: 7+ years as an infrastructure engineer Expertise with IBM products (CloudPak 4 Data + IBM Watson) Knowledge in the following areas"
    },
    {
        "skill": "IBM products expert",
        "reference": "Must haves: 7+ years as an infrastructure engineer Expertise with IBM products (CloudPak 4 Data + IBM Watson) Knowledge in the following areas"
    },
    {
        "skill": "CP4D Admin, Openshift, PortWorx",
        "reference": "Must haves: 7+ years as an infrastructure engineer Expertise with IBM products (CloudPak 4 Data + IBM Watson) Knowledge in the following areas"
    },
    {
        "skill": "CP4D patching",
        "reference": "Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products \u2013 Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway"
    },
    {
        "skill": "installation",
        "reference": "Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products \u2013 Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway"
    },
    {
        "skill": "operational management",
        "reference": "Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products \u2013 Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway"
    },
    {
        "skill": "Sub Products",
        "reference": "Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products \u2013 Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway"
    },
    {
        "skill": "Watson Knowledge Studio",
        "reference": "Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products \u2013 Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway"
    },
    {
        "skill": "Watson Discovery",
        "reference": "Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products \u2013 Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway"
    },
    {
        "skill": "WAVI Assistant Voice & Voice Gateway",
        "reference": "Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products \u2013 Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway"
    },
    {
        "skill": "GenRocket, scripting, PostGres/DB ability, API work",
        "reference": "Must required: GenRocket, scripting, PostGres/DB ability, API work"
    },
    {
        "skill": "Data analysis, data hub, data flow understanding",
        "reference": "Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine the best way to manage and create synthetic data in Data Hub; deliver solution"
    },
    {
        "skill": "Data model knowledge, table structures, dependencies",
        "reference": "Understanding data models, table structures and dependencies."
    },
    {
        "skill": "Synthetic data creation, maintenance, PoC",
        "reference": "Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team"
    },
    {
        "skill": "Agile mindset, Azure Cloud, ADO experience, python or powershell scripting",
        "reference": "Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience"
    },
    {
        "skill": "Data Engineering background",
        "reference": "Must-Haves: Strong Data Engineering background (prefer someone with 10+ years of experience), Very Strong AWS Hands-on Experience, EC2, Lambda, Step Functions, EMR, SQS/SNS, Expert skills with Python & PySpark"
    },
    {
        "skill": "Strong AWS Experience",
        "reference": "Must-Haves: Strong Data Engineering background (prefer someone with 10+ years of experience), Very Strong AWS Hands-on Experience, EC2, Lambda, Step Functions, EMR, SQS/SNS, Expert skills with Python & PySpark"
    },
    {
        "skill": "Experience in Tableau",
        "reference": "Nice-to-Haves: Database skills and SQL skills are highly preferred. Awareness of Tableau (nice-to-have: Tableau Prep)"
    },
    {
        "skill": "Tableau Prep",
        "reference": "Nice-to-Haves: Database skills and SQL skills are highly preferred. Awareness of Tableau (nice-to-have: Tableau Prep)"
    },
    {
        "skill": "Database Skills",
        "reference": "Nice-to-Haves: Database skills and SQL skills are highly preferred."
    },
    {
        "skill": "SQL Skills",
        "reference": "Nice-to-Haves: Database skills and SQL skills are highly preferred."
    },
    {
        "skill": "AWS Certification",
        "reference": "Must-Haves: AWS certification(s)."
    },
    {
        "skill": "Selenium, Databricks experience",
        "reference": "Candidate must have Selenium and Databricks experience to be considered."
    },
    {
        "skill": "Data Mesh, Databricks implementation",
        "reference": "McLaren has been hired by a Fortune 500 Consumer Packaged Goods (CPG) company to assist with implementing data mesh within Databricks."
    },
    {
        "skill": "QA experience, Data Product development",
        "reference": "The client has 120 data products they are building out."
    },
    {
        "skill": "People analytics, social science research, data mining/science",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace."
    },
    {
        "skill": "Advanced data analytics for productivity, effectiveness, happiness in remote-first companies",
        "reference": "Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "Data science, statistics, coding/scripting languages proficiency",
        "reference": "Background in data science, mathematics, actuarial science, or engineering, First work experience in People Analytics, Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)"
    },
    {
        "skill": "Overall Experience",
        "reference": "10+ years of overall experience, 5+ years of experience working on any cloud platform"
    },
    {
        "skill": "Cloud Platform Experience",
        "reference": "10+ years of overall experience, 5+ years of experience working on any cloud platform"
    },
    {
        "skill": "Enabling Tools in Cloud",
        "reference": "Experience enabling new tools in cloud platforms and making it enterprise ready"
    },
    {
        "skill": "Enterprise Readiness",
        "reference": "Experience enabling new tools in cloud platforms and making it enterprise ready"
    },
    {
        "skill": "Multi Tenancy Environment",
        "reference": "Worked with large multi tenancy environment"
    },
    {
        "skill": "Large Scale Experience",
        "reference": "Worked with large multi tenancy environment"
    },
    {
        "skill": "IAM Provisioning",
        "reference": "Hands on experience with the following IAM, Provisioning, VPC Networks and VPC Subnets."
    },
    {
        "skill": "VPC Networks & Subnets",
        "reference": "Hands on experience with the following IAM, Provisioning, VPC Networks and VPC Subnets."
    },
    {
        "skill": "Cloud Compute Experience",
        "reference": "Cloud compute (cloud function, Kubernetes and VMs)"
    },
    {
        "skill": "Kubernetes, Cloud Function & VM",
        "reference": "Cloud compute (cloud function, Kubernetes and VMs)"
    },
    {
        "skill": "Databricks development",
        "reference": "5+ years of experience in Databricks development and data engineering (AZURE)Apache SparkDatabricks NotebooksScala/Python ProgrammingData Transformation and ProcessingGit/GitHubData Pipeline Development"
    },
    {
        "skill": "Data engineering (AZURE)",
        "reference": "5+ years of experience in Databricks development and data engineering (AZURE)Apache SparkDatabricks NotebooksScala/Python ProgrammingData Transformation and ProcessingGit/GitHubData Pipeline Development"
    },
    {
        "skill": "Apache Spark",
        "reference": "5+ years of experience in Databricks development and data engineering (AZURE)Apache SparkDatabricks NotebooksScala/Python ProgrammingData Transformation and ProcessingGit/GitHubData Pipeline Development"
    },
    {
        "skill": "Databricks Notebooks",
        "reference": "5+ years of experience in Databricks development and data engineering (AZURE)Apache SparkDatabricks NotebooksScala/Python ProgrammingData Transformation and ProcessingGit/GitHubData Pipeline Development"
    },
    {
        "skill": "Data pipeline development",
        "reference": "Responsibilities: Develop and implement data pipelines and transformations on Databricks. Integrate data from various sources into the Databricks environment."
    },
    {
        "skill": "Data product deployment",
        "reference": "Responsibilities: Develop and implement data pipelines and transformations on Databricks. Integrate data from various sources into the Databricks environment."
    },
    {
        "skill": "Data Analytics",
        "reference": "Utilize advanced data analytics"
    },
    {
        "skill": "Workplace Research",
        "reference": "Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company"
    },
    {
        "skill": "Organisational Psychology",
        "reference": "Support analytics and data mining in a cross-disciplinary team of organisational psychologists"
    },
    {
        "skill": "Data Engineering Experience",
        "reference": "Must have 5+ years US based work experience Top Skills"
    },
    {
        "skill": "5+ years US based work experience",
        "reference": "Must have 5+ years US based work experience Top Skills"
    },
    {
        "skill": "Cloud Solutions",
        "reference": "Top Skills 5+ years of data engineering experience, Experience providing Cloud solutions is a must, Must have Hadoop experience, End to end use case implementation"
    },
    {
        "skill": "Hadoop Experience",
        "reference": "Top Skills 5+ years of data engineering experience, Experience providing Cloud solutions is a must, Must have Hadoop experience, End to end use case implementation"
    },
    {
        "skill": "End to end use case implementation",
        "reference": "Top Skills 5+ years of data engineering experience, Experience providing Cloud solutions is a must, Must have Hadoop experience, End to end use case implementation"
    },
    {
        "skill": "Python",
        "reference": "Working knowledge in one of the ETL tools (Teradata/Informatica/Infoworks/Ab initio), Experience with Azure Cloud or AWS environments"
    },
    {
        "skill": "ETL Tools Knowledge",
        "reference": "Working knowledge in one of the ETL tools (Teradata/Informatica/Infoworks/Ab initio), Experience with Azure Cloud or AWS environments"
    },
    {
        "skill": "Azure or AWS Environments",
        "reference": "Working knowledge in one of the ETL tools (Teradata/Informatica/Infoworks/Ab initio), Experience with Azure Cloud or AWS environments"
    },
    {
        "skill": "Data Engineering",
        "reference": "Programming experience using Python, Hands on with a wide variety of use cases like ETL, Data Hubs, Data warehousing, Data lakes"
    },
    {
        "skill": "Variety of Use Cases",
        "reference": "Programming experience using Python, Hands on with a wide variety of use cases like ETL, Data Hubs, Data warehousing, Data lakes"
    },
    {
        "skill": "Technical Customer Support",
        "reference": "Job Responsibilities"
    },
    {
        "skill": "Data Integration",
        "reference": "Job Responsibilities"
    },
    {
        "skill": "Product Knowledge",
        "reference": "Job Responsibilities"
    },
    {
        "skill": "Communication",
        "reference": "Job Requirements"
    },
    {
        "skill": "Collaboration",
        "reference": "Job Requirements"
    },
    {
        "skill": "Proactive Mindset",
        "reference": "Job Requirements"
    },
    {
        "skill": "Software as a Service",
        "reference": "Job Requirements"
    },
    {
        "skill": "Network Technologies",
        "reference": "Job Requirements"
    },
    {
        "skill": "SQL, Excel",
        "reference": "Job Requirements"
    },
    {
        "skill": "Data Integration Methods",
        "reference": "Job Requirements"
    },
    {
        "skill": "Clever or OneRoster",
        "reference": "Job Requirements"
    },
    {
        "skill": "Case Tracking",
        "reference": "Job Requirements"
    },
    {
        "skill": "Customer Relationship Management Software",
        "reference": "Job Requirements"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "As a Senior Data Engineer, you will be an integral part of our core team."
    },
    {
        "skill": "Data Ingestion and Processing Pipeline Development",
        "reference": "Lead the development and implementation of our state-of-the-art data ingestion and processing pipeline."
    },
    {
        "skill": "Greenfield Project Collaboration",
        "reference": "Collaborate on a predominantly greenfield project, leveraging your expertise to drive innovation and deliver impactful solutions."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "Overview: The Senior Data Engineer is responsible for contributing to data analytics, data operations, and software product development across the organization."
    },
    {
        "skill": "Healthcare Industry Experience",
        "reference": "Minimum of 5 years healthcare industry experience with and strong knowledge of healthcare data formats such as CMS LDS files and/or X12/EDI claims processing (specifically 837I/837P); Strong working knowledge of Centers for Medicare & Medicaid Services (CMS) data"
    },
    {
        "skill": "HIPAA Compliance",
        "reference": "Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Company. Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position"
    },
    {
        "skill": "SQL",
        "reference": "Below are the primary skills required for the role: SQL, Oracle, DB2 server"
    },
    {
        "skill": "Oracle",
        "reference": "Below are the primary skills required for the role: SQL, Oracle, DB2 server"
    },
    {
        "skill": "DB2 server",
        "reference": "Below are the primary skills required for the role: SQL, Oracle, DB2 server"
    },
    {
        "skill": "Datawarehouse/Lake building/maintenance",
        "reference": "Hands on experience of building and maintaining artifacts in Datawarehouses, Data lakes etc for both on-prem (Teradata) and on-cloud (preferably on Azure)"
    },
    {
        "skill": "ETL tools",
        "reference": "Hands on experience of using ETL tools such as Informatica; Hands on experience on MS Azure; such as, Azure Data Factory , Azure Data Bricks(pyspark), Synapse, Azure DevOps"
    },
    {
        "skill": "Microsoft Azure",
        "reference": "Hands on experience of using ETL tools such as Informatica; Hands on experience on MS Azure; such as, Azure Data Factory , Azure Data Bricks(pyspark), Synapse, Azure DevOps"
    },
    {
        "skill": "Business analytics",
        "reference": "Experience with Business analytics and Intelligence tools such as Tableau, Power BI etc."
    },
    {
        "skill": "Power BI",
        "reference": "Experience with Business analytics and Intelligence tools such as Tableau, Power BI etc."
    },
    {
        "skill": "Agile frameworks",
        "reference": "Experience with Agile frameworks"
    },
    {
        "skill": "Design and implement data architecture",
        "reference": "Responsibilities: Design and implement the data architecture, ensuring scalability, flexibility, and efficiency using pipeline authoring tools like Metaflow and large-scale data processing technologies like Spark"
    },
    {
        "skill": "Data pipelines experience on large scale datasets",
        "reference": "Expertise in designing and developing distributed data pipelines using big data technologies on large scale data sets"
    },
    {
        "skill": "Big data technologies expertise",
        "reference": "Experience with big data processing and analytics on AWS, using services such as Amazon EMR and AWS Batch"
    },
    {
        "skill": "Data Engineer, Senior/Principal level, Graph focus, Post.News web application",
        "reference": "We\u2019re looking for a Senior/Principal level Data Engineer with a Graph focus to help us build and polish the Post.News web application."
    },
    {
        "skill": "Remote, US-based work",
        "reference": "This role is Remote within the U.S. We are not able to sponsor visas at this time."
    },
    {
        "skill": "Post.News\u2014mission driven, civil conversations platform, social media reimagined",
        "reference": "Remember when social media was fun, introduced you to big ideas and cool people, and actually made you smarter? Remember when it didn't waste your time and make you angry or sad?"
    },
    {
        "skill": "Develop, scale, and tune data pipelines",
        "reference": "Experience developing, scaling, and tuning data pipelines in Spark with PySpark"
    },
    {
        "skill": "Cloud infrastructure knowledge",
        "reference": "Understanding of security around cloud infrastructure and data systems"
    },
    {
        "skill": "Data lake/lakehouse experience",
        "reference": "Understanding of data lake, lakehouse, and data warehouse systems"
    },
    {
        "skill": "Team coding workflows",
        "reference": "Git-based team coding workflows"
    },
    {
        "skill": "Communication and collaboration skills",
        "reference": "Clear communicators with team members and stakeholders"
    },
    {
        "skill": "Data Engineering, SQL, Programming",
        "reference": "Key responsibilities include creation of end-to-end architectural solutions and data management."
    },
    {
        "skill": "Business Intelligence Tools, Analytics",
        "reference": "Candidate is expected to independently collaborate with business units for actionable analytics."
    },
    {
        "skill": "Risk Management",
        "reference": "Position will have exposure to market risk, credit risk, liquidity risk, portfolio valuation and is expected to utilize a variety of technology tools."
    },
    {
        "skill": "Data Engineering, Greenfield Project Development, AI-powered Predictive Software",
        "reference": "As a Senior Data Engineer, you will be an integral part of our core team, driving the development of our groundbreaking data ingestion and processing pipeline. This role offers a unique opportunity to contribute to a predominantly greenfield project in a fast-growing, machine-learning startup."
    },
    {
        "skill": "Data Architecture, Information Modeling, Optimization Techniques",
        "reference": "Deep technical expertise in: Information Architecture, Data Engineering, and Data Warehousing, including proficiency in maintaining data quality, data versioning, and data documentation. Database architecture and data modeling, optimizing index performance, handling large time series datasets, working with stored procedures, star schema, snowflake schema, dimension tables, and fact tables."
    },
    {
        "skill": "Diverse Team Collaboration, Startup Experience, Scalable Data Pipeline",
        "reference": "Collaborate on a predominantly greenfield project, leveraging your expertise to drive innovation and deliver impactful solutions. Contribute to a fast-growing startup, where your ideas and contributions will have a direct and meaningful impact."
    },
    {
        "skill": "10+ years of migration, 10+ IT experience, ETL experience",
        "reference": "Minimum Skills They Will Need Are Listed Below"
    },
    {
        "skill": "10+ years of SQL experience, Informatica IICS, Azure platform expertise",
        "reference": "Minimum Skills They Will Need Are Listed Below"
    },
    {
        "skill": "5+ years of Informatica Power Center, Snowflake experience, Informatica IICS certifications",
        "reference": "Minimum Skills They Will Need Are Listed Below"
    },
    {
        "skill": "Big Data",
        "reference": "Develop and maintain Big Data system (Hadoop, Kafka, Hive, Spark) Maintain the system's databases (Vertica, SQL Server, MongoDB)"
    },
    {
        "skill": "Database administration",
        "reference": "Develop and maintain Big Data system (Hadoop, Kafka, Hive, Spark) Maintain the system's databases (Vertica, SQL Server, MongoDB)"
    },
    {
        "skill": "Software development",
        "reference": "Design and support Database infrastructure, with consideration for performance, availability, and specific application requirements"
    },
    {
        "skill": "Performance",
        "reference": "Design and support Database infrastructure, with consideration for performance, availability, and specific application requirements"
    },
    {
        "skill": "Availability",
        "reference": "Design and support Database infrastructure, with consideration for performance, availability, and specific application requirements"
    },
    {
        "skill": "Problem solving",
        "reference": "Accountable, dedicated and willing to be on-call as needed AdTech experience is a plus!"
    },
    {
        "skill": "Fast learner",
        "reference": "Accountable, dedicated and willing to be on-call as needed AdTech experience is a plus!"
    },
    {
        "skill": "Creative thinker",
        "reference": "Accountable, dedicated and willing to be on-call as needed AdTech experience is a plus!"
    },
    {
        "skill": "Data Warehousing",
        "reference": "YearsInsurance Industry: YearsMust - Property and casualty experience in recent 2-3 projects: Years"
    },
    {
        "skill": "Modelling",
        "reference": "YearsInsurance Industry: YearsMust - Property and casualty experience in recent 2-3 projects: Years"
    },
    {
        "skill": "End-to-end BI Solutions",
        "reference": "YearsInsurance Industry: YearsMust - Property and casualty experience in recent 2-3 projects: Years"
    },
    {
        "skill": "SQL",
        "reference": "Must - SQL and query optimization: Years"
    },
    {
        "skill": "Query Optimization",
        "reference": "Must - SQL and query optimization: Years"
    },
    {
        "skill": "Data Engineering Solutions",
        "reference": "Experience developing solutions for the Insurance industry."
    },
    {
        "skill": "Software Engineering Principles",
        "reference": "Experience developing solutions for the Insurance industry."
    },
    {
        "skill": "Best Practices",
        "reference": "Experience developing solutions for the Insurance industry."
    },
    {
        "skill": "Data Engineering",
        "reference": "Experienced Data Engineer, Database Models, Data Pipelines, ETL/ELT scripts, Quality Assurance"
    },
    {
        "skill": "SQL Experience",
        "reference": "Strong SQL experience"
    },
    {
        "skill": "Azure Technologies",
        "reference": "Experience working with Azure Data Factory and Azure Data Lake"
    },
    {
        "skill": "Exceptional client communication",
        "reference": "Exceptional client communication skills"
    },
    {
        "skill": "Business challenge understanding",
        "reference": "ability to understand client's business challenge"
    },
    {
        "skill": "Technology architecture/approach mapping",
        "reference": "map it to a technology architecture / approach"
    },
    {
        "skill": "Sales team collaboration",
        "reference": "work with Sales team on customer presentations"
    },
    {
        "skill": "Client solutioning",
        "reference": "white boarding solutioning and contribute to sales proposals"
    },
    {
        "skill": "Hiring technical analytics team",
        "reference": "hire and manage technical analytics team"
    },
    {
        "skill": "Multi-tasking",
        "reference": "The ability to multi task across these items is vital."
    },
    {
        "skill": "Script development for data processing",
        "reference": "Developing scripts to process structured and unstructured data."
    },
    {
        "skill": "Improving data reliability, efficiency, and quality",
        "reference": "Recommending, developing and implementing ways to improve data reliability, efficiency and quality."
    },
    {
        "skill": "Supporting data business needs",
        "reference": "Working with stakeholders to understand needs in order with respect to data structure, availability, scalability and accessibility."
    },
    {
        "skill": "DW/BI, Big data, ETL",
        "reference": ""
    },
    {
        "skill": "Jd 5+ Years\u2019 experience in DW/BI or Bigdata Project",
        "reference": ""
    },
    {
        "skill": "Python, Data pipeline",
        "reference": ""
    },
    {
        "skill": "Proficient in creating in data pipeline using python",
        "reference": ""
    },
    {
        "skill": "Hadoop Tools, SPARK, SQL, PL/SQL",
        "reference": ""
    },
    {
        "skill": "Strong knowledge and working experience in Hadoop Tools and technologies, Strong knowledge and working experience in SPARK, Proficient in SQL, PL/SQL",
        "reference": ""
    },
    {
        "skill": "MPP Database, Cloud database",
        "reference": ""
    },
    {
        "skill": "Working experience with MPP Database (Teradata/Greenplum/Netezza/Vertica), Working experience with cloud database (Redshift/Big Query/Snowflake)",
        "reference": ""
    },
    {
        "skill": "Airflow, Big query",
        "reference": ""
    },
    {
        "skill": "Good working experience with Airflow, Big query experience is added advantage",
        "reference": ""
    },
    {
        "skill": "data-processing",
        "reference": "Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "orchestration",
        "reference": "Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "monitoring",
        "reference": "Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "collaborate",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform."
    },
    {
        "skill": "design",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform."
    },
    {
        "skill": "validate",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform."
    },
    {
        "skill": "process improvements",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability."
    },
    {
        "skill": "automating",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability."
    },
    {
        "skill": "optimizing",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability."
    },
    {
        "skill": "technical support",
        "reference": "Provide technical support and usage guidance to the users of our platform\u2019s services."
    },
    {
        "skill": "guidance",
        "reference": "Provide technical support and usage guidance to the users of our platform\u2019s services."
    },
    {
        "skill": "creation",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "refinement",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "metrics",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "experience building data pipelines in a distributed environment",
        "reference": "Experience building and optimizing data pipelines in a distributed environment"
    },
    {
        "skill": "support",
        "reference": "Experience supporting and working with cross-functional teams"
    },
    {
        "skill": "cross-functional teams",
        "reference": "Experience supporting and working with cross-functional teams"
    },
    {
        "skill": "Linux environment",
        "reference": "Proficiency working in Linux environment"
    },
    {
        "skill": "proficiency",
        "reference": "Proficiency working in Linux environment"
    },
    {
        "skill": "SQL, Python, PySpark",
        "reference": "Proficiency working in Linux environment"
    },
    {
        "skill": "AWS technologies",
        "reference": "Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline"
    },
    {
        "skill": "5+ years experience using a broad range of AWS tools",
        "reference": "Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline"
    },
    {
        "skill": "platform monitoring and alerts tools",
        "reference": "Experience with platform monitoring and alerts tools"
    },
    {
        "skill": "People analytics, social science research, data mining",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of the digital workplace"
    },
    {
        "skill": "Advanced data analytics, quantitative and qualitative data analytics, insights",
        "reference": "Utilize advanced data analytics to understand how we hire and work (productivity, happiness and effectiveness) across a global, remote first organisation"
    },
    {
        "skill": "Data visualization, presentation skills, research",
        "reference": "Design and conduct research into trends shaping talent science and remote work. Collaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace"
    },
    {
        "skill": "IT experience",
        "reference": "10+ years in IT with at least 3+ years' experience in data warehousing, modelling, end-to-end BI solutions"
    },
    {
        "skill": "10+ Years",
        "reference": "10+ years in IT with at least 3+ years' experience in data warehousing, modelling, end-to-end BI solutions"
    },
    {
        "skill": "Data Engineering Solutions",
        "reference": "Experience in developing data platforms/ Big data and cloud technologies (e.g., Azure)"
    },
    {
        "skill": "Software Engineering best practices",
        "reference": "Experience in developing data platforms/ Big data and cloud technologies (e.g., Azure)"
    },
    {
        "skill": "Agile/SCRUM SDLC experience",
        "reference": "Problem-solving aptitude, with a willingness to work in a fast-paced product development environment"
    },
    {
        "skill": "Problem-solving aptitude",
        "reference": "Problem-solving aptitude, with a willingness to work in a fast-paced product development environment"
    },
    {
        "skill": "ETL, Backend Development",
        "reference": "Key Responsibilities ETL & Backend Development"
    },
    {
        "skill": "Data Architecture, Data Science Support",
        "reference": "Key Responsibilities ETL & Backend Development"
    },
    {
        "skill": "Innovation & Research",
        "reference": "Qualifications Required"
    },
    {
        "skill": "Data engineer, Big data technologies, 5+ years experience",
        "reference": "5+ years of experience as a Data Engineer working in creating batch and real-time data systems. * Hands-on experience with big data technologies like Hive, Spark, Hadoop."
    },
    {
        "skill": "Cloud certifications, Python or Java programming language",
        "reference": "Proven skills with either Java or Python programming language"
    },
    {
        "skill": "Communication and interpersonal skills",
        "reference": "Excellent written and verbal communication and interpersonal skills"
    },
    {
        "skill": "Data Engineering, SQL, Large Scale Data Processing",
        "reference": "4 years of experience in data engineering, Deep understanding of SQL, Experience with processing large scale data in different formats"
    },
    {
        "skill": "Airflow, Prefect, Cloud Services",
        "reference": "Experience with orchestration tools such as Airflow or Prefect, Experience with common cloud services (S3, DMS, CloudWatch, Lambda)"
    },
    {
        "skill": "Project Management, Conflict Resolution, Communication",
        "reference": "Strong problem solving and structuring skills, Strong communication skills, Ability to manage conflict and resolve difficult situations"
    },
    {
        "skill": "Data Engineering, ETL & Backend Development, Data Architecture",
        "reference": "ETL & Backend Development:Design and optimize ETL pipelines. Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB. Data Architecture:Design scalable and efficient data models for Cassandra and ScyllaDB."
    },
    {
        "skill": "Data Science Support, Innovation & Research",
        "reference": "Collaborate with data scientists, providing them with clean and reliable datasets. Assist in implementing and scaling data science models. Stay abreast of latest technologies. Recommend technical improvements for data processing and storage."
    },
    {
        "skill": "People Analytics, Data Mining, Social Science Research",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace."
    },
    {
        "skill": "Remote Work, Digital Workplace, Organizational Psychology",
        "reference": "We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation."
    },
    {
        "skill": "Data Analytics, Visualization, Dashboards, Presentations",
        "reference": "Utilize advanced data analytics to understand how we hire and work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "Job DescriptionTitle: Senior Data Engineer"
    },
    {
        "skill": "ETL Development, Technical Analysis, Design, Development, Testing & Deployment",
        "reference": "This individual may work independently or collaborate with a smaller group of developers in similar efforts"
    },
    {
        "skill": "Informatica ETL (on premise and cloud), Master Data Management, Informatica MDM, Informatica IDQ/CDQ, Snowflake, Python, Spark experience",
        "reference": "Requirements For All 3 Positions Informatica ETL on premise and on cloud.Master Data Management experience. Informatica MDM experience.Informatica IDQ/CDQ experience.Snowflake, Python, Spark experience"
    },
    {
        "skill": "Architecting, implementing, maintaining cloud-based data pipelines",
        "reference": "Experience with: architecting, implementing, and maintaining cloud-based modern data pipelines"
    },
    {
        "skill": "Working with data storage platforms",
        "reference": "Amazon Redshift, Databricks, Snowflake, or other similar technologies"
    },
    {
        "skill": "Engineering principles for building scalable data platforms",
        "reference": "Pursue high-standard engineering principles to build a scalable, reliable, and maintainable data platform"
    },
    {
        "skill": "Data integration, pipeline development",
        "reference": "4+ years working experience in data integration and pipeline development."
    },
    {
        "skill": "AWS Cloud experience, Apache Spark",
        "reference": "2+ years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDB/DynamoDB ecosystems."
    },
    {
        "skill": "Python development, AWS environment",
        "reference": "Strong real-life experience in python development especially in pySpark in AWS Cloud environment."
    },
    {
        "skill": "Azure Cloud Data Engineer, Cosmos DB",
        "reference": "As a Senior Engineer - DBA, Cloud Data Engineer you will be responsible for using your technical knowledge to solve business problems. We are looking for a talented individual that can serve as a subject matter expert in their area of focus and represent their department on complex assignments."
    },
    {
        "skill": "Microsoft Azure Cloud PaaS technologies",
        "reference": "In this role on the Cloud Data Engineering team, you will be a part of a team responsible for migrating existing data platform solutions to the cloud. You will also collaborate with others to develop new data pipelines."
    },
    {
        "skill": "Cosmos DB/Similar DB technology, Configuration management",
        "reference": "Required: Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by our company for this position now or in the future. Must be committed to incorporating security into all decisions and daily job responsibilities 3+ of related experience in Cosmos DB/Similar DB technology, Experience with configuration management and building automation capabilities such as Git/Jenkins/Bitbucket, Experience with Microsoft Azure Cloud workflows."
    },
    {
        "skill": "Microsoft Azure Platform technologies, Agile methodologies",
        "reference": "Experience with Microsoft Azure platform technologies like Databricks applications, Event Hub, Data Factory, Azure SQL, Synapse Analytics, DeltaLake, Cosmos DB, or DevOps, Prior experience with large-scale projects, Familiarity with JDBC connections to data sources, Knowledge and working experience with Agile methodologies."
    },
    {
        "skill": "Data first mindset",
        "reference": ""
    },
    {
        "skill": "Data Engineering, Leadership, Cloud Expertise",
        "reference": "Responsibilities Build and maintain data integration, ETL pipelines, and data warehouse architectures on cloud platforms like AWS, Azure, and GCP"
    },
    {
        "skill": "Team Management, Technical Mentoring, Innovation",
        "reference": "Provide technical mentorship and leadership to a team of data engineers, and drive innovation and best practices in the engineering organization"
    },
    {
        "skill": "Cross-functional Collaboration, Troubleshooting, Customer Success",
        "reference": "Work closely with the customer success team to troubleshoot and resolve issues related to data quality, ingestion, and integration"
    },
    {
        "skill": "AWS Data Engineering",
        "reference": "About the job"
    },
    {
        "skill": "Remote",
        "reference": "About the job"
    },
    {
        "skill": "100% Remote",
        "reference": "About the job"
    },
    {
        "skill": "5-7+ years experience on AWS platform",
        "reference": "This person should have 5-7+ years or more of data engineering on AWS platform leveraging some of the many AWS services that exist in the ecosystem."
    },
    {
        "skill": "Data engineering",
        "reference": "This person should have 5-7+ years or more of data engineering on AWS platform leveraging some of the many AWS services that exist in the ecosystem."
    },
    {
        "skill": "Strong communicator",
        "reference": "This person should have 5-7+ years or more of data engineering on AWS platform leveraging some of the many AWS services that exist in the ecosystem."
    },
    {
        "skill": "AWS native services experience",
        "reference": "Should have AWS tooling exp - like Glue, Redshift, data analytics. Should have AWS native services exp. Any prior background around dev ops or ETL development before going into AWS is helpful but not at all required as that is not the focus of the job. Certification in AWS Data Analytics would be a plus."
    },
    {
        "skill": "ETL development background",
        "reference": "Should have AWS tooling exp - like Glue, Redshift, data analytics. Should have AWS native services exp. Any prior background around dev ops or ETL development before going into AWS is helpful but not at all required as that is not the focus of the job. Certification in AWS Data Analytics would be a plus."
    },
    {
        "skill": "Certification in AWS Data Analytics",
        "reference": "Should have AWS tooling exp - like Glue, Redshift, data analytics. Should have AWS native services exp. Any prior background around dev ops or ETL development before going into AWS is helpful but not at all required as that is not the focus of the job. Certification in AWS Data Analytics would be a plus."
    },
    {
        "skill": "Azure Cosmos DB, Azure Cloud workflows, Microsoft Azure",
        "reference": "Required Experience Below 4+ years Azure Cosmos DB (full managed NoSQL and Relational DB for App Dev)"
    },
    {
        "skill": "Big Data solutions, Azure SQL, Databricks Delta Lake",
        "reference": "Experience with Big Data solutions (Delta Lake by Databricks and SQL DBMSs)"
    },
    {
        "skill": "Microsoft Azure Cloud workflows, Git/Jenkins/Bitbucket, Azure SQL DB",
        "reference": "Azure SQL DB (Plus)Tasked with Migrating existing data platform solutions to the cloudDevelop new data pipelinesData first mindset. Use Microsoft Azure Cloud PaaS"
    },
    {
        "skill": "Data Engineering",
        "reference": "At Life House, Sr. Data Engineers & Analysts will work in a product environment to build a game changer dynamic pricing AI-based solution."
    },
    {
        "skill": "AI-Based Pricing Solutions",
        "reference": "The Agile team is composed of Data Scientists, Product Managers, Software Engineers, QA's and Revenue Managers. Life House looks to build world-class solutions to the hospitality industry's greatest challenges."
    },
    {
        "skill": "Data Pipelines",
        "reference": "Design, build, and productionize complex data pipelines; Develop data ingestion modules that will feed the AI models"
    },
    {
        "skill": "Data Migration Engineering, ETL Development",
        "reference": "Designing and executing data migrations that allow customers to seamlessly transition critical operations to Mark43."
    },
    {
        "skill": "SQL Development, Database Management",
        "reference": "Comprehensive understanding of the ETL process, experience writing complex SQL scripts, knowledge & understanding of database design, setup, and maintenance."
    },
    {
        "skill": "Public Safety Data Expertise",
        "reference": "Experience working with public safety data systems a plus, interest in becoming an expert in public safety data as well as Mark43's products."
    },
    {
        "skill": "AWS Data Engineer",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "AWS Services",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "Redshift",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "Aurora",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "RDSS",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "S3",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "Lambda",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "AWS Batch",
        "reference": "Role: AWS Data Engineer Job Description"
    },
    {
        "skill": "Data Science Scripting",
        "reference": "Strong SAS/R/Python/PySpark Scripting Experience, Java experience nice to have."
    },
    {
        "skill": "Java experience",
        "reference": "Strong SAS/R/Python/PySpark Scripting Experience, Java experience nice to have."
    },
    {
        "skill": "Familiarity Domino Data Lab",
        "reference": "Familiarity Domino Data Lab, Sagemaker"
    },
    {
        "skill": "Sagemaker",
        "reference": "Familiarity Domino Data Lab, Sagemaker"
    },
    {
        "skill": "6+ years experience in data engineering, analytics engineering",
        "reference": "6+ years of experience as an data engineer, analytics engineer, data warehouse engineer, or a similar role."
    },
    {
        "skill": "Exceptional communication and leadership skills",
        "reference": "Strong understanding of advanced concepts in SQL. Exceptional communication and leadership skills, with a proven ability to facilitating cross-team and cross-functional collaboration and information sharing."
    },
    {
        "skill": "1+ years experience working with SQL driven transform libraries at scale",
        "reference": "1+ years of experience working with SQL-driven transform libraries that support an ELT paradigm, like dbt or sqlmesh, at scale, including setting up CI/CD pipelines that ensure high quality transformations."
    },
    {
        "skill": "Expert in Pl/SQL Development",
        "reference": "Must have working experience with tools like Airflow, Git, Python coding experience and REST API development experience"
    },
    {
        "skill": "Oracle 19 Experience",
        "reference": "Must have working experience with tools like Airflow, Git, Python coding experience and REST API development experience"
    },
    {
        "skill": "Pl/SQL Development Expertise",
        "reference": "Expert In Pl/SQL Development Work Experience With Oracle 19"
    },
    {
        "skill": "Experience with Oracle",
        "reference": "Expert In Pl/SQL Development Work Experience With Oracle 19"
    },
    {
        "skill": "Airflow",
        "reference": "Must have working experience with tools like Airflow, Git and Python coding experience"
    },
    {
        "skill": "Git",
        "reference": "Must have working experience with tools like Airflow, Git and Python coding experience"
    },
    {
        "skill": "Python Coding",
        "reference": "Must have working experience with tools like Airflow, Git and Python coding experience"
    },
    {
        "skill": "REST API Development Experience",
        "reference": "Plus Would Be Experience With Accounting And Financial Datas"
    },
    {
        "skill": "Accounting And Financial Data",
        "reference": "Plus Would Be Experience With Accounting And Financial Datas"
    },
    {
        "skill": "Data Warehousing",
        "reference": "Candidates need to have the drive to learn new technologies, and can pick up new technologies rapidly, able to self-manage a growing list of tasks and priorities"
    },
    {
        "skill": "Able to Learn New Technologies Rapidly",
        "reference": "Candidates need to have the drive to learn new technologies, and can pick up new technologies rapidly, able to self-manage a growing list of tasks and priorities"
    },
    {
        "skill": "Self-management",
        "reference": "Candidates need to have the drive to learn new technologies, and can pick up new technologies rapidly, able to self-manage a growing list of tasks and priorities"
    },
    {
        "skill": "Houston Local Preference",
        "reference": "Remote or Hybrid: We prefer someone local to Houston, who is willing to come to the office at least 3 days a week. But remote option is ok, if you are able to find great candidates"
    },
    {
        "skill": "Remote Option Acceptable",
        "reference": "Remote or Hybrid: We prefer someone local to Houston, who is willing to come to the office at least 3 days a week. But remote option is ok, if you are able to find great candidates"
    },
    {
        "skill": "Data Engineering, Dynamics 365, .NET",
        "reference": "As a Data Engineer, you will be responsible for designing, developing, and maintaining robust data solutions using the Microsoft Dynamics 365 platform and .NET technologies."
    },
    {
        "skill": "Cross-Functional Collaboration, Gathering Requirements, Translating Specifications",
        "reference": "Collaborate with stakeholders to gather requirements and translate them into technical specifications."
    },
    {
        "skill": "Designing Efficient Data Models, Databases, and Integration Workflows",
        "reference": "Design efficient data models, databases, and data integration workflows."
    },
    {
        "skill": "AWS/Data Engineer",
        "reference": "Mid-level AWS/Data Engineer responsible for designing and implementing data processing pipelines using AWS Glue, collaborating with clients."
    },
    {
        "skill": "Designing and Implementation of Data Pipelines",
        "reference": "Mid-level AWS/Data Engineer responsible for designing and implementing data processing pipelines using AWS Glue, collaborating with clients."
    },
    {
        "skill": "Experience with AWS Services",
        "reference": "Must-haves: Familiarity with AWS data services and modules. 5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline). 3+ years of experience with data migration."
    },
    {
        "skill": "AWS Data Migration",
        "reference": "Must-haves: Familiarity with AWS data services and modules. 5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline). 3+ years of experience with data migration."
    },
    {
        "skill": "SQLs",
        "reference": "Must-haves: Familiarity with AWS data services and modules. 5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline). 3+ years of experience with data migration."
    },
    {
        "skill": "Informatica",
        "reference": "Experience with Informatica and 3+ years of experience with data analysis, SQLs."
    },
    {
        "skill": "Data Analysis",
        "reference": "Experience with Informatica and 3+ years of experience with data analysis, SQLs."
    },
    {
        "skill": "SQL",
        "reference": "Experience with Structured Query Language (SQL), should be able to analyze, compare and profiling data sets."
    },
    {
        "skill": "Data Sets Analysis",
        "reference": "Experience with Structured Query Language (SQL), should be able to analyze, compare and profiling data sets."
    },
    {
        "skill": "Teamwork",
        "reference": "Ability to work in globally distributed teams. Excellent analytical, troubleshooting, and problem-solving skills. Excellent communicator (written and verbal)."
    },
    {
        "skill": "Communication Skills",
        "reference": "Ability to work in globally distributed teams. Excellent analytical, troubleshooting, and problem-solving skills. Excellent communicator (written and verbal)."
    },
    {
        "skill": "Lead Cloudera Engineer",
        "reference": "Responsibilities Include Immediate need for a Lead Cloudera Engineer with specific knowledge in Apache Solr."
    },
    {
        "skill": "Big Data Systems and Apache Solr",
        "reference": "Hands-on/advanced (expert preferred) level experience in administrating and engineering relational databases, Big Data systems (e.g., Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME."
    },
    {
        "skill": "Team Lead Experience",
        "reference": "Must have at least 3 years of Lead experience (teams of 5+ is preferred)."
    },
    {
        "skill": "Python/Pyspark",
        "reference": "Good engineers who have knowledge of programming in Python/Pyspark functional programming experience is a definite plus."
    },
    {
        "skill": "Functional programming",
        "reference": "Good engineers who have knowledge of programming in Python/Pyspark functional programming experience is a definite plus."
    },
    {
        "skill": "AWS (or any cloud)",
        "reference": "Experience in Cloud and building data flow/pipelines in mandatory AWS is preferred but any cloud is fine."
    },
    {
        "skill": "Cloud and building data flow/pipelines",
        "reference": "Experience in Cloud and building data flow/pipelines in mandatory AWS is preferred but any cloud is fine."
    },
    {
        "skill": "Distributed systems",
        "reference": "Should know how to program distributed systems"
    },
    {
        "skill": "Data pipelines, Datawarehouse",
        "reference": "Should know how to program distributed systems"
    },
    {
        "skill": "Quality engineering of Data Pipelines",
        "reference": "Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc."
    },
    {
        "skill": "Datawarehouse including Snowflake, Redshift",
        "reference": "Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc."
    },
    {
        "skill": "Knowledge of Test Automation",
        "reference": "Knowledge of Test Automation preferred"
    },
    {
        "skill": "Data Engineering",
        "reference": "Senior Data Engineer constructing applications to manage operations on a global scale, collaborating with engineering teams."
    },
    {
        "skill": "Open Source and Big Data Technologies",
        "reference": "Proficiency in Open Source and Big Data technologies, familiarity with Hadoop Cloudera or similar platforms."
    },
    {
        "skill": "Programming Languages (Java, Python)",
        "reference": "Extensive Java programming background and Python programming experience."
    },
    {
        "skill": "Data Engineering",
        "reference": "Working at Atlassian, a Senior Data Engineer to join our Data Engineering Team and build world-class data solutions."
    },
    {
        "skill": "Scalable Pipeline Development",
        "reference": "Experience building scalable data pipelines using Spark (SparkSQL) with Airflow scheduler/executor framework or similar scheduling tools."
    },
    {
        "skill": "Data Modelling & ETL",
        "reference": "Experience designing data models for optimal storage and retrieval to meet product and business requirements."
    },
    {
        "skill": "Data Engineering",
        "reference": "Experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS"
    },
    {
        "skill": "ETL/ELT and Data Warehousing",
        "reference": "Expertise in one or more programming languages (ideally Python). Experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran"
    },
    {
        "skill": "Data Pipelines and Reporting Infrastructure",
        "reference": "Creating data and reporting infrastructure by building and optimizing production-grade data pipelines through the use of continuous integration"
    },
    {
        "skill": "Azure Data Factory, SQL, T-SQL, Stored Procedures, Python, Databricks, Snowflake, Synapse, Power BI",
        "reference": "Position Overview: As a Senior Data Engineer at Tail Wind, you will be responsible for designing, developing, and maintaining data pipelines and systems on the Azure cloud platform."
    },
    {
        "skill": "Data warehousing, methodologies of data warehouse design, problem-solving skills, attention to detail, communication",
        "reference": "Key Responsibilities Design, build, and maintain data pipelines and ETL processes using Azure Data Factory"
    },
    {
        "skill": "Professional development, support for certifications, equal opportunity employer",
        "reference": "Why join Tail Wind? Competitive salary and benefits package, Opportunity to work with cutting-edge technologies, Collaborative and innovative work environment, Professional development opportunities"
    },
    {
        "skill": "ETL & Backend Development",
        "reference": "Design and optimize ETL pipelines, Develop backend systems for data processing using Elixir and database solutions like Cassandra/ScyllaDB."
    },
    {
        "skill": "Data Architecture",
        "reference": "Design scalable and efficient data models for Cassandra and ScyllaDB, Ensure data integrity, quality, and security."
    },
    {
        "skill": "Data Science Support",
        "reference": "Collaborate with data scientists, Provide clean and reliable datasets, Assist in implementing and scaling data science models."
    },
    {
        "skill": "Data Engineer",
        "reference": "Key skills: Data Engineer, MySql, SQL, Java"
    },
    {
        "skill": "MySql",
        "reference": "Key skills: Data Engineer, MySql, SQL, Java"
    },
    {
        "skill": "SQL",
        "reference": "Key skills: Data Engineer, MySql, SQL, Java"
    },
    {
        "skill": "Java",
        "reference": "Key skills: Data Engineer, MySql, SQL, Java"
    },
    {
        "skill": "8+ years\u2019 experience as a Data Engineer/ Developer",
        "reference": ""
    },
    {
        "skill": "Well versed with MySQL database queries and creation of database views.",
        "reference": ""
    },
    {
        "skill": "Development experience with REST, SOAP, LDAP, MySQL",
        "reference": ""
    },
    {
        "skill": "Development experience with Java applications",
        "reference": ""
    },
    {
        "skill": "Azure Data bricks",
        "reference": "Required Skills: Azure Data bricks, Nice to have skills: Azure Data Factory, Python"
    },
    {
        "skill": "Python",
        "reference": "Required Skills: Azure Data bricks, Nice to have skills: Azure Data Factory, Python"
    },
    {
        "skill": "Data Lake Store",
        "reference": "Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW. Hands-on Experience in SQL Server project development"
    },
    {
        "skill": "SQL DW",
        "reference": "Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW. Hands-on Experience in SQL Server project development"
    },
    {
        "skill": "SQL Server",
        "reference": "Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW. Hands-on Experience in SQL Server project development"
    },
    {
        "skill": "Tuning & Optimizing SQL Code",
        "reference": "Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW. Hands-on Experience in SQL Server project development"
    },
    {
        "skill": "Cosmos DB",
        "reference": "Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW. Hands-on Experience in SQL Server project development"
    },
    {
        "skill": "ADF Routines",
        "reference": "Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW. Hands-on Experience in SQL Server project development"
    },
    {
        "skill": "Azure Developer (Ramp Up)",
        "reference": "The Client is looking for AZURE DEVELOPER (RAMP UP). Candidates must have advanced knowledge on Azure Cloud components"
    },
    {
        "skill": "SQL Server",
        "reference": "The Client is looking for AZURE DEVELOPER (RAMP UP). Candidates must have advanced knowledge on Azure Cloud components"
    },
    {
        "skill": "View Development",
        "reference": "The Client is looking for AZURE DEVELOPER (RAMP UP). Candidates must have advanced knowledge on Azure Cloud components"
    },
    {
        "skill": "Stored Procedures & Functions",
        "reference": "The Client is looking for AZURE DEVELOPER (RAMP UP). Candidates must have advanced knowledge on Azure Cloud components"
    },
    {
        "skill": "Cosmos DB",
        "reference": "Work on ADF routines to load data from Azure Data Lake Store to Cosmos DB"
    },
    {
        "skill": "Collections, Partitions",
        "reference": "Work on ADF routines to load data from Azure Data Lake Store to Cosmos DB"
    },
    {
        "skill": "Business Analysts/Users",
        "reference": "Work closely with business analysts/users / API / Reporting to understand which entities are expected for exposing to Users"
    },
    {
        "skill": "API/Reporting",
        "reference": "Work closely with business analysts/users / API / Reporting to understand which entities are expected for exposing to Users"
    },
    {
        "skill": "Data Exposure for Users",
        "reference": "Work closely with business analysts/users / API / Reporting to understand which entities are expected for exposing to Users"
    },
    {
        "skill": "Customer Needs",
        "reference": "Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns, a high level of focus and attention to detail, Strong work ethic with good time management with the ability to work with diverse teams"
    },
    {
        "skill": "Time Management",
        "reference": "Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns, a high level of focus and attention to detail, Strong work ethic with good time management with the ability to work with diverse teams"
    },
    {
        "skill": "Diverse Teams",
        "reference": "Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns, a high level of focus and attention to detail, Strong work ethic with good time management with the ability to work with diverse teams"
    },
    {
        "skill": "Strong Work Ethic",
        "reference": "Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns, a high level of focus and attention to detail, Strong work ethic with good time management with the ability to work with diverse teams"
    },
    {
        "skill": "Data Conversion",
        "reference": "Work as Data Conversion Engineer in InvoiceCloud, convert data for clients during transitions or upgrades"
    },
    {
        "skill": "ETL Experience",
        "reference": "Designing the target data format, extract existing data, perform transform & load"
    },
    {
        "skill": "Automation",
        "reference": "Develop automated scripts and tools to facilitate repeatable conversions"
    },
    {
        "skill": "Data Engineering",
        "reference": "With a startup spirit and 90,000+ curious and courageous minds"
    },
    {
        "skill": "SQL Server SSIS",
        "reference": "Strong understanding of SQL Server SSIS"
    },
    {
        "skill": "Team Management",
        "reference": "10+ to 15 years of experience Seniority Level - Mid-Senior Management Experience Required - No Minimum Education - bachelor\u2019s degree Willingness to Travel - Occasionally"
    },
    {
        "skill": "Machine Learning",
        "reference": "Driving innovation through machine learning and building ML systems."
    },
    {
        "skill": "Applied Machine Learning",
        "reference": "As a Staff Data Engineer specializing in Applied Machine Learning."
    },
    {
        "skill": "Leadership in ML System Development",
        "reference": "Proven experience in leadership role driving ML system development."
    },
    {
        "skill": "Cloud technologies",
        "reference": "Required Qualifications: Hands on-experience working with cloud technologies - GCP knowledge strongly preferred, ETL development experience with strong SQL background"
    },
    {
        "skill": "GCP knowledge preferred",
        "reference": "Required Qualifications: Hands on-experience working with cloud technologies - GCP knowledge strongly preferred, ETL development experience with strong SQL background"
    },
    {
        "skill": "ETL development experience",
        "reference": "Required Qualifications: Hands on-experience working with cloud technologies - GCP knowledge strongly preferred, ETL development experience with strong SQL background"
    },
    {
        "skill": "SQL background",
        "reference": "Required Qualifications: Hands on-experience working with cloud technologies - GCP knowledge strongly preferred, ETL development experience with strong SQL background"
    },
    {
        "skill": "NoSQL databases",
        "reference": "Hands-on experience of building and operationalizing data processing systems - Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka, Experience with any traditional RDBMS (e.g., Teradata, Oracle, DB2), Experience working with data platforms"
    },
    {
        "skill": "Python/R, Scala, Java, Hive, Spark, Kafka",
        "reference": "Hands-on experience of building and operationalizing data processing systems - Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka, Experience with any traditional RDBMS (e.g., Teradata, Oracle, DB2), Experience working with data platforms"
    },
    {
        "skill": "Experience with RDBMS",
        "reference": "Hands-on experience of building and operationalizing data processing systems - Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka, Experience with any traditional RDBMS (e.g., Teradata, Oracle, DB2), Experience working with data platforms"
    },
    {
        "skill": "Data platforms",
        "reference": "Hands-on experience of building and operationalizing data processing systems - Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka, Experience with any traditional RDBMS (e.g., Teradata, Oracle, DB2), Experience working with data platforms"
    },
    {
        "skill": "Data warehouse",
        "reference": "Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka - Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)"
    },
    {
        "skill": "Data Lake",
        "reference": "Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka - Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)"
    },
    {
        "skill": "ODS",
        "reference": "Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka - Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)"
    },
    {
        "skill": "CI/CD pipelines",
        "reference": "Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka - Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)"
    },
    {
        "skill": "GCP experience",
        "reference": "Preferred Qualifications: GCP (google cloud platform) experience, Python"
    },
    {
        "skill": "Python",
        "reference": "Preferred Qualifications: GCP (google cloud platform) experience, Python"
    },
    {
        "skill": "Healthcare/Clinical data",
        "reference": "Experience working on healthcare / clinical data, Data analysis / Data mapping skills"
    },
    {
        "skill": "Data analysis",
        "reference": "Experience working on healthcare / clinical data, Data analysis / Data mapping skills"
    },
    {
        "skill": "Data mapping skills",
        "reference": "Experience working on healthcare / clinical data, Data analysis / Data mapping skills"
    },
    {
        "skill": "JSON and XML",
        "reference": "Preferred Qualifications: GCP (google cloud platform) experience, Python - Experience working on healthcare / clinical data, Data analysis / Data mapping skills, JSON and XML - Familiarity with HL7 / FHIR"
    },
    {
        "skill": "Familiarity with HL7 / FHIR",
        "reference": "Preferred Qualifications: GCP (google cloud platform) experience, Python - Experience working on healthcare / clinical data, Data analysis / Data mapping skills, JSON and XML - Familiarity with HL7 / FHIR"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a founding member of this team, you will be a key contributor to every critical decision, and responsible for driving the design, development, and governance of Shiftsmart's data. Your expertise in software engineering, data systems, and data governance will play a critical role in building scalable and efficient data platforms and tools while ensuring data integrity and compliance."
    },
    {
        "skill": "Leadership",
        "reference": "Collaborate with stakeholders to understand product and business needs, and translate them into scalable and reliable data systems and tools, while ensuring data quality, privacy, and compliance. Drive automation initiatives by developing scripts, utilities, and frameworks to streamline data processes, improve efficiency, and enforce data governance practices."
    },
    {
        "skill": "Data Governance",
        "reference": "Champion and enforce data governance practices, including data lineage, metadata management, data quality controls, and privacy regulations. Design and develop large-scale data systems, including databases, data warehouses, and big data platforms, with a strong focus on data governance and compliance requirements."
    },
    {
        "skill": "Data Architecture",
        "reference": "Design and develop large-scale data systems, including databases, data warehouses, and big data platforms, with a strong focus on data governance and compliance requirements. Apply software engineering best practices to build robust and maintainable data solutions, ensuring code quality, performance, and scalability in line with data governance guidelines."
    },
    {
        "skill": "Expertise in SQL",
        "reference": "Profound knowledge of SQL to craft intricate queries, especially in BigQuery"
    },
    {
        "skill": "Python Mastery",
        "reference": "Proficient in Python scripting for developing automation tasks"
    },
    {
        "skill": "BigQuery Proficiency",
        "reference": "Ability to optimize for performance and deeply understand BigQuery's unique functions, using it as a primary tool for data validation"
    },
    {
        "skill": "Passion for Automation, Optimization and Tuning",
        "reference": "Should be passionate about automation, optimizations and tuning."
    },
    {
        "skill": "Data Compute & Storage Experience",
        "reference": "Key Qualifications: Passionate about Data Compute and Storage Technologies"
    },
    {
        "skill": "Configuration Management Expertise",
        "reference": "Experience in configuration management (Spinnaker, Helm, Terraform, Crossplanes, Puppet or similar)"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Azure Data Factory, SQL, Stored Procedures, Python, Databricks, Snowflake, Synapse, Power BI",
        "reference": "Position Overview: As a Senior Data Engineer at Tail Wind, you will be responsible for designing, developing, and maintaining data pipelines and systems on the Azure cloud platform."
    },
    {
        "skill": "Bachelor's degree in Computer Science, Information Technology or related field (or equivalent work experience)",
        "reference": "About Us: Tail Wind Informatics Corporation is a Microsoft Solutions Partner- specializing in delivering Data Architecture and Business Intelligence solutions. We are currently seeking a talented and experienced Senior Data Engineer to join our team and play a crucial role in managing and optimizing data infrastructure."
    },
    {
        "skill": "5+ years of experience as a Data Engineer with a focus on Microsoft technologies",
        "reference": "Qualifications: Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience) 5+ years of experience as a Data Engineer with a focus on Microsoft technologies."
    },
    {
        "skill": "Data warehousing expertise",
        "reference": "Personal experience modeling and building data warehouses"
    },
    {
        "skill": "People analytics, data mining/science",
        "reference": "Bring your people analytics, social science research and data mining/science skills"
    },
    {
        "skill": "Advanced data analytics",
        "reference": "Utilize advanced data analytics to understand how we hire and work (productivity, happiness and effectiveness) across a global, remote first organisation"
    },
    {
        "skill": "Data analytics and visualization",
        "reference": "Focus on quantitative and qualitative data analytics to find insights and meaningful business outcomes"
    },
    {
        "skill": "Data Engineering Technologies",
        "reference": "Expertise in SQL and NoSQL databases, data warehousing, and data modeling."
    },
    {
        "skill": "Data Architecture & Analytics",
        "reference": "Strong understanding of data architecture and ability to work with large and complex data sets."
    },
    {
        "skill": "Scalable & Maintainable Data Systems",
        "reference": "Collaborate with other engineers to build scalable and maintainable data systems."
    },
    {
        "skill": "XML, Relational Databases, N-DEx IEPDs, GJXDM, NEIM, Data Integration",
        "reference": "Ideal candidate will have practical experience in these areas"
    },
    {
        "skill": "Secure File Transfer Protocols (SFTP), Web Services for data submissions and user access",
        "reference": "Experience in managing SFTP and web services"
    },
    {
        "skill": "Java, Oracle databases, PL/SQL, SQL Server, Python, Subversion in XML, XSD, XSLT",
        "reference": "Proficiency with relevant technologies mentioned"
    },
    {
        "skill": "Data/Machine Learning Engineering",
        "reference": "We are looking for a Data/Machine Learning Engineer"
    },
    {
        "skill": "Complex data and Client processing pipelines",
        "reference": "Partner with teammates to create complex data and Client processing pipelines"
    },
    {
        "skill": "Scalable implementations of models",
        "reference": "Collaborate with Data Scientists in order to design scalable implementations of their models"
    },
    {
        "skill": "ML/CD4ML practices",
        "reference": "Help to design and develop end-to-end MLOps/CD4ML practices as part of a diverse team"
    },
    {
        "skill": "MLOps collaboration & communication",
        "reference": "Communicate with MLOps and stakeholders on project insights and enable the team to make informed decisions"
    },
    {
        "skill": "TDD, Continuous delivery",
        "reference": "Pair to write clean and iterative code using TDD and leverage various continuous delivery practices"
    },
    {
        "skill": "Deploying machine learning systems in production",
        "reference": "Experience with deploying and operating machine learning systems in production, including in cloud environments"
    },
    {
        "skill": "Data infrastructure & operations for ML",
        "reference": "Understanding of data infrastructure and operations needs for machine learning, including automation and how to operate them on premise and in cloud environments"
    },
    {
        "skill": "Model Serving, Model Deployment, and Model Observability",
        "reference": "Assess and speak to the tradeoffs of using different approaches to Model Serving, Model Deployment, and Model Observability"
    },
    {
        "skill": "Distributed storage platforms & processing tools",
        "reference": "Experience with building large-scale data pipelines and data-centric applications in production settings using various distributed platforms"
    },
    {
        "skill": "End-to-end CD4ML practices design",
        "reference": "Understand and design suitable end-to-end CD4ML practices, depending on project settings"
    },
    {
        "skill": "Cloud environments experience",
        "reference": "Comfortable taking data-driven approaches and applying data security strategy to solve business problems; Excited about data infrastructure and operations in cloud environments"
    },
    {
        "skill": "Big data architecture & pipeline maintenance",
        "reference": "Experience in Big data architecture build and operate data pipelines, maintain data storage, all within distributed systems"
    },
    {
        "skill": "Kafka Data Streaming",
        "reference": "Required Skills"
    },
    {
        "skill": "T-SQL/SQL",
        "reference": "Required Skills"
    },
    {
        "skill": "DataStage or Informatica",
        "reference": "Required Skills"
    },
    {
        "skill": "ETL development",
        "reference": "Familiarity with ETL development, especially in Snowflake. AWS experience, including Redshift or other data-related modules."
    },
    {
        "skill": "Snowflake",
        "reference": "Familiarity with ETL development, especially in Snowflake. AWS experience, including Redshift or other data-related modules."
    },
    {
        "skill": "AWS experience (Redshift or other data-related modules)",
        "reference": "Familiarity with ETL development, especially in Snowflake. AWS experience, including Redshift or other data-related modules."
    },
    {
        "skill": "Data Engineering, Python, Microservices",
        "reference": "Design, build and maintain b.well\u2019s data pipeline infrastructure using Python, Spark, Prefect, Kubernetes and other modern technologies"
    },
    {
        "skill": "Data Architecture, Data Security",
        "reference": "Safeguard sensitive data by following policies and training concerning your security and privacy responsibilities"
    },
    {
        "skill": "Mentoring, Team Leadership",
        "reference": "Help lead other developers to improve their career development and coding abilities; Lead a team of data engineers to build data pipelines and infrastructure"
    },
    {
        "skill": "PySpark",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADF",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADLS",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Delta tables",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "SparkSQL",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Architecture experience",
        "reference": "Architects must have Architecture experience designing solutions and frameworks ground up"
    },
    {
        "skill": "Design solutions and frameworks",
        "reference": "Architects must have Architecture experience designing solutions and frameworks ground up"
    },
    {
        "skill": "Azure Data Factory",
        "reference": "These requirements must be strong and reflected on the resume: Azure Data Factory, Azure Pyspark, Databricks APIs"
    },
    {
        "skill": "Azure Pyspark",
        "reference": "These requirements must be strong and reflected on the resume: Azure Data Factory, Azure Pyspark, Databricks APIs"
    },
    {
        "skill": "Databricks APIs",
        "reference": "These requirements must be strong and reflected on the resume: Azure Data Factory, Azure Pyspark, Databricks APIs"
    },
    {
        "skill": "Data Engineering",
        "reference": "Understanding data in its context/insurance world sales/insurance/broker data for example"
    },
    {
        "skill": "Insurance Experience",
        "reference": "Understanding data in its context/insurance world sales/insurance/broker data for example"
    },
    {
        "skill": "Python and R",
        "reference": "Understanding data in its context/insurance world sales/insurance/broker data for example"
    },
    {
        "skill": "2-3 Years Insurance",
        "reference": "Ideal candidate 2-3 years of insurance p&c side/insurance5-10 years of data engineering"
    },
    {
        "skill": "5-10 Years Data Engineering",
        "reference": "Ideal candidate 2-3 years of insurance p&c side/insurance5-10 years of data engineering"
    },
    {
        "skill": "Azure Databricks",
        "reference": "Ideal candidate 2-3 years of insurance p&c side/insurance5-10 years of data engineering"
    },
    {
        "skill": "Python and R",
        "reference": "DAX or AAS is not required, but helpfulETL"
    },
    {
        "skill": "Azure Databricks",
        "reference": "DAX or AAS is not required, but helpfulETL"
    },
    {
        "skill": "Sharp Young Talent",
        "reference": "Will take sharp young talent who knows insurance and data engineering."
    },
    {
        "skill": "Insurance Knowledge",
        "reference": "Will take sharp young talent who knows insurance and data engineering."
    },
    {
        "skill": "Data Engineering Skills",
        "reference": "Will take sharp young talent who knows insurance and data engineering."
    },
    {
        "skill": "data engineering, data analytics",
        "reference": "We\u2019re looking for people with a strong background in data engineering and analytics to help us scale while maintaining correct and complete data."
    },
    {
        "skill": "statistical techniques, machine learning",
        "reference": "The Data Science team builds data and intelligence into our product, sales, and operations."
    },
    {
        "skill": "data pipelines, scalable solutions",
        "reference": "You\u2019ll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs. Your work will provide teams with visibility into how Stripe\u2019s products are being used and how we can better serve our customers."
    },
    {
        "skill": "data-driven decisions",
        "reference": "You\u2019ll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs. Your work will provide teams with visibility into how Stripe\u2019s products are being used and how we can better serve our customers."
    },
    {
        "skill": "data-driven product development",
        "reference": "Your work will provide teams with visibility into how Stripe\u2019s products are being used and how we can better serve our customers."
    },
    {
        "skill": "statistical modeling, econometric models",
        "reference": "The Data Science team applies and generalizes statistical and econometric models on large datasets"
    },
    {
        "skill": "data infrastructure, data structure management",
        "reference": "Help the Data Science team apply and generalize statistical and econometric models on large datasets; Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves."
    },
    {
        "skill": "SLA management",
        "reference": "Develop strong subject matter expertise and manage the SLAs for those data pipelines"
    },
    {
        "skill": "cross-functional communication, stakeholder management",
        "reference": "The ability to communicate cross-functionally with solid stakeholder management to derive requirements and architect scalable solutions."
    },
    {
        "skill": "Data Engineering, AI-driven Solutions, Chatbot Development",
        "reference": "Design, develop and maintain data pipelines, utilize hands-on experience with Google Dialogflow, BigQuery database for chatbots and automation."
    },
    {
        "skill": "Healthcare Data Compliance, AWS Cloud Expertise",
        "reference": "HIPAA, GDPR knowledge, proficiency in AWS cloud technologies"
    },
    {
        "skill": "Machine Learning, Problem-solving, Collaboration",
        "reference": "Strong understanding of AI and machine learning concepts, excellent problem-solving skills and attention to detail, communication and teamwork."
    },
    {
        "skill": "Sr. Data Engineer",
        "reference": "Jenni Kayne is looking for a Sr. Data with expertise in design, development, test and deployment of large Lake House (Data Lake and enterprise data warehouse) data solutions using cloud technologies and modern data stack."
    },
    {
        "skill": "Data Architecture",
        "reference": "As the Senior Data Engineer, your primary responsibilities include building data pipelines, creating strategy for master data, developing and following data integration and data quality standards across all development initiatives."
    },
    {
        "skill": "Master Data Management",
        "reference": "Create strategy for master data for customer, product by unifying disparate source of customer and product across digital and offline channels (retail)."
    },
    {
        "skill": "PySpark, ADF, ADLS, Delta tables, SparkSQL",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL"
    },
    {
        "skill": "Architecture experience designing solutions and frameworks",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup"
    },
    {
        "skill": "Good communication and teamwork",
        "reference": "Good communication and willingness to work as a team"
    },
    {
        "skill": "Data Engineering",
        "reference": "About the job"
    },
    {
        "skill": "Python, SQL, Cloud Database",
        "reference": "Desired Experience and Skills"
    },
    {
        "skill": "ETL/ELT, AWS Services, DevOps Tools",
        "reference": "Additional Experience And Skills"
    },
    {
        "skill": "Planning, Achievement-Driven, Initiative",
        "reference": "Behavioural Competencies"
    },
    {
        "skill": "Data Engineering, Predictive Modeling",
        "reference": "As a Data Engineer in healthcare analytics team, you will play crucial role harnessing detailed healthcare claims data and various sources to develop data solutions."
    },
    {
        "skill": "Design Data Solutions, Statistical Techniques",
        "reference": "Key Responsibilities include extracting, transforming, analyzing, aggregating, and interpreting data for predictive model development and designing data solutions."
    },
    {
        "skill": "Data Security, Quality, Assessments, Documentation",
        "reference": "Ensure data governance, data security, data quality, perform assessments on data assets, moderate complexity issues resolution, documentation of processes."
    },
    {
        "skill": "Power System Analytics",
        "reference": "Seeking talented Power System Analytics Engineer"
    },
    {
        "skill": "AI and power system principles",
        "reference": "cutting-edge projects involving analysis of smart grid data"
    },
    {
        "skill": "Data-driven algorithms, AI models",
        "reference": "Developing and maintaining for predicting power grid behavior"
    },
    {
        "skill": "Database query languages",
        "reference": "Establishing and optimizing data processing pipelines"
    },
    {
        "skill": "Advanced machine learning, time series analysis",
        "reference": "Experience with statistical modeling and programming tools for data science"
    },
    {
        "skill": "Electrical engineering, computer science",
        "reference": "An advanced degree in related STEM fields"
    },
    {
        "skill": "Utility analytics experience",
        "reference": "Demonstrated experience in a technology vendor, national laboratory or utility environment"
    },
    {
        "skill": "Python libraries for signal processing",
        "reference": "Expert knowledge of Python libraries for power and energy industry problems"
    },
    {
        "skill": "Electrical distribution system operations",
        "reference": "Familiarity with electrical distribution system practices and technologies"
    },
    {
        "skill": "Python",
        "reference": "Key Skills: Python, Azure Databricks"
    },
    {
        "skill": "Azure Databricks",
        "reference": "Key Skills: Python, Azure Databricks"
    },
    {
        "skill": "XML Shredding and Parsing",
        "reference": "Roles & Responsibilities Develop Python code to read the XML source data cleansed and generate the necessary output files for Azure cloud ingestion. XML Shredding and Parsing need to be done, Prepare Requirement specification, Tracker & Traceability Matrix."
    },
    {
        "skill": "Unit Testing",
        "reference": "Roles & Responsibilities Develop Python code to read the XML source data cleansed and generate the necessary output files for Azure cloud ingestion. XML Shredding and Parsing need to be done, Prepare Requirement specification, Tracker & Traceability Matrix."
    },
    {
        "skill": "Issue Clarification & Resolution",
        "reference": "Roles & Responsibilities Develop Python code to read the XML source data cleansed and generate the necessary output files for Azure cloud ingestion. XML Shredding and Parsing need to be done, Prepare Requirement specification, Tracker & Traceability Matrix."
    },
    {
        "skill": "Project Planning",
        "reference": "Understand existing applications/systems, if applicable, UAT / QA support. Project Planning and Set-up- Understand the project scope, identify activities/ tasks, task level estimates, schedule, dependencies, and risks, and provide inputs to Module Lead for review Provide inputs to testing strategy, configuration, deployment, hardware/software requirement, etc."
    },
    {
        "skill": "Testing Strategy",
        "reference": "Understand existing applications/systems, if applicable, UAT / QA support. Project Planning and Set-up- Understand the project scope, identify activities/ tasks, task level estimates, schedule, dependencies, and risks, and provide inputs to Module Lead for review Provide inputs to testing strategy, configuration, deployment, hardware/software requirement, etc."
    },
    {
        "skill": "Enterprise Metadata Definition",
        "reference": "Understand existing applications/systems, if applicable, UAT / QA support. Project Planning and Set-up- Understand the project scope, identify activities/ tasks, task level estimates, schedule, dependencies, and risks, and provide inputs to Module Lead for review Provide inputs to testing strategy, configuration, deployment, hardware/software requirement, etc."
    },
    {
        "skill": "Data Engineering, Interpersonal Skills, Communication Skills",
        "reference": "The ideal candidate will have outstanding technical, interpersonal, communication skills along with fundamental technique troubleshooting abilities."
    },
    {
        "skill": "Data Integrations & Architecture Design",
        "reference": "Design data integrations and data quality framework. Plan and conduct data engineering projects."
    },
    {
        "skill": "Healthcare Data Analytics, VHA Data Experience",
        "reference": "Experience using CDW. Demonstrated work experience with VHA data. Experience with ICD9 / 10 diagnosis and procedural codes and CPT procedural codes."
    },
    {
        "skill": "Pyspark",
        "reference": "Excellent (Hands-On) in Azure, ADF, Databricks (PySpark), Python, SQL, Unix Shell scripting"
    },
    {
        "skill": "Azure",
        "reference": "Excellent (Hands-On) in Azure, ADF, Databricks (PySpark), Python, SQL, Unix Shell scripting"
    },
    {
        "skill": "Matillion",
        "reference": "Excellent (Hands-On) in Azure, ADF, Databricks (PySpark), Python, SQL, Unix Shell scripting"
    },
    {
        "skill": "Python",
        "reference": "Excellent (Hands-On) in Azure, ADF, Databricks (PySpark), Python, SQL, Unix Shell scripting"
    },
    {
        "skill": "ETL",
        "reference": "Experience in ETL, tools like Matillion, QLIK, DataStage, and performance tuning / optimization"
    },
    {
        "skill": "Matillion",
        "reference": "Experience in ETL, tools like Matillion, QLIK, DataStage, and performance tuning / optimization"
    },
    {
        "skill": "QLIK",
        "reference": "Experience in ETL, tools like Matillion, QLIK, DataStage, and performance tuning / optimization"
    },
    {
        "skill": "DataStage",
        "reference": "Experience in ETL, tools like Matillion, QLIK, DataStage, and performance tuning / optimization"
    },
    {
        "skill": "Snowflake",
        "reference": "Good experience in Snowflake Datawarehouse. Experience in building dimensional data models."
    },
    {
        "skill": "Dimensional data models",
        "reference": "Good experience in Snowflake Datawarehouse. Experience in building dimensional data models."
    },
    {
        "skill": "Data Engineering, Architecture blueprints, Data Analytics",
        "reference": "As a Data Engineer, you'll play a key role in implementing architecture blueprints and conducting data analyses."
    },
    {
        "skill": "Complex Technical Challenges, Collaboration",
        "reference": "Implementing architecture blueprints, solving complex technical challenges and collaborating with teams to create systems enhancing agility and security."
    },
    {
        "skill": "Cloud Data Platform, Security Frameworks, Data Processing Tools",
        "reference": "Design and build production-ready applications for batch and streaming data processing in a multi-tenancy cloud data platform. Build data security frameworks compliant with GDPR and CCPA guidelines."
    },
    {
        "skill": "AWS Data engineer",
        "reference": "About the job"
    },
    {
        "skill": "Familiarity with AWS data services and modules",
        "reference": "About the job"
    },
    {
        "skill": "5+ years of hands-on experience with AWS services",
        "reference": "Must-haves:"
    },
    {
        "skill": "3+ years of experience with data migration, data analysis, SQLs, Informatica",
        "reference": "Must-haves:"
    },
    {
        "skill": "Experience with Structured Query Language (SQL)",
        "reference": "Experience with Structured Query Language (SQL), should be able to analyze, compare and profiling data sets"
    },
    {
        "skill": "Data profiling and analysis",
        "reference": "Experience with Structured Query Language (SQL), should be able to analyze, compare and profiling data sets"
    },
    {
        "skill": "Knowledge of IT processes",
        "reference": "Knowledge of IT processes, including quality assurance, release management, and production support"
    },
    {
        "skill": "Quality assurance, release management, production support",
        "reference": "Knowledge of IT processes, including quality assurance, release management, and production support"
    },
    {
        "skill": "Excellent analytical, troubleshooting, and problem-solving skills",
        "reference": ""
    },
    {
        "skill": "Ability to work in globally distributed teams",
        "reference": ""
    },
    {
        "skill": "Good communication",
        "reference": "Excellent communicator (written and verbal)"
    },
    {
        "skill": "Formal and informal",
        "reference": "Excellent communicator (written and verbal)"
    },
    {
        "skill": "Flexible, self-motivated",
        "reference": "Flexible and proactive/self-motivated working style with strong personal ownership"
    },
    {
        "skill": "Strong personal ownership",
        "reference": "Flexible and proactive/self-motivated working style with strong personal ownership"
    },
    {
        "skill": "Ability to multi-task, prioritize under pressure",
        "reference": "Ability to multi-task and prioritize under pressure.Ability to work independently with minimal supervision as well as in a team environment."
    },
    {
        "skill": "Work independently or in a team environment",
        "reference": "Ability to multi-task and prioritize under pressure.Ability to work independently with minimal supervision as well as in a team environment."
    },
    {
        "skill": "Undergraduate/Graduate degree in Computer Science",
        "reference": "Undergraduate or graduate degree in Computer Science, Data Science, or equivalent education/professional experience is required"
    },
    {
        "skill": "Data Science or equivalent experience",
        "reference": "Undergraduate or graduate degree in Computer Science, Data Science, or equivalent education/professional experience is required"
    },
    {
        "skill": "People Analytics, Data Mining/Science",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of the digital workplace."
    },
    {
        "skill": "Remote Workplace Research, Productivity Insights",
        "reference": "Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company."
    },
    {
        "skill": "Data Analytics & Visualization, SQL/Databases",
        "reference": "Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "People Analytics, Social Science Research, Data Mining/Science",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of the digital workplace."
    },
    {
        "skill": "Data Analytics, Visualisation",
        "reference": "Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisation. Focus on quantitative and qualitative data analytics to find insights and meaningful business outcomes."
    },
    {
        "skill": "Advanced Statistics, Coding/Scripting Languages, Databases",
        "reference": "In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace. Location: This role will be based remotely in the AMER region."
    },
    {
        "skill": "Data architecture, Acquisition/contract management, Expertise in IT",
        "reference": "Developing and maintaining data architectures, engaging in acquisition/contract management"
    },
    {
        "skill": "Strategy & transformation management, Investment analysis, Governance",
        "reference": "Changeis focuses on delivering unparalleled expertise"
    },
    {
        "skill": "Business management, Strategic planning, Decision-making",
        "reference": "Partner with Federal Agency Office of Human Resources, focusing on essential areas"
    },
    {
        "skill": "Senior Data Engineering Development",
        "reference": "About the job"
    },
    {
        "skill": "Visionary Leadership, Collaboration, Infrastructure Excellence",
        "reference": "Responsibilities"
    },
    {
        "skill": "Data Quality and Dashboards Management",
        "reference": ""
    },
    {
        "skill": "Education & Experience Requirements",
        "reference": "Qualifications"
    },
    {
        "skill": "Technical Proficiencies, Understanding AI/ML Platforms",
        "reference": "Experience"
    },
    {
        "skill": "Competitive Benefits and Employment Details",
        "reference": "Employee Benefits"
    },
    {
        "skill": "Senior Data Engineer, ETL/ELT processes",
        "reference": "As a Senior Data Engineer at Kobie, you will play a vital role within our data engineering team"
    },
    {
        "skill": "Expertise in Snowflake and Kimball style star schemas",
        "reference": "At least 6 years of Data Engineering experience, with a minimum of 2 operating in Snowflake. Deep understanding of Snowflake Data Platform (data sharing, data clean rooms, marketplace)"
    },
    {
        "skill": "Event-driven architectures, real-time data flows",
        "reference": "Design, manage and build event-driven architectures to support real-time data flows and event processing"
    },
    {
        "skill": "PySpark, ADF, ADLS, Delta tables, SparkSQL",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Architecture experience designing solutions and frameworks",
        "reference": "Architects must have Architecture experience designing solutions and frameworks ground up"
    },
    {
        "skill": "Experience with data platforms, implement end-to-end data pipelines",
        "reference": "Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines"
    },
    {
        "skill": "Data Warehousing, Modeling, BI Solutions",
        "reference": "Years in IT with at least 7-8+ years' experience in data warehousing, modelling, end-to-end BI solutions"
    },
    {
        "skill": "Insurance Industry Experience",
        "reference": "Experience developing solutions for the Insurance industry"
    },
    {
        "skill": "Big Data and Cloud Technologies",
        "reference": "Advanced knowledge of SQL and query optimization techniques and approaches"
    },
    {
        "skill": "Power BI, SQL, Spark & PySpark Programming",
        "reference": "Strong SQL, Spark, and PySpark programming skills for data analysis"
    },
    {
        "skill": "Agile/SCRUM SDLC Environment",
        "reference": "Strong troubleshooting and problem-solving skills Experience working in an Agile/SCRUM SDLC environment."
    },
    {
        "skill": "Teamwork & Independence",
        "reference": "Able to work as a team member and willing to work independently when required."
    },
    {
        "skill": "Big Data Azure Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Advanced Analytics",
        "reference": "Responsibilities"
    },
    {
        "skill": "Data Science, Machine Learning, AI",
        "reference": "Our consultants bring deep expertise in"
    },
    {
        "skill": "Data Engineering",
        "reference": "A minimum of 12 years experience in Data Engineering."
    },
    {
        "skill": "Azure",
        "reference": "A minimum of 12 years experience in Data Engineering."
    },
    {
        "skill": "Experience",
        "reference": "A minimum of 12 years experience in Data Engineering."
    },
    {
        "skill": "Azure Databricks",
        "reference": "We are looking for a talented and experienced Azure Data Engineer with expertise..."
    },
    {
        "skill": "Azure Synapse Analytics",
        "reference": "We are looking for a talented and experienced Azure Data Engineer with expertise..."
    },
    {
        "skill": "Azure Data Factory",
        "reference": "We are looking for a talented and experienced Azure Data Engineer with expertise..."
    },
    {
        "skill": "RDBMS",
        "reference": "We are looking for a talented and experienced Azure Data Engineer with expertise..."
    },
    {
        "skill": "NoSQL databases",
        "reference": "We are looking for a talented and experienced Azure Data Engineer with expertise..."
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "Data Pipeline Development: Design, develop, and maintain..."
    },
    {
        "skill": "ETL",
        "reference": "Data Pipeline Development: Design, develop, and maintain..."
    },
    {
        "skill": "Data Modeling",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Data Migration",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Data Integration",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Data Quality Checks",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Problem-Solving",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Communication Skills",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Bachelor's/Master's Degree",
        "reference": "Bachelor's degree in Computer Science, Information Technology, or a related field."
    },
    {
        "skill": "Computer Science/Information Technology",
        "reference": "Bachelor's degree in Computer Science, Information Technology, or a related field."
    },
    {
        "skill": "Proficiency in Azure Databricks/Spark",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "Azure Synapse Analytics",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "Azure Data Factory",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "RDBMS",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "NoSQL databases",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "Data Warehousing Concepts",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "Programming Skills (Python, Microservices)",
        "reference": "Proficiency in languages such as Python and programming skills."
    },
    {
        "skill": "Familiarity with Data Governance/Security",
        "reference": "Proficiency in languages such as Python and programming skills."
    },
    {
        "skill": "Data Migration",
        "reference": "Lead data migration projects, including data extraction, transformation..."
    },
    {
        "skill": "ETL Experience",
        "reference": "Lead data migration projects, including data extraction, transformation..."
    },
    {
        "skill": "AWS/GCP Experience",
        "reference": "Lead data migration projects, including data extraction, transformation..."
    },
    {
        "skill": "Strong SQL Skills",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "Data Modeling",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "Familiarity with RDBMS and NoSQL databases",
        "reference": "Proven experience as a data engineer with a strong focus on Azure data services."
    },
    {
        "skill": "People Analytics",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of digital workplace."
    },
    {
        "skill": "Data Mining/Science",
        "reference": "We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation."
    },
    {
        "skill": "Psychology",
        "reference": "Support analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace."
    },
    {
        "skill": "Data Analytics & Visualization",
        "reference": "Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global remote first organisation."
    },
    {
        "skill": "Research",
        "reference": "Collaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace."
    },
    {
        "skill": "SQL, Azure, Data Factory, DBT",
        "reference": "Top Skills SQL, Azure, Data Factory, DBT"
    },
    {
        "skill": "ETL work, Timely projects",
        "reference": "Proven ability to complete projects in a timely manner while clearly measuring progress"
    },
    {
        "skill": "Software fundamentals, Popular programming languages",
        "reference": "Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java)"
    },
    {
        "skill": "Frontend client apps, Angular, Git, Cloud",
        "reference": "Experience writing and maintaining frontend client applications, Angular preferred, Strong experience with revision control (Git), Experience with cloud-based systems (Azure / AWS / GCP)."
    },
    {
        "skill": "Big data design, Data normalization patterns",
        "reference": "High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns"
    },
    {
        "skill": "Queuing technologies, Metrics, Logging",
        "reference": "Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc), Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools."
    },
    {
        "skill": "Communication skills, RESTful APIs",
        "reference": "Strong communication skills, Strong experience with use of RESTful APIs"
    },
    {
        "skill": "HL7 V2.x / FHIR, System deployment tasks",
        "reference": "High level understanding of HL7 V2.x / FHIR based interface messages. High level understanding of system deployment tasks and technologies."
    },
    {
        "skill": "Engineer scalable systems, Collaboration with teams",
        "reference": "Interface with Electronic Health Records, Engineer scalable, reliable, and performant systems to manage data, Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization."
    },
    {
        "skill": "Building quality systems, Multiple roles",
        "reference": "Build quality systems while expanding offerings to dependent teams, Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Data Pipeline Expertise",
        "reference": "We are currently seeking a Data Engineer with data pipeline expertise for a full-time, permanent position."
    },
    {
        "skill": "TS/SCI Clearance",
        "reference": "We are currently seeking a Data Engineer with data pipeline expertise for a full-time, permanent position."
    },
    {
        "skill": "ETL Processes Management",
        "reference": "Develop and design data pipelines to support an end-to-end solution"
    },
    {
        "skill": "AWS Cloud Services",
        "reference": "Develop and design data pipelines to support an end-to-end solution"
    },
    {
        "skill": "Fault Tolerance & Redundancy",
        "reference": "Design and develop robust and functional data flows to support raw data and expected data."
    },
    {
        "skill": "Dataflows Design",
        "reference": "Design and develop robust and functional data flows to support raw data and expected data."
    },
    {
        "skill": "People Analytics, Psychology, Data Mining",
        "reference": "Support analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace."
    },
    {
        "skill": "Data Analytics, Visualization, Business Insights",
        "reference": "Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "Research, Trend Analysis, Remote Work Expertise",
        "reference": "Collaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace"
    },
    {
        "skill": "Data Engineering",
        "reference": "The Senior Data Engineering II is responsible for development and maintenance of data products and pipelines supporting OncoHealth's OneUM and Iris offerings."
    },
    {
        "skill": "Technical Leadership",
        "reference": "This role is a technical leader who participates in cross-functional projects, designs and implements scalable data solutions, mentors junior engineers, and analysts."
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "The ideal candidate will display demonstratable experience with Databricks, SQL, Python, and data pipelines (Airflow/Prefect, ADF, Workflows), as well as a working knowledge of event-driven architecture and APIs."
    },
    {
        "skill": "Collibra Ranger Certification",
        "reference": "Must have a Collibra Ranger Certification (active or expired) and be a Data Engineer."
    },
    {
        "skill": "Data Engineer",
        "reference": "Must have a Collibra Ranger Certification (active or expired) and be a Data Engineer."
    },
    {
        "skill": "Integrate ServiceNow with Collibra",
        "reference": "Integrate ServiceNow with Collibra, Develop a custom connector bridging ServiceNow and Collibra."
    },
    {
        "skill": "Custom Connector Development",
        "reference": "Integrate ServiceNow with Collibra, Develop a custom connector bridging ServiceNow and Collibra."
    },
    {
        "skill": "Collibra Administration",
        "reference": "Provide solutioning expertise in resolving technical challenges; Support the team as a Collibra administrator."
    },
    {
        "skill": "Solutioning Expertise",
        "reference": "Provide solutioning expertise in resolving technical challenges; Support the team as a Collibra administrator."
    },
    {
        "skill": "API Design",
        "reference": "Design and implement APIs to enhance functionality and extend capabilities on the Collibra platform."
    },
    {
        "skill": "Enhancing Functionality",
        "reference": "Design and implement APIs to enhance functionality and extend capabilities on the Collibra platform."
    },
    {
        "skill": "4-6 Years Experience",
        "reference": "100% remote; Demonstrated 4-6 years of data management experience in an enterprise environment."
    },
    {
        "skill": "Enterprise Data Management",
        "reference": "100% remote; Demonstrated 4-6 years of data management experience in an enterprise environment."
    },
    {
        "skill": "Establishing Collibra Operating Model",
        "reference": "Capable of supporting the establishment of a Collibra operating model aligned with a data governance operating model."
    },
    {
        "skill": "Data Governance",
        "reference": "Capable of supporting the establishment of a Collibra operating model aligned with a data governance operating model."
    },
    {
        "skill": "Automated Bulk Uploads",
        "reference": "Proficient in designing automated and bulk upload metadata import/export methods."
    },
    {
        "skill": "Metadata Import/Export Methods",
        "reference": "Proficient in designing automated and bulk upload metadata import/export methods."
    },
    {
        "skill": "Business & Technical Metadata",
        "reference": "Skilled in assessing, recommending, and implementing capabilities related to business and technical metadata, data lineage, data profiling, data quality improvement efforts, and issue/request management in Collibra."
    },
    {
        "skill": "Data Lineage, Profiling",
        "reference": "Skilled in assessing, recommending, and implementing capabilities related to business and technical metadata, data lineage, data profiling, data quality improvement efforts, and issue/request management in Collibra."
    },
    {
        "skill": "Collibra Ranger Certification",
        "reference": "Collibra Ranger certified - required (would consider Active or Expired certification)"
    },
    {
        "skill": "Active or Expired",
        "reference": "Collibra Ranger certified - required (would consider Active or Expired certification)"
    },
    {
        "skill": "Data Engineering, Scala/Spark, Spark-based cloud analytics infrastructure",
        "reference": "As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing frameworks..."
    },
    {
        "skill": "Database Optimizers, Query Planners, Cluster Communication",
        "reference": "Responsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3. Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication..."
    },
    {
        "skill": "Scalability, Infrastructure Architecture, Best Practices",
        "reference": "Scale up from proof of concept to 'cluster scale' (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure. Codify best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases..."
    },
    {
        "skill": "Data Analysis",
        "reference": "Responsibilities: Data Analysis and organizing raw data"
    },
    {
        "skill": "Organizing Raw Data",
        "reference": "Responsibilities: Data Analysis and organizing raw data"
    },
    {
        "skill": "Building Data Systems",
        "reference": "Responsibilities: Building data systems and pipelines, Preparing data for prescriptive and predictive modelling"
    },
    {
        "skill": "Pipelines",
        "reference": "Responsibilities: Building data systems and pipelines, Preparing data for prescriptive and predictive modelling"
    },
    {
        "skill": "Prescriptive Modelling",
        "reference": "Responsibilities: Building data systems and pipelines, Preparing data for prescriptive and predictive modelling"
    },
    {
        "skill": "Data Pipeline",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "Monitoring",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "Cloud Native AWS",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "Java",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "Python",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "Scala",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "R",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "SQL",
        "reference": "Responsibilities: Pipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc."
    },
    {
        "skill": "Collaboration",
        "reference": "Responsibilities: Collaborating with data scientists and architects across various projects"
    },
    {
        "skill": "Projects",
        "reference": "Responsibilities: Collaborating with data scientists and architects across various projects"
    },
    {
        "skill": "Experience",
        "reference": "What We Expect: Experience with Data lake, building data warehouse ETL, and/or leveraging Cloud based services"
    },
    {
        "skill": "Data Lake",
        "reference": "What We Expect: Experience with Data lake, building data warehouse ETL, and/or leveraging Cloud based services"
    },
    {
        "skill": "ETL",
        "reference": "What We Expect: Experience with Data lake, building data warehouse ETL, and/or leveraging Cloud based services"
    },
    {
        "skill": "Cloud Based Services",
        "reference": "What We Expect: Experience with Data lake, building data warehouse ETL, and/or leveraging Cloud based services"
    },
    {
        "skill": "Expertise",
        "reference": "What We Expect: Good expertise in Spark, Cloud Database related experience (noSQL, Relational DBs), Knowledge of any of these cloud platforms: AWS, Azure, GCP"
    },
    {
        "skill": "Spark",
        "reference": "What We Expect: Good expertise in Spark, Cloud Database related experience (noSQL, Relational DBs), Knowledge of any of these cloud platforms: AWS, Azure, GCP"
    },
    {
        "skill": "Cloud Database",
        "reference": "What We Expect: Good expertise in Spark, Cloud Database related experience (noSQL, Relational DBs), Knowledge of any of these cloud platforms: AWS, Azure, GCP"
    },
    {
        "skill": "Relational DBs",
        "reference": "What We Expect: Good expertise in Spark, Cloud Database related experience (noSQL, Relational DBs), Knowledge of any of these cloud platforms: AWS, Azure, GCP"
    },
    {
        "skill": "AWS",
        "reference": "What We Expect: Good expertise in Spark, Cloud Database related experience (noSQL, Relational DBs), Knowledge of any of these cloud platforms: AWS, Azure, GCP"
    },
    {
        "skill": "Azure",
        "reference": "What We Expect: Good expertise in Spark, Cloud Database related experience (noSQL, Relational DBs), Knowledge of any of these cloud platforms: AWS, Azure, GCP"
    },
    {
        "skill": "GCP",
        "reference": "What We Expect: Good expertise in Spark, Cloud Database related experience (noSQL, Relational DBs), Knowledge of any of these cloud platforms: AWS, Azure, GCP"
    },
    {
        "skill": "Python",
        "reference": "What We Expect: Ability to write robust code with Python, Understanding of data structures, data modeling, and software architecture"
    },
    {
        "skill": "Robust Code",
        "reference": "What We Expect: Ability to write robust code with Python, Understanding of data structures, data modeling, and software architecture"
    },
    {
        "skill": "Data Structures",
        "reference": "What We Expect: Ability to write robust code with Python, Understanding of data structures, data modeling, and software architecture"
    },
    {
        "skill": "Modeling",
        "reference": "What We Expect: Ability to write robust code with Python, Understanding of data structures, data modeling, and software architecture"
    },
    {
        "skill": "Software Architecture",
        "reference": "What We Expect: Ability to write robust code with Python, Understanding of data structures, data modeling, and software architecture"
    },
    {
        "skill": "Analytical",
        "reference": "What We Expect: Analytical and problem solving skills, Strong intermediate English or higher (B1+)"
    },
    {
        "skill": "Problem Solving",
        "reference": "What We Expect: Analytical and problem solving skills, Strong intermediate English or higher (B1+)"
    },
    {
        "skill": "English",
        "reference": "What We Expect: Analytical and problem solving skills, Strong intermediate English or higher (B1+)"
    },
    {
        "skill": "Nice to Have",
        "reference": "Experience in development using R and/or Java"
    },
    {
        "skill": "R",
        "reference": "Experience in development using R and/or Java"
    },
    {
        "skill": "Java",
        "reference": "Experience in development using R and/or Java"
    },
    {
        "skill": "Data Warehouse Solutions",
        "reference": "Responsibilities:Job Information Data/Client Engineer is responsible for solution engineering of enterprise scale data management best practices."
    },
    {
        "skill": "Data Integration Frameworks",
        "reference": "This role will be responsible for developing data integration tasks in the data and analytics space."
    },
    {
        "skill": "Cloud-based Data Design Patterns",
        "reference": "Job Information: This position will report to director of data management group under Data Operations organization. This is an individual performer role."
    },
    {
        "skill": "Data Warehouse Solutions",
        "reference": "Key Job Functions: Demonstrate expert ability in implementing data warehouse solutions using Snowflake."
    },
    {
        "skill": "Building Data Integration Solutions",
        "reference": "Job Information: Building data integration solutions between transaction systems and analytics platform."
    },
    {
        "skill": "Transforming Data for Business Consumption",
        "reference": "Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs."
    },
    {
        "skill": "Security Policies in Snowflake",
        "reference": "Create security policies in Snowflake to manage fine-grained access control."
    },
    {
        "skill": "Data Patterns Development",
        "reference": "Develop tasks for a multitude of data patterns, e.g., real-time data integration, advanced analytics, machine learning, BI and reporting."
    },
    {
        "skill": "Leading Proof-of-Concept Efforts",
        "reference": "Lead POC efforts to build foundational AI/Client services for Predictive Analytics."
    },
    {
        "skill": "Data Product Development",
        "reference": "Building of data products by data enrichment and Client."
    },
    {
        "skill": "Teamwork and Knowledge Sharing",
        "reference": "Be a team player and share knowledge with the existing team members."
    },
    {
        "skill": "PySpark",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADF",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADLS",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Delta tables",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "SparkSQL",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Architecture experience",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup"
    },
    {
        "skill": "Design solutions and frameworks ground up",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup"
    },
    {
        "skill": "Not just build data pipelines using the existing setup",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup"
    },
    {
        "skill": "Azure Data Factory",
        "reference": "These requirements must be strong and reflected on the resume: Azure Data Factory, Azure PySpark, Databricks APIs"
    },
    {
        "skill": "Azure Pyspark",
        "reference": "These requirements must be strong and reflected on the resume: Azure Data Factory, Azure PySpark, Databricks APIs"
    },
    {
        "skill": "Databricks APIs",
        "reference": "These requirements must be strong and reflected on the resume: Azure Data Factory, Azure PySpark, Databricks APIs"
    },
    {
        "skill": "Azure APIs, Multi-tenant data pipeline testing, Parquet files and Data lake",
        "reference": "Experience with Azure APIs, Strong doing multi-tenant data pipeline testing, Hands-on working knowledge of parquet files and data lake"
    },
    {
        "skill": "C# API Automation, Advanced SQL, Entity Framework, Testing using Postman",
        "reference": "Strong C# for backend API Automation, Advanced Microsoft SQL, Experience using Entity Framework, Testing API experience using Postman"
    },
    {
        "skill": "Azure DevOps, UI automation framework (Playwright/Jest), QA Automation",
        "reference": "Azure DevOps experience (CI/CD pipelines) \u2013 Integration of tests and execution, Playwright/ Jest preferred UI automation framework experience, Selenium is fine, Bonus Points for Sr. Software Engineer interested in QA Automation and Data or SDET with an emphasis on Data."
    },
    {
        "skill": "People analytics",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of the digital workplace."
    },
    {
        "skill": "Data mining/science",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of the digital workplace."
    },
    {
        "skill": "Organisational psychology",
        "reference": "In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace."
    },
    {
        "skill": "Data analytics and visualisation",
        "reference": "In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace."
    },
    {
        "skill": "Advanced statistics",
        "reference": "First work experience in People Analytics; Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)."
    },
    {
        "skill": "Coding/scripting languages (Python, R)",
        "reference": "First work experience in People Analytics; Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)."
    },
    {
        "skill": "Databases (SQL)",
        "reference": "First work experience in People Analytics; Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)."
    },
    {
        "skill": "Data analytics",
        "reference": "Utilize advanced data analytics to understand how we hire and work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "Visualization tools (Looker Studio, Tableau)",
        "reference": "Utilize advanced data analytics to understand how we hire and work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "Communication skills",
        "reference": "Utilize advanced data analytics to understand how we hire and work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "Research methodology",
        "reference": "Ability to translate business questions to key research objectives; Ability to identify the best methodology to execute research, synthesize and analyse findings."
    },
    {
        "skill": "Business analysis",
        "reference": "Ability to translate business questions to key research objectives; Ability to identify the best methodology to execute research, synthesize and analyse findings."
    },
    {
        "skill": "Synthesis and analysis of findings",
        "reference": "Ability to translate business questions to key research objectives; Ability to identify the best methodology to execute research, synthesize and analyse findings."
    },
    {
        "skill": "Data Engineer",
        "reference": "We're looking for a Senior Data Engineer to work on our massive data processing pipeline and lead our data lake and data warehouse building."
    },
    {
        "skill": "Senior",
        "reference": "We're looking for a Senior Data Engineer to work on our massive data processing pipeline and lead our data lake and data warehouse building."
    },
    {
        "skill": "Solutions",
        "reference": "We're looking for a Senior Data Engineer to work on our massive data processing pipeline and lead our data lake and data warehouse building."
    },
    {
        "skill": "Designing Databricks Notebooks",
        "reference": "Designs and develops Databricks Notebooks to support data ingestion, curation and provisioning of complex enterprise data."
    },
    {
        "skill": "Data Ingestion",
        "reference": "Designs and develops Databricks Notebooks to support data ingestion, curation and provisioning of complex enterprise data."
    },
    {
        "skill": "Transformation",
        "reference": "Designs and develops Databricks Notebooks to support data ingestion, curation and provisioning of complex enterprise data."
    },
    {
        "skill": "Data Architecture",
        "reference": "Designs and builds data architecture and applications that successfully enable speed, quality, and efficient pipelines."
    },
    {
        "skill": "Pipeline Management",
        "reference": "Designs and builds data architecture and applications that successfully enable speed, quality, and efficient pipelines."
    },
    {
        "skill": "Efficient Pipelines",
        "reference": "Designs and builds data architecture and applications that successfully enable speed, quality, and efficient pipelines."
    },
    {
        "skill": "Data Engineering",
        "reference": "The Data Engineer will be responsible for delivering and maintaining data pipelines, building dashboards, and supporting business metrics."
    },
    {
        "skill": "ETL Pipelines",
        "reference": "The Data Engineer will be responsible for delivering and maintaining data pipelines, building dashboards, and supporting business metrics."
    },
    {
        "skill": "Dashboards",
        "reference": "The Data Engineer will be responsible for delivering and maintaining data pipelines, building dashboards, and supporting business metrics."
    },
    {
        "skill": "Data Processing & Cleaning",
        "reference": "Contribute to the data collection/data cleaning process and have knowledge of SQL and NoSQL database technologies like SQL, Postgres, Cassandra, Mongo etc."
    },
    {
        "skill": "SQL & NoSQL Databases",
        "reference": "Contribute to the data collection/data cleaning process and have knowledge of SQL and NoSQL database technologies like SQL, Postgres, Cassandra, Mongo etc."
    },
    {
        "skill": "Software Development",
        "reference": "Experience in building production grade data platforms using AWS or GCP, and proficiency with Python, Spark, Kafka, RabbitMQ, REST and GraphQL APIs."
    },
    {
        "skill": "Cloud Platforms",
        "reference": "Experience in building production grade data platforms using AWS or GCP, and proficiency with Python, Spark, Kafka, RabbitMQ, REST and GraphQL APIs."
    },
    {
        "skill": "Data Protection",
        "reference": "The Data Protection Engineer is responsible for ensuring the protection of data and implementing and maintaining critical services."
    },
    {
        "skill": "Administration and Operations",
        "reference": "Extensive administrative and operational experience with technologies including Commvault, TSM/Spectrum Protect, Veeam, N2WS, EVault, Zerto, and AWS DRS/FSx."
    },
    {
        "skill": "Communication Skills",
        "reference": "Communicate professionally with internal and external customers and have excellent verbal and written communication skills."
    },
    {
        "skill": "Data Engineering",
        "reference": "Responsible for data systems' development and maintenance, handling battery storage systems."
    },
    {
        "skill": "Software Development Life Cycle",
        "reference": "Participate in software dev lifecycle from ideation to support."
    },
    {
        "skill": "Python Programming",
        "reference": "Experience with Pandas, SciPy stack, request, data tools for data processing workflows."
    },
    {
        "skill": "PySpark",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADF",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADLS",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Delta tables",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "SparkSQL",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Architecture experience",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup"
    },
    {
        "skill": "Designing solutions and frameworks ground up",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup"
    },
    {
        "skill": "Experience with column-oriented database technologies",
        "reference": "Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)"
    },
    {
        "skill": "NoSQL database technologies",
        "reference": "Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)"
    },
    {
        "skill": "Traditional database systems",
        "reference": "Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)"
    },
    {
        "skill": "Handling multiple responsibilities",
        "reference": "Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks \u2018hands-on\u2019"
    },
    {
        "skill": "Leadership",
        "reference": "Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks \u2018hands-on\u2019"
    },
    {
        "skill": "Contributing to tasks",
        "reference": "Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks \u2018hands-on\u2019"
    },
    {
        "skill": "Data modeling",
        "reference": "Understanding of data modeling, warehouse design and fact/dimension concepts"
    },
    {
        "skill": "Warehouse design",
        "reference": "Understanding of data modeling, warehouse design and fact/dimension concepts"
    },
    {
        "skill": "Fact/dimension concepts",
        "reference": "Understanding of data modeling, warehouse design and fact/dimension concepts"
    },
    {
        "skill": "Data Analysis, Modeling",
        "reference": "Analyze, document and articulate business requirements for complex mathematical, business, and financial modeling logic"
    },
    {
        "skill": "Testing, Software Coding",
        "reference": "Design and execute test cases for modeling and analytical software applications to ensure they meet business needs"
    },
    {
        "skill": "Agile Methodology",
        "reference": "Collaborate with managers or practitioners in the business unit to determine systems requirements"
    },
    {
        "skill": "SQL, Oracle, Python, Java",
        "reference": "Extract data requirements through various methods including independent data analysis and mathematical functions"
    },
    {
        "skill": "Communication Skills",
        "reference": "Confer with business units and technical staff to understand data usage and perform coding reviews"
    },
    {
        "skill": "Data Modeling, Documentation",
        "reference": "Develop detailed specifications of application and document in a form that it can be used for coding"
    },
    {
        "skill": "Troubleshooting & Support",
        "reference": "Support applications in production by tracking problems and troubleshooting them"
    },
    {
        "skill": "Project Management",
        "reference": "Manage end to end business process impacts, lead cross functions, promote productive relationships between stakeholders"
    },
    {
        "skill": "Snowflake",
        "reference": "This candidate will focus on data integrations and mapping between Snowflake & ThoughtSpot."
    },
    {
        "skill": "ThoughtSpot (Data Visualization)",
        "reference": "Candidate will be responsible for evaluating existing dashboards & data, mapping/remapping new data to fit those dashboards, and creating/optimizing views in Snowflake."
    },
    {
        "skill": "Data Mapping/Modeling",
        "reference": "This role may interface with Dealers or Business"
    },
    {
        "skill": "Python (Familiarity)",
        "reference": ""
    },
    {
        "skill": "Customer/Dealer Data Experience",
        "reference": "Experience in handling dealer/customer data"
    },
    {
        "skill": "SQL, NoSQL databases, data warehousing, data modeling",
        "reference": "Expertise in SQL and NoSQL databases, data warehousing, and data modeling."
    },
    {
        "skill": "Python, SQL, additional data languages",
        "reference": "Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus."
    },
    {
        "skill": "Cloud-based data systems, multi-cloud experience",
        "reference": "Experience designing cloud-based data systems such as AWS, Azure, or GCP. Multi-cloud experience, experience with infrastructure-as-code and/or DevOps tooling a plus."
    },
    {
        "skill": "People analytics, social science research, data mining/science",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of the digital workplace."
    },
    {
        "skill": "Cross-disciplinary collaboration, psychology, data analytics, visualisation",
        "reference": "Support analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace."
    },
    {
        "skill": "Advanced data analytics, business insights",
        "reference": "Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisation."
    },
    {
        "skill": "Data Engineering, Scalable and Efficient Data Pipelines",
        "reference": "Design and build scalable and efficient data pipelines and ensure end to end orchestration in MedArrive's AWS/Databricks based data platform."
    },
    {
        "skill": "Intelligence Layer Integration, Real-time Data Supporting Systems",
        "reference": "Expand and integrate the MedArrive Intelligence Layer with application system wide event architecture and point to point API for real-time data."
    },
    {
        "skill": "Data Platform Infrastructure Enhancement, ML Ops, IaC CI/CD",
        "reference": "Contribute to design and implementation of conceptual, physical, and logical data models based on understanding of MedArrive business processes, systems, data flows, dependencies, and relationships."
    },
    {
        "skill": "Software Development",
        "reference": "Develops, tests and maintains code using software development methodology"
    },
    {
        "skill": "Project Management",
        "reference": "Provides project plan with tasks and time estimates"
    },
    {
        "skill": "Communication Skills",
        "reference": "Strong verbal and written communication skills"
    },
    {
        "skill": "people analytics, social science research, data mining/science",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace."
    },
    {
        "skill": "technology, human mission, distributed remote-first workplace",
        "reference": "We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere."
    },
    {
        "skill": "understanding digital workplace, collaboration, productivity, effectiveness",
        "reference": "Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company."
    },
    {
        "skill": "Data Science, Mathematics",
        "reference": "Background in data science, mathematics, actuarial science, or engineering"
    },
    {
        "skill": "People Analytics Experience",
        "reference": "First work experience in People Analytics"
    },
    {
        "skill": "Advanced Statistics Knowledge",
        "reference": "Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)"
    },
    {
        "skill": "Data Analytics & Visualization Proficiency",
        "reference": "Strength in data analytics and visualization (Looker Studio, Tableau, etc)"
    },
    {
        "skill": "Research Skills",
        "reference": "Ability to translate business questions to key research objectives; ability to identify the best methodology to execute research"
    },
    {
        "skill": "Communication & Writing Skills",
        "reference": "Excellent writing and communication skills"
    },
    {
        "skill": "Openness to Question Status Quo",
        "reference": "Willingness to examine the status quo and resilient in the face of challenges"
    },
    {
        "skill": "Healthcare Software Integration",
        "reference": "Knowledge and familiarity working in healthcare software integration; using HL7, FHIR, Json, XML, or other related integration protocols or interoperability toolsets."
    },
    {
        "skill": "Experience with ETL Programming",
        "reference": "3+ years of experience in extract, transform, and load (ETL) programming."
    },
    {
        "skill": "Relational Database Experience",
        "reference": "3+ years of experience working with relational databases."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "Cherre is looking for an enthusiastic Senior Data Engineer to design and implement server side services."
    },
    {
        "skill": "Real Estate Data Services",
        "reference": "You will be part of designing and implementing server side services to ingest, organize, analyze, and display real estate data and insight."
    },
    {
        "skill": "Remote-First Company, ETL Processes Development, Design Data Warehouse Solutions",
        "reference": "You will work in a small team and be a real partner in the design and implementation of all aspects of our product. We are remote-first company."
    },
    {
        "skill": "Data Engineering, Software Development, Scrum Activities",
        "reference": "The Data Engineer III is a subject matter expert within the Software Development team and will support the business intelligence team by implementing new and maintaining existing data flows and dimensional models with a focus on consistent and accurate enterprise data solutions on premise and in the cloud."
    },
    {
        "skill": "ETL/ELT Processes, Query Optimization",
        "reference": "Develops foundational patterns to be utilized by other members of the business intelligence team, Addresses issues in data flow logic and data model design, as well as the interoperability of new datasets with existing data models."
    },
    {
        "skill": "Agile Methodology, Project Management",
        "reference": "Participates in and conducts code-reviews for all changes to the codebase and conveys coding standards clearly and concisely, Tests work on each assignment before working with Product Owners to ensure business requirements are fulfilled."
    },
    {
        "skill": "ETL Frameworks, Relational Database Design",
        "reference": "Takes part in regular Scrum activities (daily standup meetings, weekly planning poker, post-sprint retrospectives, etc.)"
    },
    {
        "skill": "Data Integration Strategies, ETL/ELT Processes Optimization",
        "reference": "Performance tunes ETL/ELT processes, queries, notebooks, and other data flows, Coordinates identification of requirements and recommending new data features in conjunction with development manager, product owners, and department managers."
    },
    {
        "skill": "Project Leadership, Team Management",
        "reference": "Acts as a subject matter expert by sharing information and providing support and training to others, as well as spearheading team projects and establishing goals and milestones for projects, Ensures goals and commitments to the team are met."
    },
    {
        "skill": "Healthcare Data Knowledge",
        "reference": "Ensures goals and commitments to the team are met (preferred)."
    },
    {
        "skill": "Security, Seguridad",
        "reference": "Trabajar de manera aut\u00f3noma junto a diversos equipos, definiendo requisitos y proceso en materia de seguridadImplementaci\u00f3n de herramientas SAST, DASTDefinir e implementar pol\u00edticas transversales de seguridad para toda la compa\u00f1\u00eda."
    },
    {
        "skill": "IT Security",
        "reference": "Trabajar de manera aut\u00f3noma junto a diversos equipos, definiendo requisitos y proceso en materia de seguridadImplementaci\u00f3n de herramientas SAST, DASTDefinir e implementar pol\u00edticas transversales de seguridad para toda la compa\u00f1\u00eda."
    },
    {
        "skill": "Security Management",
        "reference": "Trabajar de manera aut\u00f3noma junto a diversos equipos, definiendo requisitos y proceso en materia de seguridadImplementaci\u00f3n de herramientas SAST, DASTDefinir e implementar pol\u00edticas transversales de seguridad para toda la compa\u00f1\u00eda."
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Data Engineer for our Data Platform Engineering team"
    },
    {
        "skill": "Scala, Spark, Kafka",
        "reference": "6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)"
    },
    {
        "skill": "Team Leadership",
        "reference": "Track record of recruiting and leading technical teams in a demanding talent market"
    },
    {
        "skill": "Snowflake",
        "reference": "Job Description: Snowflake Data Pipeline Engineer"
    },
    {
        "skill": "Data Pipeline",
        "reference": "Job Description: Snowflake Data Pipeline Engineer"
    },
    {
        "skill": "Engineering",
        "reference": "Job Description: Snowflake Data Pipeline Engineer"
    },
    {
        "skill": "Healthcare",
        "reference": "In-depth Healthcare / Health Insurance / Health Plan / Health Provider Industry experience is required for this role"
    },
    {
        "skill": "Health Insurance",
        "reference": "In-depth Healthcare / Health Insurance / Health Plan / Health Provider Industry experience is required for this role"
    },
    {
        "skill": "Health Plan",
        "reference": "In-depth Healthcare / Health Insurance / Health Plan / Health Provider Industry experience is required for this role"
    },
    {
        "skill": "Python",
        "reference": "Using Python to orchestrate data pipelines; Integrating clinical and claims data; Experience in a Cloud Data Engineering environment - using AWS Services"
    },
    {
        "skill": "PySpark",
        "reference": "Using Python to orchestrate data pipelines; Integrating clinical and claims data; Experience in a Cloud Data Engineering environment - using AWS Services"
    },
    {
        "skill": "AWS",
        "reference": "Using Python to orchestrate data pipelines; Integrating clinical and claims data; Experience in a Cloud Data Engineering environment - using AWS Services"
    },
    {
        "skill": "Data Pipelines, API Integration, Scalability",
        "reference": "Develops and maintains scalable data pipelines and builds out new API integrations"
    },
    {
        "skill": "Data Modeling, Analytics, Business Intelligence",
        "reference": "Improves data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization."
    },
    {
        "skill": "Monitoring Data Quality, Accuracy, Availability",
        "reference": "Monitors data quality to ensure production data is accurate and available for key stakeholders."
    },
    {
        "skill": "Data Analysis, Troubleshooting Issues, Resolution",
        "reference": "Performs data analysis required to troubleshoot data related issues and assists in the resolution of data issues."
    },
    {
        "skill": "Collaboration, Teamwork, Communication Skills",
        "reference": "Works closely with a team of frontend and backend engineers, product managers, and analysts. Excellent oral and written communication skills."
    },
    {
        "skill": "Data Asset Management, Spark SQL, HiveSQL Jobs",
        "reference": "Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models."
    },
    {
        "skill": "Data Integrations, Data Quality Framework, Lineage",
        "reference": "Designs data integrations and data quality framework. Designs and evaluates open source and vendor tools for data lineage."
    },
    {
        "skill": "Long Term Architecture Strategy, Business Units, Engineering Teams",
        "reference": "Works closely with all business units and engineering teams to develop strategy for long term data platform architecture."
    },
    {
        "skill": "Data Pipelines, ETL Processes",
        "reference": "Own efficacy and quality of data pipelines and ETL processes that bring data into the enterprise data warehouse."
    },
    {
        "skill": "Scalable Infrastructure Development",
        "reference": "Design and architect scalable infrastructure to build, train, and deploy machine learning models, ETL, and CI/CD with an eye on efficiency."
    },
    {
        "skill": "Cloud Technologies & Tools",
        "reference": "Be hands-on with multiple cloud technologies, tools and programming languages (Python, PySpark, SQL, AWS, GCP, Databricks, etc.)"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "AWS",
        "reference": "About the job"
    },
    {
        "skill": "Spark",
        "reference": "About the job"
    },
    {
        "skill": "Python",
        "reference": "About the job"
    },
    {
        "skill": "Expertise in AWS, Spark and Python",
        "reference": "Senior Data Engineer with expertise in AWS, Spark and Python"
    },
    {
        "skill": "Data Engineering, Druid Engineer",
        "reference": "Working knowledge of modelling, loading, optimizing large amounts of data into druid to be queried by a low latency web applications via API."
    },
    {
        "skill": "Spark, Hive, Python, Airflow",
        "reference": "Prior working experience in building data pipelines using Spark, Hive, Python, Airflow or similar technologies."
    },
    {
        "skill": "Optimizing performance, Data Modelling",
        "reference": "Working knowledge of optimizing performance of data pipelines jobs and data modelling, data processing infrastructure."
    },
    {
        "skill": "AWS Redshift, Data Engineering, SQL, Redshift Stored Procedures, AWS DMS, Airflow, Python Scripting",
        "reference": "About the job: We are seeking a talented and experienced AWS Redshift Data Engineer / Consultant to join our team in designing, developing, and optimizing data pipelines."
    },
    {
        "skill": "ETL Processes, CDC, SCD Strategies, AWS DMS, Complex ETL Scenarios",
        "reference": "Key Responsibilities: Collaborate with data engineering and development teams to design, develop, test, and maintain robust and scalable ELT/ETL pipelines using SQL scripts, Redshift stored procedures, and other AWS tools and services."
    },
    {
        "skill": "Redshift Database Optimization, Performance Tuning, Query Optimization",
        "reference": "Provide expertise in Redshift database optimization, performance tuning, and query optimization."
    },
    {
        "skill": "Data Engineering",
        "reference": "Trustworthy, actionable data plays a critical role in Pomelo's mission to improve pregnancy outcomes. As a data engineer, you will build and orchestrate pipelines to ingest and harmonize data."
    },
    {
        "skill": "Developing Data Insights",
        "reference": "Collaborate on the design and improvement of our data infrastructure; partner with product managers and data scientists to turn raw data into actionable insights."
    },
    {
        "skill": "Technical Proficiency",
        "reference": "Proficiency in Python and SQL or similar languages; have experience developing and maintaining data pipelines in a production setting; knowledge of visualization tools."
    },
    {
        "skill": "People analytics, data mining/science, social science research",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand the future of the digital workplace"
    },
    {
        "skill": "Advanced statistics, coding languages (Python, R), databases (SQL)",
        "reference": "First work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)"
    },
    {
        "skill": "Data analytics and visualization",
        "reference": "Strength in data analytics and visualization (Looker Studio, Tableau, etc)"
    },
    {
        "skill": "Research, insight creation, business objectives",
        "reference": "Ability to translate business questions to key research objectives Ability to identify the best methodology to execute research, synthesize and analyse findings"
    },
    {
        "skill": "Communication skills, status quo challenge resilience",
        "reference": "Excellent writing and communication skills Willingness to examine the status quo and resilient in the face of challenges"
    },
    {
        "skill": "Azure Big Data Services, Azure Data Lake Storage, SQL/TSQL, Agile",
        "reference": "At least five years of experience in Azure big data services/technologies (Spark, Scala, Azure Data Factory, Azure Data Bricks, Synapse Analytics). Strong knowledge of SQL/TSQL, Agile software process."
    },
    {
        "skill": "C#, Python, RDBMS, NoSQL Databases",
        "reference": "Strong coding skills (C#), experience working with python is a plus, Working experiences with RDBMS (Oracle 11g, MS SQL) including database design, developing stored procedures and functions, performance tuning. Worked on No-SQL databases."
    },
    {
        "skill": "Test-driven Development",
        "reference": "Working experiences of test-driven development framework."
    },
    {
        "skill": "Data Engineering, Advanced Subject Matter Knowledge",
        "reference": "Applies advanced subject matter knowledge to solve complex business issues and is regarded as a subject matter expert. Frequently contributes to the development of new ideas and methods."
    },
    {
        "skill": "Leadership, Cross-Functional Collaboration",
        "reference": "Acts as an expert providing direction and guidance to process improvements and establishing policies. Leads and/or provides expertise to functional project teams and may participate in cross-functional initiatives."
    },
    {
        "skill": "Complex Problem Solving, Design Innovation",
        "reference": "Works on complex problems where analysis of situations or data requires an in-depth evaluation of multiple factors. Drives innovation and integration of new technologies into projects and activities in the big data space."
    },
    {
        "skill": "Scala, Spark, SQL, Database Optimization",
        "reference": "Writing Scala code with Apache Spark + Apache Arrow + Apache Kafka for hosted cloud analytics infrastructure"
    },
    {
        "skill": "Infrastructure, Architecture, Problem Structure",
        "reference": "Scaling up from proof of concept to 'cluster scale'"
    },
    {
        "skill": "Database Engineering, Optimizers, Query Planners",
        "reference": "Developing database optimizers, query planners, and routing mechanisms"
    },
    {
        "skill": "Scala, Spark, Apache Arrow, Kafka",
        "reference": "6+ years experience engineering software and data platforms / enterprise-scale data warehouses"
    },
    {
        "skill": "Database optimization, query planning",
        "reference": "Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques"
    },
    {
        "skill": "Web3, blockchain knowledge",
        "reference": "Understand data and analytics use cases across Web3 / blockchains"
    },
    {
        "skill": "Information Security, Cyber Security, Project Management",
        "reference": "Identify business unit requirements, create project and process specifications, facilitate testing, coordinate with project teams"
    },
    {
        "skill": "Financial Management, Budget Analysis, Leadership Skills",
        "reference": "10 hands-on experience on financials / budget analysis, tracking actuals, working with leads to address the gaps"
    },
    {
        "skill": "Project Delivery, Interface Management, Problem Solving",
        "reference": "Responsible for acting as the primary interface between a specific business/functional area and their IT partners"
    },
    {
        "skill": "Strategy Creation",
        "reference": "About the job"
    },
    {
        "skill": "Pipeline Development",
        "reference": "About the job"
    },
    {
        "skill": "Performance Optimization",
        "reference": "About the job"
    },
    {
        "skill": "Data Governance",
        "reference": "About the job"
    },
    {
        "skill": "Technical Leadership",
        "reference": "About the job"
    },
    {
        "skill": "Documentation",
        "reference": "About the job"
    },
    {
        "skill": "Third-party Management",
        "reference": "Requirements"
    },
    {
        "skill": "Experience with Databricks, Azure Cloud Ecosystem, Real-time ML Model Serving Pipelines",
        "reference": "Requirements"
    },
    {
        "skill": "Data Engineering",
        "reference": "Senior Data Engineer"
    },
    {
        "skill": "Data Architecture",
        "reference": "Design and maintain scalable data pipelines using AWS, Databricks, Python, and SQL technologies."
    },
    {
        "skill": "Cross-Functional Collaboration",
        "reference": "Collaborate with various teams to grasp data requirements and implement solutions aligning with business needs."
    },
    {
        "skill": "Python",
        "reference": "Required Technical Skills: Python, PySpark, SQL, AWS"
    },
    {
        "skill": "Pyspark",
        "reference": "Required Technical Skills: Python, PySpark, SQL, AWS"
    },
    {
        "skill": "SQL",
        "reference": "Required Technical Skills: Python, PySpark, SQL, AWS"
    },
    {
        "skill": "AWS",
        "reference": "Required Technical Skills: Python, PySpark, SQL, AWS"
    },
    {
        "skill": "Data Engineering",
        "reference": "Required Technical Skills: Python, PySpark, SQL, AWS"
    },
    {
        "skill": "Healthcare/Insurance",
        "reference": "Candidate is expected to have experience in Shell scripting Extensive data analysis skills and Good communication skills"
    },
    {
        "skill": "Shell scripting",
        "reference": "Candidate is expected to have experience in Shell scripting Extensive data analysis skills and Good communication skills"
    },
    {
        "skill": "Data Analysis",
        "reference": "Candidate is expected to have experience in Shell scripting Extensive data analysis skills and Good communication skills"
    },
    {
        "skill": "7+ years of experience in Data Engineering",
        "reference": "Minimum of 7+ years of experience in Data Engineering"
    },
    {
        "skill": "Data Engineering",
        "reference": "You must be comfortable owning end-to-end data engineering pipeline as part of a small team"
    },
    {
        "skill": "Problem Solving",
        "reference": "Tackle a wide variety of technical problems and contribute daily to improve system health and code base"
    },
    {
        "skill": "Data Science",
        "reference": "You'll be joining an early-stage company with a small, tight-knit team, backed by top-tier VCs (including First Round, Floodgate, Fuel and Y Combinator) that develops machine learning algorithms."
    },
    {
        "skill": "Data Architecture",
        "reference": "Develop and maintain robust data architecture leveraging Snowflake and Qdrant"
    },
    {
        "skill": "ETL Pipelines",
        "reference": "Develop and maintain ETL (Extract, Transform, Load) pipelines to ensure data consistency and availability"
    },
    {
        "skill": "Data Warehouse Infrastructure",
        "reference": "Create Data Warehouse infrastructure for use in Data Science, Analytics, and Reporting functionalities"
    },
    {
        "skill": "Data Engineering, Experience",
        "reference": "5+ yrs of data engineering experience"
    },
    {
        "skill": "Oracle Graph Database, 1yr",
        "reference": "1+ yr of experience working with Oracle Graph Database"
    },
    {
        "skill": "6+ years of overall IT experience",
        "reference": "6+ years of overall IT experience"
    },
    {
        "skill": "3+ years of experience with high-velocity high-volume stream processing: Apache Spark",
        "reference": "3+ years of experience with high-velocity high-volume stream processing: Apache Spark"
    },
    {
        "skill": "Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka",
        "reference": "Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka"
    },
    {
        "skill": "Deep knowledge of troubleshooting and tuning Spark applications",
        "reference": "Deep knowledge of troubleshooting and tuning Spark applications"
    },
    {
        "skill": "3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV",
        "reference": "3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV"
    },
    {
        "skill": "3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, or HDFS",
        "reference": "3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, or HDFS"
    },
    {
        "skill": "3+ years of experience building, testing, and optimizing 'Big Data' data ingestion pipelines, architectures, and data sets",
        "reference": "3+ years of experience building, testing, and optimizing 'Big Data' data ingestion pipelines, architectures, and data sets"
    },
    {
        "skill": "2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark",
        "reference": "2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark"
    },
    {
        "skill": "3+ years of experience with AWS cloud platform",
        "reference": "3+ years of experience with AWS cloud platform"
    },
    {
        "skill": "3+ years of experience with database solutions like Databricks or Snowflake",
        "reference": "3+ years of experience with database solutions like Databricks or Snowflake"
    },
    {
        "skill": "2+ years of experience with NoSQL databases, including HBASE and/or Cassandra",
        "reference": "2+ years of experience with NoSQL databases, including HBASE and/or Cassandra"
    },
    {
        "skill": "Knowledge of Unix/Linux platform and shell scripting is a must",
        "reference": "Knowledge of Unix/Linux platform and shell scripting is a must"
    },
    {
        "skill": "Strong analytical and problem-solving skills",
        "reference": "Strong analytical and problem-solving skills"
    },
    {
        "skill": "PySpark",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADF",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "ADLS",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Delta tables",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "SparkSQL",
        "reference": "Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc."
    },
    {
        "skill": "Architecture experience",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up"
    },
    {
        "skill": "Design solutions and frameworks",
        "reference": "Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up"
    },
    {
        "skill": "Azure Data Factory",
        "reference": "Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines"
    },
    {
        "skill": "Azure",
        "reference": "Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines"
    },
    {
        "skill": "Cloud data platforms",
        "reference": "Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines"
    },
    {
        "skill": "Data platform implementation",
        "reference": "Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines"
    },
    {
        "skill": "End-to-end data pipelines",
        "reference": "Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines"
    },
    {
        "skill": "Data Engineering (Oracle, DB2, MySQL, SQL Server, MS Access, MS Excel)",
        "reference": "Looking for Data enginner include Oracle, DB2, MySQL, and SQL Server, as well as desktop-based applications such as MS Access and MS Excel."
    },
    {
        "skill": "Enterprise Data Model Standardization",
        "reference": "establish an enterprise data model with a primary goal of standardizing shared data entities and attributes and a secondary goal of standardizing database management systems and tools."
    },
    {
        "skill": "Azure Data Platform (Azure, Microsoft Dataverse, SQL Server)",
        "reference": "Related transactional databases and data warehouses will be hosted in Azure, generally using Microsoft Dataverse and SQL Server."
    },
    {
        "skill": "Data Analysis Project Leadership",
        "reference": "The Senior Data Analyst/Architect will provide analysis and leadership on data analysis projects across the client enterprise."
    },
    {
        "skill": "Microsoft Data Tools Expertise (Azure Data Factory, SSIS)",
        "reference": "You will also provide subject matter expertise related to Microsoft Dataverse, SQL Server, Azure Data Factory, and SSIS."
    },
    {
        "skill": "Real-world Problem Solving",
        "reference": "A proven track record of solving real-world problems using data."
    },
    {
        "skill": "Critical Thinking & Data Stewardship",
        "reference": "Excellent critical thinking skills is a must have for this role."
    },
    {
        "skill": "Database Platform Knowledge (Oracle, SQL Server, DB2, MS Access)",
        "reference": "Understanding of different database platforms: Oracle, SQL Server (On-Prem / Azure), DB2, and MS Access."
    },
    {
        "skill": "Data Warehousing & Data Mining",
        "reference": "Understanding of Data Warehousing and data mining."
    },
    {
        "skill": "Modeling Strategies (Dimensional, Snowflake, Relational, Unstructured)",
        "reference": "Understanding of modeling strategies (dimensional, snowflake, relational, unstructured)."
    },
    {
        "skill": "Technical Stewardship and Collaboration",
        "reference": "Strong interest in playing a technical data steward role across our business and technology partners to understand."
    },
    {
        "skill": "ERD/Data Modeling Tools Experience (Toad or others)",
        "reference": "Proven experience with ERD/Data Modelling tools (Toad or others)."
    },
    {
        "skill": "Agile/Hybrid Project Environment",
        "reference": "Experience in executing projects in an Agile / Hybrid environment."
    },
    {
        "skill": "Data Reporting Tools Knowledge (Power BI, others)",
        "reference": "Experience with data reporting tools (Power BI, others)."
    },
    {
        "skill": "Data warehouse modernization",
        "reference": "Data warehouse modernization: building complete data warehouse solutions"
    },
    {
        "skill": "Cloud experience",
        "reference": "4+ years of experience designing and implementing large-scale, complex, data-driven applications on the cloud, preferably on Google Cloud / AWS."
    },
    {
        "skill": "Data manipulation",
        "reference": "3+ years of experience modeling data warehouses"
    },
    {
        "skill": "Python, Postgres, SQL, Redshift",
        "reference": "Tools & Technology: Postgres, Python, SQL, Stored Procedures Jenkins AWS Redshift Git and GitHub AWS AirflowConfluence PySparkDBTAWS services such as EMR, EKS, Lambda"
    },
    {
        "skill": "Healthcare industry experience, PHI/PII",
        "reference": "Experience working in the healthcare industry with PHI/PII Federal Government contracting work experience."
    },
    {
        "skill": "Agile teamwork, time management, organizational skills",
        "reference": "Strong written and verbal communication skills Demonstrated time management skills. Strong organizational skills with attention to detail"
    },
    {
        "skill": "Experience with cloud computing platforms, preferably AWS",
        "reference": "Experience with cloud computing platforms, preferably AWS"
    },
    {
        "skill": "Proficiency in developing ETL pipelines using Data Integration tools",
        "reference": "Strong proficiency in developing, debugging ETL pipelines using Data Integration tool Pentaho or similar ETL tools such as Talend, Informatica"
    },
    {
        "skill": "Hands-on programming experience in Python or Java",
        "reference": "Experience with big data batch computing tool Spark and developing distributed data processing solutions"
    },
    {
        "skill": "Data design",
        "reference": "Data design, data architecture and data modeling (both transactional and analytic)"
    },
    {
        "skill": "Data architecture",
        "reference": "Data design, data architecture and data modeling (both transactional and analytic)"
    },
    {
        "skill": "Data modeling",
        "reference": "Data design, data architecture and data modeling (both transactional and analytic)"
    },
    {
        "skill": "Communication",
        "reference": "Demonstrate excellent communication skills, including effectively communicating with internal and external customers."
    },
    {
        "skill": "Customer relationship",
        "reference": "Demonstrate excellent communication skills, including effectively communicating with internal and external customers."
    },
    {
        "skill": "Problem-solving",
        "reference": "Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and high level of focus and attention to detail. Strong work ethic with good time management"
    },
    {
        "skill": "Attention to detail",
        "reference": "Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and high level of focus and attention to detail. Strong work ethic with good time management"
    },
    {
        "skill": "Data Engineering",
        "reference": "Client treats Data as Product"
    },
    {
        "skill": "3+ years of data software development experience",
        "reference": "Required: 3+ years of experience in data software development, programming languages and developing with big data technologies"
    },
    {
        "skill": "Cloud DevOps concepts",
        "reference": "2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework"
    },
    {
        "skill": "Azure Data Engineering",
        "reference": "Experience in Azure Cloud technologies"
    },
    {
        "skill": "Data Modeling & SQL",
        "reference": "Experience in Data Modeling and Advanced SQL techniques"
    },
    {
        "skill": "Business Intelligence",
        "reference": "Experience working with industry-leading Business Intelligence tools"
    },
    {
        "skill": "Data-processing, orchestration, monitoring",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring"
    },
    {
        "skill": "Collaborate with teams, design capabilities",
        "reference": "Responsibilities Collaborate with product and technology teams to design and validate the capabilities of the data platform"
    },
    {
        "skill": "Process improvements, automation, optimization",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability"
    },
    {
        "skill": "Technical support, usage guidance",
        "reference": "Provide technical support and usage guidance to the users of our platform's services."
    },
    {
        "skill": "Creation and refinement of metrics, monitoring, alerting mechanisms",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "Experience building data pipelines, distributed environment",
        "reference": "Experience building and optimizing data pipelines in a distributed environment"
    },
    {
        "skill": "Cross-functional team support, proficiency Linux",
        "reference": "Proficiency working in Linux environment and 8+ years of advanced working knowledge of SQL, Python, and PySpark"
    },
    {
        "skill": "Advanced AWS technologies experience",
        "reference": "5+ years of experience with using a broad range of AWS technologies"
    },
    {
        "skill": "Git/Bitbucket, Jenkins/CodeBuild, CodePipeline tools usage",
        "reference": "Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline"
    },
    {
        "skill": "Data Science",
        "reference": "Work with large, complex datasets to solve complex analysis problems and applying advanced analytical methods"
    },
    {
        "skill": "Analytics",
        "reference": "Conduct analysis that includes problem formulation, data gathering, requirements specification, processing, analysis, ongoing deliverables, and presentations"
    },
    {
        "skill": "Machine Learning",
        "reference": "Apply machine learning models during the analysis process"
    },
    {
        "skill": "Data Migration",
        "reference": "Responsibilities: Collaborate with stakeholders to understand data migration requirements, design and develop a comprehensive data migration plan, utilize Komprise for assessments, migrate data using Komprise while configuring and optimizing settings, monitor process in real-time, implement post-migration procedures, work with storage administrators, system engineers, document processes, and provide knowledge transfer."
    },
    {
        "skill": "Komprise Software",
        "reference": "Responsibilities: Collaborate with stakeholders to understand data migration requirements, design and develop a comprehensive data migration plan, utilize Komprise for assessments, migrate data using Komprise while configuring and optimizing settings, monitor process in real-time, implement post-migration procedures, work with storage administrators, system engineers, document processes, and provide knowledge transfer."
    },
    {
        "skill": "NetApp Environment",
        "reference": "Responsibilities: Collaborate with stakeholders to understand data migration requirements, design and develop a comprehensive data migration plan, utilize Komprise for assessments, migrate data using Komprise while configuring and optimizing settings, monitor process in real-time, implement post-migration procedures, work with storage administrators, system engineers, document processes, and provide knowledge transfer."
    },
    {
        "skill": "Data Migration",
        "reference": "Responsibilities: Collaborate with stakeholders to understand data migration requirements, design and develop a comprehensive data migration plan, utilize Komprise for assessments, migrate data using Komprise while configuring and optimizing settings, monitor process in real-time, implement post-migration procedures, work with storage administrators, system engineers, document processes, and provide knowledge transfer."
    },
    {
        "skill": "Komprise Software",
        "reference": "Responsibilities: Collaborate with stakeholders to understand data migration requirements, design and develop a comprehensive data migration plan, utilize Komprise for assessments, migrate data using Komprise while configuring and optimizing settings, monitor process in real-time, implement post-migration procedures, work with storage administrators, system engineers, document processes, and provide knowledge transfer."
    },
    {
        "skill": "NetApp Environment",
        "reference": "Responsibilities: Collaborate with stakeholders to understand data migration requirements, design and develop a comprehensive data migration plan, utilize Komprise for assessments, migrate data using Komprise while configuring and optimizing settings, monitor process in real-time, implement post-migration procedures, work with storage administrators, system engineers, document processes, and provide knowledge transfer."
    },
    {
        "skill": "Data Migration",
        "reference": "Requirements: Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise, Strong proficiency in Komprise data management software and related tools."
    },
    {
        "skill": "Komprise Software",
        "reference": "Requirements: Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise, Strong proficiency in Komprise data management software and related tools."
    },
    {
        "skill": "Develop and enhance data-processing",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "Identify, design, implement process improvements",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "Collaborate with teams",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform. Provide technical support and usage guidance to the users of our platform\u2019s services."
    },
    {
        "skill": "Provide technical support",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform. Provide technical support and usage guidance to the users of our platform\u2019s services."
    },
    {
        "skill": "Experience in distributed environment",
        "reference": "Qualifications Experience building and optimizing data pipelines in a distributed environment; Experience supporting and working with cross-functional teams; Proficiency working in Linux environment; 8+ years of advanced working knowledge of SQL, Python, and PySpark"
    },
    {
        "skill": "8+ years SQL, Python, PySpark",
        "reference": "Qualifications Experience building and optimizing data pipelines in a distributed environment; Experience supporting and working with cross-functional teams; Proficiency working in Linux environment; 8+ years of advanced working knowledge of SQL, Python, and PySpark"
    },
    {
        "skill": "5+ years AWS technologies",
        "reference": "Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline; Experience with platform monitoring and alerts tools"
    },
    {
        "skill": "Tools experience",
        "reference": "Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline; Experience with platform monitoring and alerts tools"
    },
    {
        "skill": "Working knowledge of SQL, Python, PySpark",
        "reference": "5+ years of experience with using a broad range of AWS technologies"
    },
    {
        "skill": "Tools experience",
        "reference": "5+ years of experience with using a broad range of AWS technologies"
    },
    {
        "skill": "Designing, building, managing data pipelines",
        "reference": "Responsible for building, managing, and optimizing complex reusable enterprise data pipelines."
    },
    {
        "skill": "Data integration technologies expertise",
        "reference": "Experience with ETL development tools, Informatica or Azure Data Factory (ADF) preferred."
    },
    {
        "skill": "NoSQL databases knowledge",
        "reference": "Experience with NoSQL databases are a plus."
    },
    {
        "skill": "Data-processing",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "Orchestration",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "Monitoring",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "Collaboration",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform"
    },
    {
        "skill": "Design Validation",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform"
    },
    {
        "skill": "Process Improvement",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability"
    },
    {
        "skill": "Automation",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability"
    },
    {
        "skill": "Usability",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability"
    },
    {
        "skill": "Support",
        "reference": "Provide technical support and usage guidance to the users of our platform's services."
    },
    {
        "skill": "Usage Guidance",
        "reference": "Provide technical support and usage guidance to the users of our platform's services."
    },
    {
        "skill": "Creation",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "Refinement Metrics",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "Experience Building Data Pipelines",
        "reference": "Experience building and optimizing data pipelines in a distributed environment"
    },
    {
        "skill": "Optimization Distributed Environment",
        "reference": "Experience building and optimizing data pipelines in a distributed environment"
    },
    {
        "skill": "Cross Functional Teams Supporting",
        "reference": "Experience supporting and working with cross-functional teams, Proficiency working in Linux environment"
    },
    {
        "skill": "Proficiency Linux",
        "reference": "Experience supporting and working with cross-functional teams, Proficiency working in Linux environment"
    },
    {
        "skill": "8+ Years SQL Python PySpark",
        "reference": "8+ years of advanced working knowledge of SQL, Python, and PySpark, 5+ years of experience with using a broad range of AWS technologies"
    },
    {
        "skill": "5+ Years AWS Experience",
        "reference": "8+ years of advanced working knowledge of SQL, Python, and PySpark, 5+ years of experience with using a broad range of AWS technologies"
    },
    {
        "skill": "Tools GitBitbucket JenkinsCodeBuild CodePipeline",
        "reference": "Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline, Experience with platform monitoring and alerts tools."
    },
    {
        "skill": "Monitoring Alerts Tools",
        "reference": "Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline, Experience with platform monitoring and alerts tools."
    },
    {
        "skill": "Data Engineering",
        "reference": "CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers."
    },
    {
        "skill": "Cloud Data Engineer",
        "reference": "Specific responsibilities for the Data Engineer \u2013 Cloud position include: Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)."
    },
    {
        "skill": "Problem Solving and Creative Solutions",
        "reference": "We love solving problems and providing creative solutions for our clients."
    },
    {
        "skill": "Tableau Data Engineer",
        "reference": "Summary Of Essential Job Functions"
    },
    {
        "skill": "Fulltime",
        "reference": "Summary Of Essential Job Functions"
    },
    {
        "skill": "Business Requirements",
        "reference": "SUMMARY OF ESSENTIAL JOB FUNCTIONS"
    },
    {
        "skill": "Data Visualization",
        "reference": "SUMMARY OF ESSENTIAL JOB FUNCTIONS"
    },
    {
        "skill": "Tableau",
        "reference": "Minimum Requirements: 5+ year experience in developing Tableau in a large enterprise"
    },
    {
        "skill": "ETL Principles",
        "reference": "Minimum Requirements: 5+ year experience in developing Tableau in a large enterprise"
    },
    {
        "skill": "Cloud Data Engineering",
        "reference": "CapTech Data Engineering consultants enable clients to build and maintain advanced data systems on AWS, Azure or GCP"
    },
    {
        "skill": "Data Pipelines Development",
        "reference": "Developing data pipelines and other data products using major cloud platforms like AWS, Azure, or GCP"
    },
    {
        "skill": "Architecture Advising",
        "reference": "Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly"
    },
    {
        "skill": "Data Infrastructure Development, Fast Processing, Machine Learning",
        "reference": "Develop fast data infrastructure leveraging data streaming, batch processing, and machine learning to personalize experiences for our customers"
    },
    {
        "skill": "Solution Delivery, Teamwork, Cross-functional Collaboration",
        "reference": "Work and deliver elegant and scalable solutions, Work with a nimble team, solve real problems and meet real customer needs"
    },
    {
        "skill": "Big Data Expertise, Technology Tools",
        "reference": "4 years of hands-on experience as a Data Engineer in a Big Data environment (Spark, Hive, HDFS, Sqoop), Strong SQL knowledge, Programming skills, Experience with modern workflow/orchestration tools and databases"
    },
    {
        "skill": "Preferred Skills & Certifications",
        "reference": "Hadoop or Spark Certification, BI Tools experience for visualizations and dashboards"
    },
    {
        "skill": "Cloud Platforms Knowledge",
        "reference": "Experience with AWS, Azure, GCP (Big Data Cloud Analytics)"
    },
    {
        "skill": "Keywords\u2014max 3 words",
        "reference": "Keywords from job description"
    },
    {
        "skill": "Data Engineering",
        "reference": "Established consulting company with expertise in cutting-edge technologies"
    },
    {
        "skill": "Advanced Analytics",
        "reference": "Work hands-on with challenging data engineering projects"
    },
    {
        "skill": "Data Management & SQL Skills",
        "reference": "Strong SQL skills in multiple database platforms"
    },
    {
        "skill": "Cloud Experience",
        "reference": "Experience with Azure, AWS, or GCP"
    },
    {
        "skill": "ETL Pipelines Development",
        "reference": "Develop and maintain ETL pipelines"
    },
    {
        "skill": "Database Design & Principles",
        "reference": "Database design and principles"
    },
    {
        "skill": "Data Modeling & Schema Development",
        "reference": "Data modeling, schema development, and data-centric documentation"
    },
    {
        "skill": "Optimal Data Models",
        "reference": "Experience improving code performance and query optimization"
    },
    {
        "skill": "Continuous Integration/Delivery Concepts",
        "reference": "Use CI/CD concepts to engineer a standardized data environment"
    },
    {
        "skill": "Data Science, Mathematics, Actuarial Science, Engineering",
        "reference": "Background in data science, mathematics, actuarial science, or engineering"
    },
    {
        "skill": "People Analytics Experience",
        "reference": "First work experience in People Analytics"
    },
    {
        "skill": "Advanced Statistics Knowledge",
        "reference": "Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)"
    },
    {
        "skill": "Coding / Scripting Languages",
        "reference": "First work experience in People Analytics"
    },
    {
        "skill": "Databases Knowledge",
        "reference": "Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)"
    },
    {
        "skill": "Data Analytics & Visualization",
        "reference": "Strength in data analytics and visualization (Looker Studio, Tableau, etc)"
    },
    {
        "skill": "Business Question Translation",
        "reference": "Ability to translate business questions to key research objectives"
    },
    {
        "skill": "Research Methodology Execution",
        "reference": "Ability to identify the best methodology to execute research, synthesize and analyse findings"
    },
    {
        "skill": "Excellent Communication Skills",
        "reference": "Excellent writing and communication skills"
    },
    {
        "skill": "Challenge Tolerance",
        "reference": "Willingness to examine the status quo and resilient in the face of challenges"
    },
    {
        "skill": "People analytics, social science research, data mining/science",
        "reference": "Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace."
    },
    {
        "skill": "Productivity, effectiveness, happiness in remote-first workplaces",
        "reference": "Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company."
    },
    {
        "skill": "Advanced data analytics, statistics, coding/scripting languages, databases",
        "reference": "Background in data science, mathematics, actuarial science, or engineering; First work experience in People Analytics"
    },
    {
        "skill": "Data Engineering, Scala/Spark, Spark-based frameworks",
        "reference": "As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure."
    },
    {
        "skill": "Distributed SQL processing, Proprietary data science platforms, Core database optimization",
        "reference": "This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance."
    },
    {
        "skill": "Scaling up, Database optimizers, Query planners",
        "reference": "Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques"
    },
    {
        "skill": "Cluster scale infrastructure, Architecture problem structure",
        "reference": "Scaling up from proof of concept to 'cluster scale' (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure."
    },
    {
        "skill": "Code reuse, Meta data capturing, Management",
        "reference": "Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management"
    },
    {
        "skill": "Team management, Executive team interaction",
        "reference": "Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)"
    },
    {
        "skill": "Web3/Blockchain industry knowledge",
        "reference": "Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective. Understand data and analytics use cases across Web3 / blockchains."
    },
    {
        "skill": "Azure Cloud Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Cosmos DB, T-SQL, Microsoft Azure Cloud PaaS",
        "reference": "Skills"
    },
    {
        "skill": "Git/Jenkins/Bitbucket, Azure Cloud workflows, Azure cloud managed DBs/Systems",
        "reference": "Experience with configuration management and building automation capabilities such as Git/Jenkins/Bitbucket; Experience with Microsoft Azure Cloud workflows; Experience in T-SQL and scripting skills"
    },
    {
        "skill": "Big Data solutions, Delta Lake by Databricks, SQL DBMSs",
        "reference": "Experience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSs"
    },
    {
        "skill": "10+ years in Data Engineering, 5+ years in Palantir Foundry, Pharmacy Benefit Management domain experience",
        "reference": "Jd10+ years in Data Engineering.Must-Have Extensive 5+ years in Palantir Foundry should be able to suggest optimization strategies for the client platform.Pharmacy Benefit Management domain experience."
    },
    {
        "skill": "Liaising with business, requirement gathering, ownership till deployment",
        "reference": "Should be able to liaise with the business for requirement gathering and take ownership till deployment"
    },
    {
        "skill": "Data Engineering, Scala/Spark, Spark-based Analytics Infrastructure",
        "reference": "As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure."
    },
    {
        "skill": "Automated Query Planner, Cluster Management Techniques, Fault-Tolerance",
        "reference": "This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database system that's highly parallel, efficient and fault-tolerant."
    },
    {
        "skill": "Scalable Infrastructure, Open Source Data Tools",
        "reference": "Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)."
    },
    {
        "skill": "Data Engineering, Scala/Spark, Spark-based Analytics Infrastructure",
        "reference": "As a Data Engineer for our Data Platform Engineering team..."
    },
    {
        "skill": "Distributed SQL Processing Frameworks, Proprietary Data Science Platforms",
        "reference": "...you will join skilled Scala/Spark engineers and core database developers..."
    },
    {
        "skill": "Core Database Optimization, Query Planning, Execution Engines",
        "reference": "This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines..."
    },
    {
        "skill": "Senior Health Data Migration Engineer",
        "reference": "Position: Senior Health Data Migration Engineer Remote"
    },
    {
        "skill": "15+ years of experience, InterSystems IRIS in a healthcare environment",
        "reference": "Minimum qualifications: 15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environment"
    },
    {
        "skill": "Bachelor's degree Computer Science/Engineering/Math/Equivalent or additional 8 years relevant experience",
        "reference": "Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirements"
    },
    {
        "skill": "Data Warehouse Modernization",
        "reference": "Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools."
    },
    {
        "skill": "Cloud Experience",
        "reference": "4+ years of experience designing and implementing large-scale, complex, data-driven applications on the cloud, preferably on Google Cloud / AWS."
    },
    {
        "skill": "Data Processing Software",
        "reference": "Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub)."
    },
    {
        "skill": "Scala, Apache Spark, Open Source stack, Data Engineering",
        "reference": "As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/Spark engineers"
    },
    {
        "skill": "Query Optimization, SQL, Database Optimizers",
        "reference": "Developing database optimizers, query planners, and query routing mechanisms"
    },
    {
        "skill": "Scalability, Architecture Design, Leadership",
        "reference": "From proof of concept to 'cluster scale', and managing a team"
    },
    {
        "skill": "Develop production-level code",
        "reference": "Develop production level code."
    },
    {
        "skill": "Own process features from design to implementation",
        "reference": "Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores."
    },
    {
        "skill": "Ensure quality of code, processes and data assets",
        "reference": "Ensure quality of code, processes, and data assets."
    },
    {
        "skill": "Great communication",
        "reference": "Must have GREAT communication skills"
    },
    {
        "skill": "Asset management",
        "reference": "Must have experience:('Asset management' or 'Wealth management')"
    },
    {
        "skill": "Wealth management",
        "reference": "Must have experience:('Asset management' or 'Wealth management')"
    },
    {
        "skill": "GCP",
        "reference": "Must have experience:('Asset management' or 'Wealth management')GCPAWSSnowflakeInformaticaETL"
    },
    {
        "skill": "AWS",
        "reference": "Must have experience:('Asset management' or 'Wealth management')GCPAWSSnowflakeInformaticaETL"
    },
    {
        "skill": "Snowflake",
        "reference": "Must have experience:('Asset management' or 'Wealth management')GCPAWSSnowflakeInformaticaETL"
    },
    {
        "skill": "Informatica",
        "reference": "Must have experience:('Asset management' or 'Wealth management')GCPAWSSnowflakeInformaticaETL"
    },
    {
        "skill": "ETL",
        "reference": "Must have experience:('Asset management' or 'Wealth management')GCPAWSSnowflakeInformaticaETL"
    },
    {
        "skill": "PySpark, Databricks, Azure, ETL, Data Pipelines",
        "reference": "Proficiency in PySpark, Databricks, and Azure for developing data pipelines. Strong experience in ETL processes and data transformation."
    },
    {
        "skill": "Data Quality, Reliability, Communication Skills",
        "reference": "Ensure data quality and reliability while collaborating with others to solve complex challenges."
    },
    {
        "skill": "Continuous Learning, Adaptability",
        "reference": "Stay updated with evolving technologies and maintain adaptable skills throughout career growth."
    },
    {
        "skill": "Data Engineering",
        "reference": "CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers."
    },
    {
        "skill": "Cloud Data Engineer",
        "reference": "Specific responsibilities for the Data Engineer \u2013 Cloud position include: Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)..."
    },
    {
        "skill": "Data Architecture Solutions",
        "reference": "Experience in the design and implementation of data architecture solutions A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines..."
    },
    {
        "skill": "Data warehouse modernization",
        "reference": "Experience with designing and implementing large-scale, complex data-driven applications on the cloud"
    },
    {
        "skill": "Cloud data processing software experience",
        "reference": "Hands-on experience using batch or streaming data processing tools like Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub"
    },
    {
        "skill": "4+ years of experience in designing and implementing large-scale complex applications",
        "reference": "4+ years of experience designing and implementing large-scale, complex data-driven applications on the cloud"
    },
    {
        "skill": "Data Architecture",
        "reference": "Lead efforts to enhance, extend and strengthen our existing data architectures"
    },
    {
        "skill": "Integration",
        "reference": "Develop and optimize data pipelines for the extraction, loading, and transformation of healthcare data"
    },
    {
        "skill": "Healthcare Data Expertise",
        "reference": "Proven expert experience in data engineering, having built complex data systems end-to-end while being accountable for the outcome. Strong domain knowledge working in the healthcare industry, expertise dealing with healthcare data is a must have."
    },
    {
        "skill": "Data Quality Assurance",
        "reference": "Implement robust data quality assurance processes to validate and ensure the accuracy of integrated healthcare data"
    },
    {
        "skill": "Cross-Functional Collaboration",
        "reference": "Collaborate with cross-functional teams, including our Lead Clinical Data Architect, product managers, analytics, and IT professionals"
    },
    {
        "skill": "Data Governance & Security",
        "reference": "Collaborate with data governance and IT teams to establish and enforce data quality and security policies and standards for full business compliance"
    },
    {
        "skill": "Execution and Delivery",
        "reference": "Act as a 'player-coach' for other data engineers and be willing to dive into the implementation and get things done as needed. Be a strong partner to engineering leadership to achieve repeatable successes in project delivery while upholding high standards on engineering excellence and team building"
    },
    {
        "skill": "Communication",
        "reference": "Communicate technical concepts and solutions to non-technical stakeholders, fostering a culture of data-driven decision-making"
    },
    {
        "skill": "AWS, Node JS, Lambda Functions, SQL, NoSQL",
        "reference": "4+ years of experience developing complex enterprise applications"
    },
    {
        "skill": "Leadership, Communication, Teamwork, Shared Ownership, Willingness to Learn",
        "reference": "Great leadership skills, Impeccable communication and team skills"
    },
    {
        "skill": "AWS Cloud Experience (Kinesis, S3, SQS), Relational & NoSQL Databases",
        "reference": "Must have Good knowledge and experience of AWS Cloud. Kinesis, S3, SQSExtensive experience working with relational and NoSql databases"
    },
    {
        "skill": "Docker Architectures, Terraform Deployments, Node.js, MuleSoft, GraphQL",
        "reference": "Experience with Docker architectures, and Terraform deployments. Expert in Node.js, Experience with MuleSoft and GraphQL"
    },
    {
        "skill": "Distributed Systems, Federated Authentication, Development Tools (Maven/Gradle)",
        "reference": "Familiarity with Docker architectures, and Terraform deployments. Experience with development of self-healing, reliable and reactive systems."
    },
    {
        "skill": "Serverless Architectures (AWS Lambda, DynamoDB), BFF Patterns",
        "reference": "Preferable Experience with Docker and Serverless architectures like AWS Lambda, DynamoDB, ECS, S3, Amazon Kinesis, EventBridge, SQS, CloudFormation, Terraform"
    },
    {
        "skill": "SQL & Procedural Language (Python, R)",
        "reference": "Experience writing SQL and a procedural language (Python, R, etc.) for data handling."
    },
    {
        "skill": "CI/CD with Automation, Integrations (Swagger, Apigee), Git",
        "reference": "Strong focus on automation including Continuous Integration / Deployment with writing unit and integration tests. Experience in Agile/SCRUM Software Development Process"
    },
    {
        "skill": "Data Analytics & Visualization Tools (Tableau, Grafana, Google Sheets)",
        "reference": "Experience implementing data analytics, visualization tools and programs using Tableau, Grafana, and Google Sheets."
    },
    {
        "skill": "Security Practices & Customer Data Management",
        "reference": "Good security practices and experience writing code that manages customer data."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "We are currently seeking an experienced and highly skilled Senior Data Engineer for an exciting opportunity."
    },
    {
        "skill": "Design and develop data pipelines, ensure efficiency and reliability throughout the lifecycle.",
        "reference": "Key Responsibilities"
    },
    {
        "skill": "Architecting and engineering data integration activities",
        "reference": "Contribute your expertise one day a week at our Fort Worth location."
    },
    {
        "skill": "Data analytics, dashboards and integrations",
        "reference": "Collaborate with Business Intelligence developers and analysts to create robust, scalable data architectures for reports, dashboards, integrations, and advanced analytics products."
    },
    {
        "skill": "Business needs-technology solutions alignment",
        "reference": "Interface with business customers to bridge the gap between business needs and technology solutions, ensuring technical solutions align with business requirements."
    },
    {
        "skill": "Expertise in SQL, data modeling, ETL/ELT development",
        "reference": "Utilize strong expertise in SQL, data modeling, and ETL/ELT development, along with familiarity with modern cloud-based data platforms, integration technologies, and business intelligence tools."
    },
    {
        "skill": "Coding proficiency in at least one programming language",
        "reference": "Demonstrate coding proficiency in at least one modern programming language (Python, Scala, Java, etc.)."
    },
    {
        "skill": "Data architectures with business requirements",
        "reference": "Align data architectures with business requirements and build data pipelines that interface well with various data sources."
    },
    {
        "skill": "Experience in Health Care environment",
        "reference": "Minimum of 5 years of experience in Information Technology, with at least 2 years in a Health Care environment preferred."
    },
    {
        "skill": "Data Engineering, Product Focused, Senior Member, Mentoring, Maintaining Data Quality, Scalability, Flexibility, On-call Rotation, Adaptability",
        "reference": "In this Data Engineering role, you will help us build out a new team of product-focused data engineers. As the most senior member on this new team, you will work closely with one of our data product verticals, setting the culture, best practices, and expectations for how your new team's data engineers will work with our different product verticals."
    },
    {
        "skill": "7+ Years Data Engineering Experience, SQL, Python, Spark, Databricks, DBT, Redshift, AWS",
        "reference": "Strong experience using SQL, Python and/or Spark for data manipulation, Experience with efficiently working with datasets at TB scale, Familiarity with Databricks, DBT, Redshift, and AWS in particular is helpful"
    },
    {
        "skill": "Passion for Shipping Production Quality Code, Test Coverage, Understanding Product Verticals, Documentation",
        "reference": "Really understand your product vertical's datasets, with an ability to document and educate your team on how certain tables / fields are meant to be used. Be the liaison between your product vertical and the core data infrastructure team to make sure business needs are met in a computationally efficient manner."
    },
    {
        "skill": "Data Science",
        "reference": "Background in data science, mathematics, actuarial science, or engineering"
    },
    {
        "skill": "Advanced Statistics",
        "reference": "Background in data science, mathematics, actuarial science, or engineering"
    },
    {
        "skill": "Coding/Scripting",
        "reference": "Background in data science, mathematics, actuarial science, or engineering"
    },
    {
        "skill": "People Analytics",
        "reference": "Support analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace."
    },
    {
        "skill": "Research",
        "reference": "Support analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace."
    },
    {
        "skill": "Tooling",
        "reference": "Support analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace."
    },
    {
        "skill": "Communication",
        "reference": "Tell the story from the insights through dashboards, visualizations and presentations"
    },
    {
        "skill": "Storytelling",
        "reference": "Tell the story from the insights through dashboards, visualizations and presentations"
    },
    {
        "skill": "Data Infrastructure Engineering",
        "reference": "Senior Data Infrastructure Engineer"
    },
    {
        "skill": "Machine Learning",
        "reference": "support additional data sources and scale infrastructure"
    },
    {
        "skill": "Clever Insights",
        "reference": "surface valuable insights for customers"
    },
    {
        "skill": "AWS",
        "reference": "Title: AWS Data Engineer"
    },
    {
        "skill": "Data Engineer",
        "reference": "Title: AWS Data Engineer"
    },
    {
        "skill": "Experience",
        "reference": "Experience: 10+ years"
    },
    {
        "skill": "10+ years",
        "reference": "Experience: 10+ years"
    },
    {
        "skill": "Visa",
        "reference": "Visa: GC USC TNSkills Required AWS AWS Glue Lambda AWS S3 Athena"
    },
    {
        "skill": "GC USC TN",
        "reference": "Visa: GC USC TNSkills Required AWS AWS Glue Lambda AWS S3 Athena"
    },
    {
        "skill": "Data Platform Engineer, Streaming Technologies",
        "reference": "We are looking for passionate and skilled Data Platform Engineers with a focus on streaming technologies to join our team."
    },
    {
        "skill": "Cloud Computing Platforms",
        "reference": "You will have experience with cloud computing platforms, such as AWS, Azure, and Google Cloud Platform."
    },
    {
        "skill": "Big Data Storage Technologies",
        "reference": "Experience with big data storage systems: Iceberg, Hudi, FoundationDB, etc."
    },
    {
        "skill": "SQL, Python, Big Data, Azure, Platform Engineering",
        "reference": "Data Engineers have several key responsibilities and 5+ years experience in building and optimizing 'big data' pipelines, architectures, and data sets."
    },
    {
        "skill": "Self-motivation, Problem Solving, Communication",
        "reference": "A self-motivating, self-directing, great communicator (written and oral) eager to teach others. Excellent problem solving skills."
    },
    {
        "skill": "Agile Methodologies, Remote Work, Microsoft Technologies",
        "reference": "Experience with Agile methodologies, working remotely, and/or working with distributed teams."
    },
    {
        "skill": "Scala/Spark, Data Platform Engineering, Database Optimization",
        "reference": "As a Data Engineer for the Data Platform Engineering team..."
    },
    {
        "skill": "Query Planning, Distributed Data Warehouse Systems, Query Planners and Execution Engines",
        "reference": "Responsibilities include..."
    },
    {
        "skill": "HTAP Database Development, Apache Spark/Arrow/Kafka",
        "reference": "Codifying best practices for future reuse in the form of accessible patterns, templates, and code bases."
    },
    {
        "skill": "ADF",
        "reference": "Key Skills: ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql"
    },
    {
        "skill": "ADLS",
        "reference": "Key Skills: ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql"
    },
    {
        "skill": "Synapse",
        "reference": "Key Skills: ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql"
    },
    {
        "skill": "Experienced Azure Cloud Lead",
        "reference": "This is a lead level role, someone who has lead a team."
    },
    {
        "skill": "Microsoft Stack",
        "reference": "This is a lead level role, someone who has lead a team."
    },
    {
        "skill": "Data Warehouse Experience",
        "reference": "This is a lead level role, someone who has lead a team."
    },
    {
        "skill": "Azure Cloud Data Engineering Expert",
        "reference": "Looking for someone who been a BI/Sql/ETL Developer etc.. and moved in to Azure cloud Data engineering."
    },
    {
        "skill": "BI/Sql/ETL Developer",
        "reference": "Looking for someone who been a BI/Sql/ETL Developer etc.. and moved in to Azure cloud Data engineering."
    },
    {
        "skill": "Azure Data Lake",
        "reference": "Should have good experience in Azure Data lake, Azure Data Factory, Azure Databricks, Azure Synapse and data warehousing concepts."
    },
    {
        "skill": "Data Factory",
        "reference": "Should have good experience in Azure Data lake, Azure Data Factory, Azure Databricks, Azure Synapse and data warehousing concepts."
    },
    {
        "skill": "Azure Databricks",
        "reference": "Should have good experience in Azure Data lake, Azure Data Factory, Azure Databricks, Azure Synapse and data warehousing concepts."
    },
    {
        "skill": "Data Warehouse Concepts Experience",
        "reference": "Should have good experience in Azure Data lake, Azure Data Factory, Azure Databricks, Azure Synapse and data warehousing concepts."
    },
    {
        "skill": "Enterprise ETL Tool",
        "reference": "Description / Enterprise Data modelling / Design Azure SQL Data Warehouse (Synapse) T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..)"
    },
    {
        "skill": "T-SQL (Hands-on)",
        "reference": "Description / Enterprise Data modelling / Design Azure SQL Data Warehouse (Synapse) T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..)"
    },
    {
        "skill": "Azure SQL Data Warehouse",
        "reference": "Description / Enterprise Data modelling / Design Azure SQL Data Warehouse (Synapse) T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..)"
    },
    {
        "skill": "Independent Problem Solver",
        "reference": "Must Have Skills / Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric.Proven experience with SQL, namely schema design and dimensional data modellingSolid knowledge of data warehouse best practices, development standards and methodologiesStrong experience with Azure Cloud on data integration with Databricks"
    },
    {
        "skill": "Fast Paced Environment",
        "reference": "Must Have Skills / Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric.Proven experience with SQL, namely schema design and dimensional data modellingSolid knowledge of data warehouse best practices, development standards and methodologiesStrong experience with Azure Cloud on data integration with Databricks"
    },
    {
        "skill": "Self-Learner",
        "reference": "Must Have Skills / Be an independent self-learner with the \"let's get this done\" approach and ability to work in Fast paced and Dynamic environment"
    },
    {
        "skill": "Get this Done Approach",
        "reference": "Must Have Skills / Be an independent self-learner with the \"let's get this done\" approach and ability to work in Fast paced and Dynamic environment"
    },
    {
        "skill": "ML Studio, AI/ML, MLOps",
        "reference": "Nice-to-Have Skills / Basic understanding on ML Studio, AI/ML, MLOps etc.Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge."
    },
    {
        "skill": "Azure Stream Analytics",
        "reference": "Nice-to-Have Skills / Basic understanding on ML Studio, AI/ML, MLOps etc.Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge."
    },
    {
        "skill": "Azure Analysis Service",
        "reference": "Nice-to-Have Skills / Basic understanding on ML Studio, AI/ML, MLOps etc.Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge."
    },
    {
        "skill": "SAP Hana",
        "reference": "Nice-to-Have Skills / Good to have SAP Hana knowledgeIntermediate knowledge on Power BIGood to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes."
    },
    {
        "skill": "Power BI",
        "reference": "Nice-to-Have Skills / Good to have SAP Hana knowledgeIntermediate knowledge on Power BIGood to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes."
    },
    {
        "skill": "DevOps and CI/CD",
        "reference": "Nice-to-Have Skills / Good to have SAP Hana knowledgeIntermediate knowledge on Power BIGood to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes."
    },
    {
        "skill": "Cloud Migration",
        "reference": "Nice-to-Have Skills / Good to have SAP Hana knowledgeIntermediate knowledge on Power BIGood to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes."
    },
    {
        "skill": "CI/CD Deployments",
        "reference": "Nice-to-Have Skills / Good to have SAP Hana knowledgeIntermediate knowledge on Power BIGood to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes."
    },
    {
        "skill": "Data visualization, analysis, manipulation",
        "reference": "As part of the Data Insights team, we are passionate about empowering the local church and leveraging data to address children's needs. The ideal candidate for this position will excel in data visualization, analysis, and manipulation."
    },
    {
        "skill": "Data storytelling, communication",
        "reference": "To build high-value products, you will interpret a broad set of data and analyses and then communicate them in a compelling, contextualized story. The ideal candidate for this position will be skilled in efficient data storytelling, communication, and cross-team collaboration."
    },
    {
        "skill": "Cross-Team Collaboration",
        "reference": "Works closely with subject matter experts to understand and align conclusions from various sources. Incorporates ministry strategy impacts to ensure products are relevant and actionable. Collaborate with partnership facilitators, national, regional or global program support team members and leaders."
    },
    {
        "skill": "SQL",
        "reference": "Responsibilities SQL Relational Databases ETL Python"
    },
    {
        "skill": "Relational Databases",
        "reference": "Responsibilities SQL Relational Databases ETL Python"
    },
    {
        "skill": "ETL",
        "reference": "Responsibilities SQL Relational Databases ETL Python"
    },
    {
        "skill": "Python",
        "reference": "Responsibilities SQL Relational Databases ETL Python"
    },
    {
        "skill": "ML/AI",
        "reference": "Nice-to-have ML/AI Geospatial Automation"
    },
    {
        "skill": "Geospatial",
        "reference": "Nice-to-have ML/AI Geospatial Automation"
    },
    {
        "skill": "Automation",
        "reference": "Nice-to-have ML/AI Geospatial Automation"
    },
    {
        "skill": "Applied Machine Learning",
        "reference": "As an Experienced Data Engineer working in Applied Machine Learning, you will be working to regularly improve our ML services, chiefly our text-to-speech service."
    },
    {
        "skill": "Text-to-Speech Technology",
        "reference": "Experience with Text-to-Speech technology and familiarity with software releases and customer impact of AI implementations is required."
    },
    {
        "skill": "Model Evaluation",
        "reference": "You should balance immediate results and technical debt with vision for ideal, automated systems. You have worked in ML pipelines and have experience developing test suites, automating testing frameworks, and feeding data analysis back into model development."
    },
    {
        "skill": "Data Engineering",
        "reference": "Role Lead Data Engineer / Senior Data (13+ Years minimum)"
    },
    {
        "skill": "Senior Data",
        "reference": "Role Lead Data Engineer / Senior Data (13+ Years minimum)"
    },
    {
        "skill": "13+ Years Experience",
        "reference": "Role Lead Data Engineer / Senior Data (13+ Years minimum)"
    },
    {
        "skill": "Develop data-processing",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "orchestration",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "monitoring",
        "reference": "Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation."
    },
    {
        "skill": "Collaborate with teams",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform"
    },
    {
        "skill": "Design capabilities",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform"
    },
    {
        "skill": "Validate data platform",
        "reference": "Collaborate with product and technology teams to design and validate the capabilities of the data platform"
    },
    {
        "skill": "Identify process improvements",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability"
    },
    {
        "skill": "Automating manual processes",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability"
    },
    {
        "skill": "Optimizing for usability",
        "reference": "Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability"
    },
    {
        "skill": "Technical support",
        "reference": "Provide technical support and usage guidance to the users of our platform's services."
    },
    {
        "skill": "Usage guidance",
        "reference": "Provide technical support and usage guidance to the users of our platform's services."
    },
    {
        "skill": "Users platform services",
        "reference": "Provide technical support and usage guidance to the users of our platform's services."
    },
    {
        "skill": "Drive metrics creation",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "Monitoring alerts mechanisms",
        "reference": "Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services."
    },
    {
        "skill": "Experience with data pipelines in distributed environment",
        "reference": ""
    },
    {
        "skill": "Proficiency working Linux environment",
        "reference": ""
    },
    {
        "skill": "7+ years advanced SQL, Python, PySpark experience",
        "reference": "Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment7+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools"
    },
    {
        "skill": "5+ years AWS technologies experience",
        "reference": "Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment7+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools"
    },
    {
        "skill": "Data Engineering",
        "reference": "Build extensible data acquisition and integration solutions to meet business requirements and reporting needs"
    },
    {
        "skill": "AWS/Cloud Infrastructure Expertise",
        "reference": "1+ years experience deploying data processing infrastructure into AWS / cloud environments"
    },
    {
        "skill": "Advanced Big Data Processing Technology Understanding",
        "reference": "Advanced understanding of at least one big data processing technology works under the hood (e.g. Spark / Hadoop / HDFS / Redshift / BigQuery / Snowflake / Parquet / Avro / Kinesis / Kafka)"
    },
    {
        "skill": "Data governance, MuleSoft API pipelines",
        "reference": "A Data Governance specialist who can build MuleSoft API pipelines within Azure DevOps to send applications/data to the cloud."
    },
    {
        "skill": "MuleSoft, Azure cloud, C# modification",
        "reference": "Also Someone who can modify and configure with C# if needed."
    },
    {
        "skill": "API Development, Data Governance tools, EDC",
        "reference": "This team member will work with our business stakeholders and other data team members to create APIs to provide data to and from Data Governance tools like EDC and Profisee."
    },
    {
        "skill": "Data Engineering",
        "reference": "Sr. Data Engineer role, key member of engineering staff"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "3+ years data software development experience, big data technologies use"
    },
    {
        "skill": "Cloud DevOps",
        "reference": "2+ years in Cloud Services and Architecture, Azure/AWS/GCP knowledge"
    },
    {
        "skill": "Open-Source Data Tools",
        "reference": "Experience with frameworks like .NET Core, Angular or Express"
    },
    {
        "skill": "Data Software Development",
        "reference": "Ideal candidate should have broad and deep technical knowledge in data"
    },
    {
        "skill": "Data Pipeline Building",
        "reference": "Building data pipelines (ETL/ELT) with batch or streaming ingestion, error handling"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Using technologies like Spark, Hadoop, and MapReduce, experience in analytics solutions"
    },
    {
        "skill": "DevOps Concepts",
        "reference": "Advanced understanding of DevOps, Azure DevOps Operational Framework"
    },
    {
        "skill": "Microservices Architecture",
        "reference": "Experience building products using micro-service oriented architecture"
    },
    {
        "skill": "REST API Development",
        "reference": "Extensible REST APIs development experience"
    },
    {
        "skill": "Open Source Frameworks",
        "reference": "Advanced understanding of open-source frameworks and usage"
    },
    {
        "skill": "Continuous Delivery & Infrastructure as Code",
        "reference": "Knowledge of continuous delivery and infrastructure as code"
    },
    {
        "skill": "Monitoring Tools",
        "reference": "Experience with Splunk or Application Insights, monitoring portals usage"
    },
    {
        "skill": "Security Protocols & Products",
        "reference": "Understanding of Active Directory, Windows Authentication, security protocols"
    },
    {
        "skill": "Azure Network & Tools Knowledge",
        "reference": "Advanced understanding of Azure Network (Subscription, Security zoning), tools like Genesis usage"
    },
    {
        "skill": "Existing Operational Portals",
        "reference": "Knowledge of existing operational portals such as Azure Portal"
    },
    {
        "skill": "CS Data Structures & Algorithms",
        "reference": "Knowledge of data structures and algorithms"
    },
    {
        "skill": "Developer Tooling",
        "reference": "Knowledge of developer tooling across software development life cycle"
    },
    {
        "skill": "Agile Environment Experience",
        "reference": "Practical knowledge in working in an Agile environment (Scrum/Kanban/SAFe)"
    },
    {
        "skill": "Problem-Solving Ability & Fast-Paced Work Environment",
        "reference": "Ability to excel in a fast-paced, startup-like environment"
    },
    {
        "skill": "Fast-paced, ever-changing landscape",
        "reference": "Ability to thrive in a fast-paced, ever-changing landscape"
    },
    {
        "skill": "5+ years of experience with SQL-focused conversion role",
        "reference": "5 + years of experience working in a SQL-focused conversion role"
    },
    {
        "skill": "Knowledge of T-SQL and SQL Server Management Studio",
        "reference": "Solid knowledge of T-SQL and SQL Server Management Studio"
    },
    {
        "skill": "Collaboration",
        "reference": "Veda is made up of talented professionals that are driven to do meaningful work to change healthcare from the inside out. We are also friends, parents, partners and caregivers."
    },
    {
        "skill": "Openness",
        "reference": "Veda is made up of talented professionals that are driven to do meaningful work to change healthcare from the inside out. We are also friends, parents, partners and caregivers."
    },
    {
        "skill": "Integrity",
        "reference": "Veda is made up of talented professionals that are driven to do meaningful work to change healthcare from the inside out. We are also friends, parents, partners and caregivers."
    },
    {
        "skill": "Grit",
        "reference": "Veda is made up of talented professionals that are driven to do meaningful work to change healthcare from the inside out. We are also friends, parents, partners and caregivers."
    },
    {
        "skill": "Ready to build the future",
        "reference": "Veda is made up of talented professionals that are driven to do meaningful work to change healthcare from the inside out. We are also friends, parents, partners and caregivers."
    },
    {
        "skill": "Self-motivated",
        "reference": "As a Data Engineer on the team, you will help move our software development practices forward by guiding others on design and best practices. You'll mentor other engineers, conduct code reviews, and help write technical requirements across product-driven teams."
    },
    {
        "skill": "Learning",
        "reference": "As a Data Engineer on the team, you will help move our software development practices forward by guiding others on design and best practices. You'll mentor other engineers, conduct code reviews, and help write technical requirements across product-driven teams."
    },
    {
        "skill": "Proficient in Python and SQL code",
        "reference": "As a Data Engineer on the team, you will help move our software development practices forward by guiding others on design and best practices. You'll mentor other engineers, conduct code reviews, and help write technical requirements across product-driven teams."
    },
    {
        "skill": "Data Pipeline Architecture",
        "reference": "As a Data Engineer on the team, you will help move our software development practices forward by guiding others on design and best practices. You'll mentor other engineers, conduct code reviews, and help write technical requirements across product-driven teams."
    },
    {
        "skill": "High-performing Teams",
        "reference": "As a Data Engineer on the team, you will help move our software development practices forward by guiding others on design and best practices. You'll mentor other engineers, conduct code reviews, and help write technical requirements across product-driven teams."
    },
    {
        "skill": "Experience in Product Development",
        "reference": "You have a background in product development, and a love for wrangling data and delivering solutions to business problems on a fast-paced team."
    },
    {
        "skill": "Data Modeling",
        "reference": "You have a background in product development, and a love for wrangling data and delivering solutions to business problems on a fast-paced team."
    },
    {
        "skill": "Database Design",
        "reference": "You have a background in product development, and a love for wrangling data and delivering solutions to business problems on a fast-paced team."
    },
    {
        "skill": "Performance Optimization",
        "reference": "You have a background in product development, and a love for wrangling data and delivering solutions to business problems on a fast-paced team."
    },
    {
        "skill": "Report Development Tools",
        "reference": "You have a background in product development, and a love for wrangling data and delivering solutions to business problems on a fast-paced team."
    },
    {
        "skill": "Data Visualization",
        "reference": "You are passionate about telling stories with data and can drive the development process from the initial prototype through the implementation of the final product."
    },
    {
        "skill": "D3.js",
        "reference": "You are passionate about telling stories with data and can drive the development process from the initial prototype through the implementation of the final product."
    },
    {
        "skill": "JavaScript",
        "reference": "You are passionate about telling stories with data and can drive the development process from the initial prototype through the implementation of the final product."
    },
    {
        "skill": "JavaScript Development",
        "reference": "Data Visualization/D3/JavaScript Engineer | Quick Summary"
    },
    {
        "skill": "5+ years experience",
        "reference": "Data Visualization/D3/JavaScript Engineer | Quick Summary"
    },
    {
        "skill": "SOAP and RESTful web services",
        "reference": "Experience implementing web services (SOAP and RESTful) and Experience connecting front-end interfaces with SQL or NoSQL"
    },
    {
        "skill": "SQL or NoSQL knowledge",
        "reference": "Experience implementing web services (SOAP and RESTful) and Experience connecting front-end interfaces with SQL or NoSQL"
    },
    {
        "skill": "Agile/Scrum development",
        "reference": "An understanding of Agile/Scrum development and a collaborative, proactive attitude"
    },
    {
        "skill": "collaborative attitude",
        "reference": "An understanding of Agile/Scrum development and a collaborative, proactive attitude"
    },
    {
        "skill": "Cloud Data Engineering",
        "reference": "Senior Cloud Data Engineer will work on algorithmic data science products, design and build solutions using relevant programming languages."
    },
    {
        "skill": "Software Development Life Cycle",
        "reference": "Experience with every phase of the software development life cycle including requirements gathering, analysis, design, development, and testing."
    },
    {
        "skill": "AWS Cloud Expertise",
        "reference": "Hands on experience with AWS cloud architecture and development data pipelines using S3, Redshift, Athena, DynamoDB, Lambda, Glue, EMR, Kinesis, API Gateway, and other AWS technologies."
    },
    {
        "skill": "Databricks",
        "reference": "Must have hands-on experience with Databricks"
    },
    {
        "skill": "Experience",
        "reference": "Must have hands-on experience with Databricks"
    },
    {
        "skill": "Responsibilities",
        "reference": "Must have hands-on experience with Databricks"
    },
    {
        "skill": "Python/Scala",
        "reference": "Must have hands-on experience with Python and/or Scala i.e. PySpark/Scala-Spark"
    },
    {
        "skill": "i.e. PySpark/Scala-Spark",
        "reference": "Must have hands-on experience with Python and/or Scala i.e. PySpark/Scala-Spark"
    },
    {
        "skill": "Big Data Ingestion Pipelines",
        "reference": "Must have experience with Databricks 5. Must have hands-on experience building, testing, and optimizing \u2018Big Data\u2019 data ingestion pipelines, architectures and data sets"
    },
    {
        "skill": "Architectures",
        "reference": "Must have experience with Databricks 5. Must have hands-on experience building, testing, and optimizing \u2018Big Data\u2019 data ingestion pipelines, architectures and data sets"
    },
    {
        "skill": "Data Sets",
        "reference": "Must have experience with Databricks 5. Must have hands-on experience building, testing, and optimizing \u2018Big Data\u2019 data ingestion pipelines, architectures and data sets"
    },
    {
        "skill": "Azure/AWS Serverless Technologies",
        "reference": "6. Experience in successfully building and deploying a new data platform on Azure/ AWS 7. Experience in Azure / AWS Serverless technologies"
    },
    {
        "skill": "Experience",
        "reference": "6. Experience in successfully building and deploying a new data platform on Azure/ AWS 7. Experience in Azure / AWS Serverless technologies"
    },
    {
        "skill": "Messaging Platforms",
        "reference": "8. Strong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series"
    },
    {
        "skill": "Knowledge",
        "reference": "8. Strong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series"
    },
    {
        "skill": "Databricks UI",
        "reference": "9. Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog"
    },
    {
        "skill": "Managing Notebooks",
        "reference": "9. Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog"
    },
    {
        "skill": "Delta Lake",
        "reference": "9. Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog"
    },
    {
        "skill": "Data Ingestion",
        "reference": "10. Experience with data ingestion of different file formats across like JSON, XML, CSV"
    },
    {
        "skill": "File Formats",
        "reference": "10. Experience with data ingestion of different file formats across like JSON, XML, CSV"
    },
    {
        "skill": "NoSQL Databases",
        "reference": "11. Experience with NoSQL databases, including HBASE and/or Cassandra"
    },
    {
        "skill": "Experience",
        "reference": "11. Experience with NoSQL databases, including HBASE and/or Cassandra"
    },
    {
        "skill": "Unix/Linux Platform",
        "reference": "12. Knowledge of Unix/Linux platform and shell scripting is a must 13. Experience with Cloud platforms e.g. AWS, GCP, etc."
    },
    {
        "skill": "Shell Scripting",
        "reference": "12. Knowledge of Unix/Linux platform and shell scripting is a must 13. Experience with Cloud platforms e.g. AWS, GCP, etc."
    },
    {
        "skill": "Experience",
        "reference": "12. Knowledge of Unix/Linux platform and shell scripting is a must 13. Experience with Cloud platforms e.g. AWS, GCP, etc."
    },
    {
        "skill": "Database Solutions",
        "reference": "Experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or Big Query"
    },
    {
        "skill": "Experience",
        "reference": "Experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or Big Query"
    },
    {
        "skill": "Bachelor's Degree",
        "reference": "Qualifications: Bachelor\u2019s Degree required. Preferably in Information Systems, Computer Science, Computer Information Systems or related field"
    },
    {
        "skill": "Preferably",
        "reference": "Qualifications: Bachelor\u2019s Degree required. Preferably in Information Systems, Computer Science, Computer Information Systems or related field"
    },
    {
        "skill": "Information Systems",
        "reference": "Qualifications: Bachelor\u2019s Degree required. Preferably in Information Systems, Computer Science, Computer Information Systems or related field"
    },
    {
        "skill": "Data Migration",
        "reference": "Design and develop a data migration plan, use Komprise software for assessments, configure settings, monitor process, and provide training."
    },
    {
        "skill": "Komprise Expertise",
        "reference": "Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise."
    },
    {
        "skill": "NetApp and Isilon Knowledge",
        "reference": "In-depth knowledge of Isilon, NetApp storage platforms, and file systems, protocols, and administration."
    },
    {
        "skill": "Innovative features",
        "reference": "As a Lead Software Engineer, you will be directly responsible for many of the innovative features we\u2019ll be working on"
    },
    {
        "skill": "Scalable software",
        "reference": "Build highly scalable software to ensure data quality and reliability of our microservices architecture"
    },
    {
        "skill": "Technical leadership",
        "reference": "As a hands-on lead, you will work closely with our team of software engineers and product managers to deliver rapid value"
    },
    {
        "skill": "Java, Kotlin, Azure, Cloud Native Technologies, Healthcare Data, ETL Pipelines",
        "reference": "3+ years of experience programming with Java and/or Kotlin; 3+ years of experience working with Azure cloud native technologies; 2+ years of experience building a pipeline for Public health/health care data (HL7, FHIR, vocabulary, and HHS data standards); 2+ years experience with developing ETL pipelines in Azure using tools such as Azure Data factory, Event hubs, Event grid, Azure functions; 2+ years developing data engineering pipeline with Databricks or Spark"
    },
    {
        "skill": "Agile Teams, CI/CD, SDLC Cycle, CosmoDB, AzureSQL, Redis, Synapse",
        "reference": "Familiar with devOps process, CI/CD, security, coding standard and SDLC cycle; 1+ year working with technologies such as CosmoDB, AzureSQL, Redis, Synapse"
    },
    {
        "skill": "CosmoDB, AzureSQL, Redis, Synapse",
        "reference": "1+ year working with technologies such as CosmoDB, AzureSQL, Redis, Synapse"
    },
    {
        "skill": "Azure Data Factory, Snowflake, Databricks, SQL, Python",
        "reference": "3+ years of data engineering experience leveraging technologies such as Snowflake, Azure Data Factory, ADLS Gen 2, Logic Apps, Azure Functions, Databricks, Apache Spark, Scala, Synapse, SQL Server"
    },
    {
        "skill": "Data Lake, Microsoft Azure Data Lake Storage",
        "reference": "Experience structuring Data Lake for the reliability, security and performance"
    },
    {
        "skill": "ETL, ETL framework, Troubleshooting, SQL optimization",
        "reference": "5 years writing SQL, TSQL queries against any RDBMS with query optimization and performance tuning Experience implementing ETL for Data Warehouse and Business intelligence solutions Working experience with Python, and Power Shell Scripting Skills"
    },
    {
        "skill": "Agile teamwork, Software development skills",
        "reference": "Experience working within an agile team, In-depth knowledge of agile process and principles"
    },
    {
        "skill": "Data analytics and data science support",
        "reference": "Create data tools for data analytics and data science team members to deliver actionable insights"
    },
    {
        "skill": "Process improvement, data delivery optimization, infrastructure design",
        "reference": "Orchestrate large, complex data sets that meet functional/non-functional business requirements. Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability."
    },
    {
        "skill": "Data systems functionality",
        "reference": "Partner with data and analytics talent to strive for greater functionality in our data systems."
    },
    {
        "skill": "data-oriented",
        "reference": "Do you want to help build the next generation network security planning solution?"
    },
    {
        "skill": "senior software engineer",
        "reference": "Do you want to help build the next generation network security planning solution?"
    },
    {
        "skill": "network security",
        "reference": "Do you want to help build the next generation network security planning solution?"
    },
    {
        "skill": "ETL",
        "reference": "Candidates should have deep hands-on experience working on data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking."
    },
    {
        "skill": "database technologies",
        "reference": "Candidates should have deep hands-on experience working on data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking."
    },
    {
        "skill": "TCP/IP networking",
        "reference": "Candidates should have deep hands-on experience working on data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking."
    },
    {
        "skill": "backend development",
        "reference": "The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development."
    },
    {
        "skill": "business logic",
        "reference": "The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development."
    },
    {
        "skill": "polyglot",
        "reference": "The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development."
    },
    {
        "skill": "Cloud Data Engineering",
        "reference": "CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources"
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly"
    },
    {
        "skill": "Data Architecture Solutions Design",
        "reference": "Utilizing your skills in engineering best practices to solve complex data problems Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization."
    },
    {
        "skill": "Big Data Engineer",
        "reference": "For the 12.Big Data Engineer: Specialized expertise in big data technologies and tools."
    },
    {
        "skill": "Distributed Systems Proficiency",
        "reference": "Proficiency in working with distributed systems."
    },
    {
        "skill": "Data Ingestion & Processing Expertise",
        "reference": "Experience with data ingestion and processing at scale."
    },
    {
        "skill": "GenAI (Generative Artificial Intelligence)",
        "reference": "GenAI (Generative Artificial Intelligence) Roles:"
    },
    {
        "skill": "ETL/ELT development, implementation, support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data integration concepts",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "SQL, Python programming",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "MS SQL server",
        "reference": "About the job"
    },
    {
        "skill": "SSIS",
        "reference": "About the job"
    },
    {
        "skill": "SSRS",
        "reference": "About the job"
    },
    {
        "skill": "Strong SQL queries",
        "reference": "Your Primary Responsibilities Will Include MS SQL Server, SSIS, SSRS, strong SQL Queries, DB Design, Performance Tuning, Stored Procedures, Functions and Views"
    },
    {
        "skill": "DB design",
        "reference": "Your Primary Responsibilities Will Include MS SQL Server, SSIS, SSRS, strong SQL Queries, DB Design, Performance Tuning, Stored Procedures, Functions and Views"
    },
    {
        "skill": "Performance tuning",
        "reference": "Your Primary Responsibilities Will Include MS SQL Server, SSIS, SSRS, strong SQL Queries, DB Design, Performance Tuning, Stored Procedures, Functions and Views"
    },
    {
        "skill": "Stored procedures",
        "reference": "Advanced knowledge of stored procedures, functions and views for data manipulation and extraction"
    },
    {
        "skill": "Functions and views",
        "reference": "Advanced knowledge of stored procedures, functions and views for data manipulation and extraction"
    },
    {
        "skill": "GCP Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "10+ years experience",
        "reference": "10+ years of experience in designing and implementing large scale data processing/data storage/data distribution systems"
    },
    {
        "skill": "Multi-technology teamwork",
        "reference": "Extensive experience working with large data sets with hands-on technology skills to design and build robust Big Data solutions using Spark framework, GCP Big data services and industry standard frameworks"
    },
    {
        "skill": "Project management",
        "reference": "Must be willing and able to work in a global project environment across multiple time zones. Project is very fast paced with tight deadlines."
    },
    {
        "skill": "ETL",
        "reference": "Notes: ETL experience with SSIS, SQL, SSRS, Tableau experience, Healthcare experience"
    },
    {
        "skill": "SQL",
        "reference": "Notes: ETL experience with SSIS, SQL, SSRS, Tableau experience, Healthcare experience"
    },
    {
        "skill": "SSRS",
        "reference": "Notes: ETL experience with SSIS, SQL, SSRS, Tableau experience, Healthcare experience"
    },
    {
        "skill": "Tableau experience",
        "reference": "Notes: ETL experience with SSIS, SQL, SSRS, Tableau experience, Healthcare experience"
    },
    {
        "skill": "Healthcare experience",
        "reference": "Notes: ETL experience with SSIS, SQL, SSRS, Tableau experience, Healthcare experience"
    },
    {
        "skill": "Consult on complex data engineering efforts",
        "reference": "Be able to consult on complex data engineering efforts and lead project teams through the solution design process, Be able to teach and mentor to less-experienced technical team members. Ability to compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding"
    },
    {
        "skill": "Lead project teams",
        "reference": "Be able to consult on complex data engineering efforts and lead project teams through the solution design process, Be able to teach and mentor to less-experienced technical team members. Ability to compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding"
    },
    {
        "skill": "Teach and mentor",
        "reference": "Be able to consult on complex data engineering efforts and lead project teams through the solution design process, Be able to teach and mentor to less-experienced technical team members. Ability to compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding"
    },
    {
        "skill": "Compliance",
        "reference": "Be able to consult on complex data engineering efforts and lead project teams through the solution design process, Be able to teach and mentor to less-experienced technical team members. Ability to compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding"
    },
    {
        "skill": "Communication",
        "reference": "Be able to consult on complex data engineering efforts and lead project teams through the solution design process, Be able to teach and mentor to less-experienced technical team members. Ability to compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding"
    },
    {
        "skill": "Work management",
        "reference": "Be able to consult on complex data engineering efforts and lead project teams through the solution design process, Be able to teach and mentor to less-experienced technical team members. Ability to compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding"
    },
    {
        "skill": "Data architecture development",
        "reference": "Manage the innovation development processes and be responsible for driving the data architecture for the company's products and IT processes. Ability to research, evaluate and formally recommend third party software and technology package Keep big picture concepts in mind when designing solutions; fully understand business needs Strong experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance"
    },
    {
        "skill": "Innovation",
        "reference": "Manage the innovation development processes and be responsible for driving the data architecture for the company's products and IT processes. Ability to research, evaluate and formally recommend third party software and technology package Keep big picture concepts in mind when designing solutions; fully understand business needs Strong experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance"
    },
    {
        "skill": "Third party software evaluation",
        "reference": "Manage the innovation development processes and be responsible for driving the data architecture for the company's products and IT processes. Ability to research, evaluate and formally recommend third party software and technology package Keep big picture concepts in mind when designing solutions; fully understand business needs Strong experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance"
    },
    {
        "skill": "Big picture thinking",
        "reference": "Manage the innovation development processes and be responsible for driving the data architecture for the company's products and IT processes. Ability to research, evaluate and formally recommend third party software and technology package Keep big picture concepts in mind when designing solutions; fully understand business needs Strong experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance"
    },
    {
        "skill": "Database technologies",
        "reference": "Manage the innovation development processes and be responsible for driving the data architecture for the company's products and IT processes. Ability to research, evaluate and formally recommend third party software and technology package Keep big picture concepts in mind when designing solutions; fully understand business needs Strong experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance"
    },
    {
        "skill": "Data warehouse",
        "reference": "Manage the innovation development processes and be responsible for driving the data architecture for the company's products and IT processes. Ability to research, evaluate and formally recommend third party software and technology package Keep big picture concepts in mind when designing solutions; fully understand business needs Strong experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance"
    },
    {
        "skill": "Technical oversight",
        "reference": "Providing proactive technical oversight and advice to application architecture and development teams fostering re-use, design for scale, stability, and operational efficiency of data/analytical solutions Knowledge of and experienced in rolling out best practices in all facets of DW architecture, data flow strategy, data modeling, metadata and master data management"
    },
    {
        "skill": "Application architecture",
        "reference": "Providing proactive technical oversight and advice to application architecture and development teams fostering re-use, design for scale, stability, and operational efficiency of data/analytical solutions Knowledge of and experienced in rolling out best practices in all facets of DW architecture, data flow strategy, data modeling, metadata and master data management"
    },
    {
        "skill": "Data/analytical solution development",
        "reference": "Providing proactive technical oversight and advice to application architecture and development teams fostering re-use, design for scale, stability, and operational efficiency of data/analytical solutions Knowledge of and experienced in rolling out best practices in all facets of DW architecture, data flow strategy, data modeling, metadata and master data management"
    },
    {
        "skill": "Big Data Developer",
        "reference": "Job Requirements"
    },
    {
        "skill": "Bachelor's degree in Computer Engineering or Computer Science",
        "reference": "Job Requirements"
    },
    {
        "skill": "7+ years post-bachelor's experience",
        "reference": "Job Requirements"
    },
    {
        "skill": "5+ years Big Data Software Engineering experience",
        "reference": "Technical Requirements"
    },
    {
        "skill": "Spark, Hadoop and similar frameworks",
        "reference": "Technical Requirements"
    },
    {
        "skill": "4+ years Apache Spark/Hadoop Framework",
        "reference": "Technical Requirements"
    },
    {
        "skill": "3+ years Java, PL/SQL, SQL",
        "reference": "Job Requirements"
    },
    {
        "skill": "2+ years Azure Cloud (HDInsight, Databricks)",
        "reference": "Job Requirements"
    },
    {
        "skill": "2+ years Agile Development",
        "reference": "Job Requirements"
    },
    {
        "skill": "Scala and Python programming",
        "reference": "Technical Requirements"
    },
    {
        "skill": "Apache Kafka and HBase experience",
        "reference": "Technical Requirements"
    },
    {
        "skill": "Strong technical skills",
        "reference": "Job Requirements"
    },
    {
        "skill": "Communicate complex technology solutions",
        "reference": "Job Requirements"
    },
    {
        "skill": "Analytical and critical thinking",
        "reference": "Job Requirements"
    },
    {
        "skill": "Manage multiple complex problems",
        "reference": "Job Requirements"
    },
    {
        "skill": "Familiarity with code versioning tools like Git, CICD tools like Jenkins and XL Deploy",
        "reference": "Familiarity with code versioning tools like Git, CICD tools like Jenkins and XL Deploy"
    },
    {
        "skill": "Data Analytics",
        "reference": "Senior Data Analyst, contract-to-hire position"
    },
    {
        "skill": "Data Management",
        "reference": "2+ years of direct experience in managing and curating data catalogs"
    },
    {
        "skill": "Cloud Platforms",
        "reference": "Experience with Cloud Platforms including Azure Data Lake Storage (ADLS), Unity Catalog, Databricks, and Azure Synapse"
    },
    {
        "skill": "Data Governance",
        "reference": "2+ years of experience working closely with data stewardship principles and practices, demonstrating a strong understanding of data governance and stewardship concepts"
    },
    {
        "skill": "Python & SQL",
        "reference": "Expertise with Python and SQL for querying and managing databases for data storage and retrieval"
    },
    {
        "skill": "Data Catalogs",
        "reference": "Strong experience in network administration, familiarity with Alation or other leading data catalog platforms"
    },
    {
        "skill": "Network Administration",
        "reference": "Strong experience in network administration, with a solid foundation in managing data access and security in a networked environment"
    },
    {
        "skill": "E-Commerce Experience",
        "reference": "Experience with an E-commerce or multi-channel retail environment"
    },
    {
        "skill": "Communication Skills",
        "reference": "Excellent verbal and written communication skills"
    },
    {
        "skill": "Passion for Metadata & Databases",
        "reference": "Be passionate about Metadata and Databases!"
    },
    {
        "skill": "Talend, SQL, AWS, Healthcare, Big Data",
        "reference": "Position Purpose Develops and operationalizes data pipelines to make data available for consumption."
    },
    {
        "skill": "Data Management Procedures, BI Analytics, Real-time Processing Applications",
        "reference": "Designs and implements standardized data management procedures around data staging, ingestion, preparation, provisioning, destruction, designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analytics."
    },
    {
        "skill": "Data Quality Improvement, Automation",
        "reference": "Ensure quality of technical solutions as data moves across Centene\u2019s environments; Provides insight into the changing data environment; Offers suggestions for solutions; Develops and constructs, tests, maintains architectures; Identifies ways to improve data reliability, efficiency, and quality; Use data to discover tasks that can be automated."
    },
    {
        "skill": "Data Security, Communication, Organization",
        "reference": "Strong Communication and Organizational skills"
    },
    {
        "skill": "Programming Language and Tools",
        "reference": "Develops, constructs, tests, and maintains architectures using programming language and tools"
    },
    {
        "skill": "InterSystems IRIS",
        "reference": "Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS"
    },
    {
        "skill": "Healthcare",
        "reference": "Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS"
    },
    {
        "skill": "15+ Years of Professional Work Experience",
        "reference": "15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environment"
    },
    {
        "skill": "Data Migration",
        "reference": "15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environment"
    },
    {
        "skill": "Bachelor's Degree",
        "reference": "Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirements"
    },
    {
        "skill": "Computer Science or Equivalent",
        "reference": "Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirements"
    },
    {
        "skill": "Data Engineering",
        "reference": "Creating products using data from both internal and external vendor sources, optimizing data delivery for greater scalability."
    },
    {
        "skill": "Cloud Experience",
        "reference": "Experience working in a cloud environment such as Azure, AWS or other private or public clouds."
    },
    {
        "skill": "Agile Development",
        "reference": "Working in an Agile environment and end-to-end automation skills."
    },
    {
        "skill": "Data Pipeline Expertise",
        "reference": "About the job"
    },
    {
        "skill": "Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Database Administration and Development Experience",
        "reference": "Candidates must hold an active TS/SCI clearance and live in MD, DC or VA"
    },
    {
        "skill": "Active TS/SCI Clearance",
        "reference": "Candidates must hold an active TS/SCI clearance and live in MD, DC or VA"
    },
    {
        "skill": "Data Engineering",
        "reference": "Duties - Develop and design data pipelines to support an end-to-end solution, Develop and maintain artifacts related to ETL processes, Integrate data pipelines with AWS cloud services, Extract meaningful insights"
    },
    {
        "skill": "AWS Cloud Services",
        "reference": "Duties - Develop and design data pipelines to support an end-to-end solution, Develop and maintain artifacts related to ETL processes, Integrate data pipelines with AWS cloud services, Extract meaningful insights"
    },
    {
        "skill": "SQL",
        "reference": "Top Skills SQL, Azure Data Factory, DBT ETL work"
    },
    {
        "skill": "Azure Data Factory",
        "reference": "Top Skills SQL, Azure Data Factory, DBT ETL work"
    },
    {
        "skill": "DBT ETL work",
        "reference": "Top Skills SQL, Azure Data Factory, DBT ETL work"
    },
    {
        "skill": "Proven project completion",
        "reference": "Proven ability to complete projects in a timely manner while clearly measuring progress"
    },
    {
        "skill": "Timely progress measurement",
        "reference": "Proven ability to complete projects in a timely manner while clearly measuring progress"
    },
    {
        "skill": "Software engineering fundamentals",
        "reference": "Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs."
    },
    {
        "skill": "Popular programming languages (.NET, Java)",
        "reference": "Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs."
    },
    {
        "skill": "SQL constructs",
        "reference": "Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs."
    },
    {
        "skill": "Frontend client applications",
        "reference": "Experience writing and maintaining frontend client applications, Angular preferred"
    },
    {
        "skill": "Angular",
        "reference": "Experience writing and maintaining frontend client applications, Angular preferred"
    },
    {
        "skill": "Git revision control",
        "reference": "Experience with cloud-based systems (Azure / AWS / GCP). Experience with revision control (Git)"
    },
    {
        "skill": "Cloud-based systems (Azure/AWS/GCP)",
        "reference": "Experience with cloud-based systems (Azure / AWS / GCP). Experience with revision control (Git)"
    },
    {
        "skill": "Big data design",
        "reference": "High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns"
    },
    {
        "skill": "Data normalization patterns",
        "reference": "High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns"
    },
    {
        "skill": "Queuing technologies",
        "reference": "Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc) Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools"
    },
    {
        "skill": "Metrics Logging, Monitoring, Alerting tools",
        "reference": "Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc) Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools"
    },
    {
        "skill": "Strong communication",
        "reference": "Strong communication skills Strong experience with use of RESTful APIs"
    },
    {
        "skill": "RESTful API use",
        "reference": "Strong communication skills Strong experience with use of RESTful APIs"
    },
    {
        "skill": "Hl7 V2.x/Fhir-based interface messages understanding",
        "reference": "High level understanding of HL7 V2.x / FHIR based interface messages. High level understanding of system deployment tasks and technologies."
    },
    {
        "skill": "System deployment tasks and technologies (CI/CD Pipeline, K8s, Terraform)",
        "reference": "High level understanding of HL7 V2.x / FHIR based interface messages. High level understanding of system deployment tasks and technologies."
    },
    {
        "skill": "Business logic analysis",
        "reference": "Communicate with business leaders to help translate requirements into functional specification Develop broad understanding of business logic and functionality of current systems Analyze and manipulate data by writing and running SQL queries"
    },
    {
        "skill": "Data manipulation (SQL queries)",
        "reference": "Communicate with business leaders to help translate requirements into functional specification Develop broad understanding of business logic and functionality of current systems Analyze and manipulate data by writing and running SQL queries"
    },
    {
        "skill": "Data consumption from various sources",
        "reference": "Consume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health Records"
    },
    {
        "skill": "Interface with Electronic Health Records",
        "reference": "Consume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health Records"
    },
    {
        "skill": "Engineering scalable, reliable and performant systems",
        "reference": "Engineer scalable, reliable, and performant systems to manage data Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization"
    },
    {
        "skill": "Collaboration across teams (Engineers, QA, Scrum master, Product Manager)",
        "reference": "Engineer scalable, reliable, and performant systems to manage data Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization"
    },
    {
        "skill": "Build quality systems",
        "reference": "Build quality systems while expanding offerings to dependent teams Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Expanding offerings to dependent teams",
        "reference": "Build quality systems while expanding offerings to dependent teams Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Big Data Engineering",
        "reference": "The successful candidate must have Big Data engineering experience"
    },
    {
        "skill": "Data Ingestion Experience",
        "reference": "3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV"
    },
    {
        "skill": "Experience in Building Data Platforms",
        "reference": "2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark Experience in Azure / AWS Serverless technologies, like, S3, Kinesis/MSK, lambda, and Glue"
    },
    {
        "skill": "Strong Communication Skills",
        "reference": "The candidate must be a very good communicator, both written and verbal"
    },
    {
        "skill": "Data Engineering Life Cycle Participation",
        "reference": "The candidate will participate in all phases of the Data Engineering life cycle and will independently and collaboratively write project requirements, architect solutions and perform data ingestion development and support duties"
    },
    {
        "skill": "Sr. Data Engineer, Big Data and Cloud Experience",
        "reference": "Role: Sr. Data Engineer with Big Data and Cloud Exp."
    },
    {
        "skill": "Data-driven company, delivering high quality technology products",
        "reference": "Our team thrives and succeeds in supporting Data Driven company"
    },
    {
        "skill": "3+ years experience in data software dev., programming languages, big data technologies",
        "reference": "Required: 3+ years of experience in data software development, programming languages and developing with big data technologies"
    },
    {
        "skill": "Data Engineering, Big Data + Azure",
        "reference": "Role: Sr. Data Engineer (Big Data + Azure Exp)"
    },
    {
        "skill": "3+ years data software dev experience, Cloud DevOps, Open-source tools",
        "reference": "Required: 3+ years of experience in data software development, programming languages and developing with big data technologies"
    },
    {
        "skill": "Advanced Data Analytics, Python, Scala, Spark, Hadoop",
        "reference": "Experience with analytics solutions. Experience in development using Python or PySpark, Spark, Scala."
    },
    {
        "skill": "Data Engineering, Healthcare Experience",
        "reference": "About The Role: We are looking for a mission-driven Senior Data Engineer to join our early team at a critical and exciting time for the company. In this position, you will play a pivotal role in building out Hopscotch Health\u2019s data and analytics platform and data team."
    },
    {
        "skill": "Results-Driven, Problem Solving",
        "reference": "From a cultural perspective, you are: Thoughtful and can work effectively in a fast-paced, ever-changing environment. Constantly seeking ways to simplify and improve how things are done."
    },
    {
        "skill": "Data Modeling, Analytical Insights",
        "reference": "ABOUT YOU: You would be a great fit for this position if you have 4+ years of experience in data engineering, preferably in healthcare services, and you are: Knowledgeable about healthcare, preferably in the services/provider sector. Creative in finding solutions to arrive at mutually beneficial outcomes."
    },
    {
        "skill": "Scala/Spark engineering, database optimization",
        "reference": "Join skilled Scala/Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure"
    },
    {
        "skill": "Query planning, optimizing, distributed data warehouse systems",
        "reference": "Developing novel query optimization techniques for industry-leading performance, building a databasesystem that's highly parallel, efficient and fault-tolerant"
    },
    {
        "skill": "Web3, blockchain, data analytics understanding",
        "reference": "Experience with rapid development cycles in a web-based environment; Strong scripting and test automation knowledge; Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this"
    },
    {
        "skill": "AWS Certified Big Data - Specialty",
        "reference": "AWS Certified Big Data - Specialty"
    },
    {
        "skill": "Data Engineer Experience",
        "reference": "Proven experience working as a Data Engineer"
    },
    {
        "skill": "Programming Languages Proficiency",
        "reference": "Strong proficiency in programming languages like Python, Java, or Scala"
    },
    {
        "skill": "Azure Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Data Architecture Design",
        "reference": "Design, develop, and maintain data pipelines using Azure Data Factory and other relevant Azure technologies"
    },
    {
        "skill": "Cross-Functional Collaboration",
        "reference": "Collaborate with cross-functional teams to understand data requirements and design scalable ETL processes using Azure services"
    },
    {
        "skill": "Data flow patterns",
        "reference": "Title: Senior Data Engineer, Jd Experience establishing data flow patterns that cover ADLS 2 -> Bronze"
    },
    {
        "skill": "ADLS 2",
        "reference": "Title: Senior Data Engineer, Jd Experience establishing data flow patterns that cover ADLS 2 -> Bronze"
    },
    {
        "skill": "Autoloader",
        "reference": "Title: Senior Data Engineer, Jd Experience establishing data flow patterns that cover ADLS 2 -> Bronze"
    },
    {
        "skill": "Differentiating datasets",
        "reference": "Jd Experience laying the foundation for generic data flows to support our goals around re-use and developer experience."
    },
    {
        "skill": "Dimensional models",
        "reference": "Jd Experience laying the foundation for generic data flows to support our goals around re-use and developer experience."
    },
    {
        "skill": "Integration of tools",
        "reference": "Experience integrating and controlling Databricks, Azure Infrastructure, and other BI Pipeline artifacts with source control under a CI/CD paradigm."
    },
    {
        "skill": "CI/CD",
        "reference": "Experience integrating and controlling Databricks, Azure Infrastructure, and other BI Pipeline artifacts with source control under a CI/CD paradigm."
    },
    {
        "skill": "Data Engineering, Architecture",
        "reference": "Lead Data Engineer is responsible for the design, architecture and support of systems"
    },
    {
        "skill": "AWS Cloud, Big Data, Real-Time Streaming Analytics",
        "reference": "Architect, build, and support the operation of AWS Cloud data infrastructure"
    },
    {
        "skill": "Data APIs, Services, Automation",
        "reference": "Design robust, reusable and scalable solutions for ingestion, processing and delivery of batch and streaming data"
    },
    {
        "skill": "DevOps, CI/CD, Data Analytics",
        "reference": "Execution of DevOps methodologies and continuous integration/continuous delivery"
    },
    {
        "skill": "SQL, Data Profiling, Extraction",
        "reference": "Expertise in SQL for data profiling, analysis, and extraction"
    },
    {
        "skill": "GCP Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Remote",
        "reference": "About the job"
    },
    {
        "skill": "8 months",
        "reference": "About the job"
    },
    {
        "skill": "Solid Communication Skills",
        "reference": "No H1B or TN visa / GCP Data Engineer Location :Remote (need to be in EST / CST only) / Duration : 8 months"
    },
    {
        "skill": "Highly Motivated Designers and Engineers",
        "reference": "No H1B or TN visa / GCP Data Engineer Location :Remote (need to be in EST / CST only) / Duration : 8 months"
    },
    {
        "skill": "Cloud Journey",
        "reference": "No H1B or TN visa / GCP Data Engineer Location :Remote (need to be in EST / CST only) / Duration : 8 months"
    },
    {
        "skill": "Self-Motivation",
        "reference": "No H1B or TN visa / GCP Data Engineer Location :Remote (need to be in EST / CST only) / Duration : 8 months"
    },
    {
        "skill": "Collaboration",
        "reference": "What You Do"
    },
    {
        "skill": "Analytics and Business Teams",
        "reference": "What You Do"
    },
    {
        "skill": "Data Models Improvement",
        "reference": "What You Do"
    },
    {
        "skill": "Experience with Data Processing Systems",
        "reference": "Design, implement and deploy new data models and data processes / Experience in data processing using traditional and distributed systems such as Hadoop, Spark, Dataproc, Airflow, etc."
    },
    {
        "skill": "Designing Data Models and Warehouses",
        "reference": "Design, implement and deploy new data models and data processes / Experience in data processing using traditional and distributed systems such as Hadoop, Spark, Dataproc, Airflow, etc."
    },
    {
        "skill": "Data Pipelines Scalability",
        "reference": "Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity"
    },
    {
        "skill": "API Integration",
        "reference": "Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity"
    },
    {
        "skill": "Data Quality Management",
        "reference": "Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it."
    },
    {
        "skill": "Accurate and Available Data",
        "reference": "Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it."
    },
    {
        "skill": "Unit/Integration Testing",
        "reference": "Writes unit/integration tests, contributes to engineering wiki, and documents work / Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues."
    },
    {
        "skill": "Engineering Wiki Documentation",
        "reference": "Writes unit/integration tests, contributes to engineering wiki, and documents work / Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues."
    },
    {
        "skill": "Data Analysis Troubleshooting",
        "reference": "Writes unit/integration tests, contributes to engineering wiki, and documents work / Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues."
    },
    {
        "skill": "4+ Years Experience with Python",
        "reference": "BS in Computer Science or a related technical field / 4+ years experience with Python / 4+ years of experience with Spark (Databricks) / 4+ years experience with SQL and NoSQL"
    },
    {
        "skill": "4+ Years Terraform",
        "reference": "BS in Computer Science or a related technical field / 4+ years experience with Python / 4+ years of experience with Spark (Databricks) / 4+ years experience with SQL and NoSQL"
    },
    {
        "skill": "4+ Years Spark",
        "reference": "BS in Computer Science or a related technical field / 4+ years experience with Python / 4+ years of experience with Spark (Databricks) / 4+ years experience with SQL and NoSQL"
    },
    {
        "skill": "2+ Years Experience with Looker/Tableau",
        "reference": "2+ years experience in GCP / 2+ years experience with Looker or Tableau / Defines company data assets and data models, spark, sparkSQL, and hiveSQL jobs to populate data models / Designs data integrations and data quality framework"
    },
    {
        "skill": "Data Assets Design",
        "reference": "2+ years experience in GCP / 2+ years experience with Looker or Tableau / Defines company data assets and data models, spark, sparkSQL, and hiveSQL jobs to populate data models / Designs data integrations and data quality framework"
    },
    {
        "skill": "Data Quality Framework",
        "reference": "2+ years experience in GCP / 2+ years experience with Looker or Tableau / Defines company data assets and data models, spark, sparkSQL, and hiveSQL jobs to populate data models / Designs data integrations and data quality framework"
    },
    {
        "skill": "Active Google Cloud Data Engineer Professional Certification",
        "reference": "Required Skills;Active Google Cloud Data Engineer Professional Certification or willingness to get one during first 30 days of the project. Please share your resume at career@tekintegral.com and nkumar@tekintegral.com"
    },
    {
        "skill": "Resume Sharing",
        "reference": "Required Skills;Active Google Cloud Data Engineer Professional Certification or willingness to get one during first 30 days of the project. Please share your resume at career@tekintegral.com and nkumar@tekintegral.com"
    },
    {
        "skill": "Data Engineering, ETL Pipelines, Data Warehousing",
        "reference": "As a Senior Data Engineer, you will work within the Informatics team and contribute to Tendo\u2019s strategic data engineering solutions by ingesting, transforming, and warehousing healthcare-related data from various sources."
    },
    {
        "skill": "Software Engineering, Data Modeling, Database Design",
        "reference": "The ideal candidate should have a strong background in software engineering, data modeling, data warehousing, ETL pipelines, and database design."
    },
    {
        "skill": "Collaboration, Data Scientists, Product Managers, ML Engineers",
        "reference": "You will collaborate with Tendo\u2019s Data Scientists, Product Managers, and Machine Learning Engineers to produce quality data flows and transformations."
    },
    {
        "skill": "Data Engineering, 15+ years experience in data analysis",
        "reference": "Identifying new datasets and integrate them"
    },
    {
        "skill": "Machine Learning, Python/JavaScript development",
        "reference": "7 years of experience in data and software engineering"
    },
    {
        "skill": "Cloud Computing, Active AWS certifications",
        "reference": "5 years of professional experience in at least one programming language (Python, JavaScript or C#)"
    },
    {
        "skill": "Cloud Data Engineering",
        "reference": "Developing data pipelines and other data products using AWS, Microsoft Azure, or Google Cloud Platform"
    },
    {
        "skill": "Data Architecture Experience",
        "reference": "Experience in the design and implementation of data architecture solutions"
    },
    {
        "skill": "Data Processing Expertise",
        "reference": "Substantial SQL expertise, database administration, and scripting data pipelines"
    },
    {
        "skill": "Data Engineering",
        "reference": "You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse."
    },
    {
        "skill": "Data Analysis",
        "reference": "You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You are passionate about lakehouse architecture and have experience using Delta Lake, bronze, silver, and gold data lake design."
    },
    {
        "skill": "Soft Skills",
        "reference": "We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways! We are committed to building a diverse and inclusive environment."
    },
    {
        "skill": "Data Extraction & Transformation",
        "reference": "Proficiency in data extraction and transformation utilizing Spark, Python and REST APIs. Proficiency in data reporting and data integration utilizing Transact-SQL, SQL Views, REST APIs, or other BI Tools."
    },
    {
        "skill": "Cloud Computing",
        "reference": "Experience designing data pipelines with a cloud-native mindset using Azure or AWS. Experience ingesting data from SaaS solutions and other services via API or other related technologies."
    },
    {
        "skill": "Data Modeling & Pipelines",
        "reference": "Gathering data requirements from various business units and translating these requirements into data models. Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems."
    },
    {
        "skill": "Data Integration",
        "reference": "Identifying and modeling all current disparate data sources and the data flows between these data sources. Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success."
    },
    {
        "skill": "Data Security & Privacy",
        "reference": "Ensuring data repositories meet company standards for storage of PII. Applying best practices to ensure the security and privacy of the data repositories."
    },
    {
        "skill": "SAP HANA",
        "reference": "Mandatory Skills SAP Native HANA, Implementation, Configuration, Safe Agile Methodology"
    },
    {
        "skill": "SAP Business Objects (BO)",
        "reference": "Nice To Have Below Teradata, Snowflake, Oracle, Tableau and SAP Business Objects (BO) will add advantage"
    },
    {
        "skill": "Data Analysis and Analyst knowledge",
        "reference": "Candidate should have hand of experience Data Analysis and Analyst knowledge with writing functional and technical specifications."
    },
    {
        "skill": "AWS Developer",
        "reference": "AWS Developer (data Engineer) and Date Governance Manager"
    },
    {
        "skill": "Data Engineering",
        "reference": ""
    },
    {
        "skill": "Date Governance Management",
        "reference": ""
    },
    {
        "skill": "Prototype",
        "reference": "Responsibilities Prototype, build, deploy and manage data engineering pipelines."
    },
    {
        "skill": "build",
        "reference": "Responsibilities Prototype, build, deploy and manage data engineering pipelines."
    },
    {
        "skill": "deploy",
        "reference": "Responsibilities Prototype, build, deploy and manage data engineering pipelines."
    },
    {
        "skill": "manage data engineering pipelines",
        "reference": "Responsibilities Prototype, build, deploy and manage data engineering pipelines."
    },
    {
        "skill": "Contribute to design",
        "reference": "Contribute to the design and creation of high-quality solutions."
    },
    {
        "skill": "creation of high-quality solutions",
        "reference": "Contribute to the design and creation of high-quality solutions."
    },
    {
        "skill": "Work with experts",
        "reference": "Work with other data engineers, business intelligence and machine learning experts to solve real-life, challenging business problems."
    },
    {
        "skill": "solve real-life challenges",
        "reference": "Work with other data engineers, business intelligence and machine learning experts to solve real-life, challenging business problems."
    },
    {
        "skill": "3+ years experience",
        "reference": "Required Qualifications 3+ years of relevant professional experience Deep experience with data engineering, big data and analytical technologies using Azure cloud-based data platforms."
    },
    {
        "skill": "deep data engineering",
        "reference": "Required Qualifications 3+ years of relevant professional experience Deep experience with data engineering, big data and analytical technologies using Azure cloud-based data platforms."
    },
    {
        "skill": "Experience batch/real time data processing tools",
        "reference": "Required Qualifications Experience with batch and real-time data processing tools and technologies: Azure Data Factory, Databricks/Spark, Azure Synapse/DW, Azure Analysis Services"
    },
    {
        "skill": "Azure Data Factory",
        "reference": "Required Qualifications Experience with batch and real-time data processing tools and technologies: Azure Data Factory, Databricks/Spark, Azure Synapse/DW, Azure Analysis Services"
    },
    {
        "skill": "Databricks/Spark",
        "reference": "Required Qualifications Experience with batch and real-time data processing tools and technologies: Azure Data Factory, Databricks/Spark, Azure Synapse/DW, Azure Analysis Services"
    },
    {
        "skill": "Azure Synapse/DW",
        "reference": "Required Qualifications Experience with batch and real-time data processing tools and technologies: Azure Data Factory, Databricks/Spark, Azure Synapse/DW, Azure Analysis Services"
    },
    {
        "skill": "Azure Analysis Services",
        "reference": "Required Qualifications Experience with batch and real-time data processing tools and technologies: Azure Data Factory, Databricks/Spark, Azure Synapse/DW, Azure Analysis Services"
    },
    {
        "skill": "Extensive experience SQL",
        "reference": "Required Qualifications Extensive experience in SQL Knowledge of distributed data solutions, storage systems and columnar databases"
    },
    {
        "skill": "distributed data solutions",
        "reference": "Required Qualifications Extensive experience in SQL Knowledge of distributed data solutions, storage systems and columnar databases"
    },
    {
        "skill": "storage systems",
        "reference": "Required Qualifications Extensive experience in SQL Knowledge of distributed data solutions, storage systems and columnar databases"
    },
    {
        "skill": "Familiarity CI/CD & Git",
        "reference": "Required Qualifications Familiarity with Continuous Integration/Continuous Deployment, & Git Knowledge about Agile development methods like Scrum and Kanban"
    },
    {
        "skill": "Knowledge Agile development methods",
        "reference": "Required Qualifications Familiarity with Continuous Integration/Continuous Deployment, & Git Knowledge about Agile development methods like Scrum and Kanban"
    },
    {
        "skill": "Scrum or Kanban",
        "reference": "Required Qualifications Familiarity with Continuous Integration/Continuous Deployment, & Git Knowledge about Agile development methods like Scrum and Kanban"
    },
    {
        "skill": "Python (3.x)",
        "reference": "Nice to Have Python (3.x), PySpark, R, Kafka"
    },
    {
        "skill": "PySpark",
        "reference": "Nice to Have Python (3.x), PySpark, R, Kafka"
    },
    {
        "skill": "R",
        "reference": "Nice to Have Python (3.x), PySpark, R, Kafka"
    },
    {
        "skill": "Kafka",
        "reference": "Nice to Have Python (3.x), PySpark, R, Kafka"
    },
    {
        "skill": "Knowledge machine learning concepts & frameworks",
        "reference": "Nice to Have Knowledge of key machine learning concepts & ML frameworks like scikit-learn, H2O.ai, Keras, etc."
    },
    {
        "skill": "Data Engineering",
        "reference": "3+ years of experience in data engineering"
    },
    {
        "skill": "Programming Fundamentals",
        "reference": "3+ years of experience in data engineering"
    },
    {
        "skill": "Agile Processes",
        "reference": "You have strong programming fundamentals and are comfortable with agile software processes"
    },
    {
        "skill": "Multi-Language Comfortability",
        "reference": "You have strong programming fundamentals and are comfortable with agile software processes"
    },
    {
        "skill": "Cloud Computing",
        "reference": "Comfortable working with APIs/Webhooks, significant experience with cloud computing environments and infrastructure, understanding & perspective on data catalogs and schema registries"
    },
    {
        "skill": "Data Warehousing",
        "reference": "Comfortable working with APIs/Webhooks, significant experience with cloud computing environments and infrastructure, understanding & perspective on data catalogs and schema registries"
    },
    {
        "skill": "Event Buses",
        "reference": "Comfortable working with APIs/Webhooks, significant experience with cloud computing environments and infrastructure, understanding & perspective on data catalogs and schema registries"
    },
    {
        "skill": "Advanced Python",
        "reference": "Advanced Python and data package (Numpy, Pandas, etc.) skills, you are comfortable with different data modeling techniques and have experience with a data warehouse platform (Redshift, Snowflake, etc.)"
    },
    {
        "skill": "Data Modeling",
        "reference": "Advanced Python and data package (Numpy, Pandas, etc.) skills, you are comfortable with different data modeling techniques and have experience with a data warehouse platform (Redshift, Snowflake, etc.)"
    },
    {
        "skill": "Redshift Experience",
        "reference": "Advanced Python and data package (Numpy, Pandas, etc.) skills, you are comfortable with different data modeling techniques and have experience with a data warehouse platform (Redshift, Snowflake, etc.)"
    },
    {
        "skill": "DevOps",
        "reference": "You practice empathy and kindness, and you look to help others"
    },
    {
        "skill": "DataOps",
        "reference": "You practice empathy and kindness, and you look to help others"
    },
    {
        "skill": "Empathy",
        "reference": "Employee premiums paid 100% by When I Work (Medical benefits, Dental benefits)"
    },
    {
        "skill": "Helpfulness",
        "reference": "Employee premiums paid 100% by When I Work (Medical benefits, Dental benefits)"
    },
    {
        "skill": "Paid Parental Leave",
        "reference": "Professional development allowance, Paid parental leave, Mediated work environment"
    },
    {
        "skill": "Vacation/Holidays",
        "reference": "Professional development allowance, Paid parental leave, Mediated work environment"
    },
    {
        "skill": "Data Engineering, Software Development, SQL",
        "reference": "Required Skills & Experience 2 + yrs of experience in Data Engineering Software Development background SQL (Can do simple code commands) NoSQL Databases Python AWS - EMR, RDS, Redshift, Kinesis, etc"
    },
    {
        "skill": "AWS, Python",
        "reference": "Tech Breakdown 40% SQL/NoSQL 40% AWS 20% Python Daily Responsibilities 100% Data Engineering/Analytics The Offer Bonus eligible You Will Receive The Following Benefits Medical Insurance Dental Benefits Vision Benefits Paid Time Off (PTO)"
    },
    {
        "skill": "Data Analysis",
        "reference": "Daily Responsibilities 100% Data Engineering/Analytics"
    },
    {
        "skill": "Data Engineering",
        "reference": "The Principal Data Engineer manages business requirements gathering, end-to-end solution planning and optimizes data analytics delivery within the Context Engine."
    },
    {
        "skill": "Communication and Leadership",
        "reference": "Partners closely with teams across MD Anderson, including Enterprise Development & Integration and Enterprise Data Science departments in the build out and delivery of end-to-end analytic solutions through the Context Engine Framework."
    },
    {
        "skill": "Data Analysis and Reporting",
        "reference": "Lead/Communicate/Participate and implement complex data analytics deliverables, including data analysis, report requests, metrics, extracts, visualizations, projects or dashboards in a timely manner by leveraging tools and methodologies in line with the Context Engine Strategy."
    },
    {
        "skill": "Support for on-prem to cloud migration",
        "reference": "Role: Data Engineer"
    },
    {
        "skill": "Spark/Scala or Spark/SQL skills",
        "reference": "Role: Data Engineer"
    },
    {
        "skill": "Azure exp required",
        "reference": "Role: Data Engineer"
    },
    {
        "skill": "Kubernetes",
        "reference": "They will be building Medallion architecture - Remote (But prefer candidates from MN/TX/ San Fransico, CA)"
    },
    {
        "skill": "Scala",
        "reference": "They will be building Medallion architecture - Remote (But prefer candidates from MN/TX/ San Fransico, CA)"
    },
    {
        "skill": "Spark",
        "reference": "They will be building Medallion architecture - Remote (But prefer candidates from MN/TX/ San Fransico, CA)"
    },
    {
        "skill": "Python",
        "reference": ""
    },
    {
        "skill": "Synapse",
        "reference": ""
    },
    {
        "skill": "Relevant Experience",
        "reference": "5 years of relevant experience with the following: Bachelor's or master's degree in computer science, Data Engineering, or a related field"
    },
    {
        "skill": "Bachelor's or Master's in Computer Science",
        "reference": "5 years of relevant experience with the following: Bachelor's or master's degree in computer science, Data Engineering, or a related field"
    },
    {
        "skill": "Data Engineering Focus",
        "reference": "Professional experience as a Data Engineer, with a focus on AWS data services and technologies."
    },
    {
        "skill": "AWS data services and technologies",
        "reference": "Professional experience as a Data Engineer, with a focus on AWS data services and technologies."
    },
    {
        "skill": "ETL Expertise",
        "reference": "Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies."
    },
    {
        "skill": "AWS Glue, AWS Lambda, Apache Spark",
        "reference": "Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies."
    },
    {
        "skill": "Programming Languages",
        "reference": "Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing"
    },
    {
        "skill": "Python, Scala, Java",
        "reference": "Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing"
    },
    {
        "skill": "AWS Data Storage Experience",
        "reference": "Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB."
    },
    {
        "skill": "Amazon S3, Amazon Redshift, Amazon DynamoDB",
        "reference": "Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB."
    },
    {
        "skill": "Data Modeling and Warehousing Knowledge",
        "reference": "In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices."
    },
    {
        "skill": "Data Integration Concepts and Best Practices",
        "reference": "In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices."
    },
    {
        "skill": "Big Data Technologies Familiarity",
        "reference": "Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus."
    },
    {
        "skill": "Hadoop, Hive, Presto",
        "reference": "Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus."
    },
    {
        "skill": "SQL Proficiency",
        "reference": "Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle."
    },
    {
        "skill": "Database Technologies",
        "reference": "Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle."
    },
    {
        "skill": "Problem Solving Ability",
        "reference": "Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions."
    },
    {
        "skill": "Analyzing Complex Data Requirements",
        "reference": "Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions."
    },
    {
        "skill": "Communication and Collaboration Skills",
        "reference": "Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment"
    },
    {
        "skill": "Working in Team Environment",
        "reference": "Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Data Engineer at Spokeo, you will be responsible for developing, optimizing, and maintaining the ETL data pipeline."
    },
    {
        "skill": "Big Data Ecosystems",
        "reference": "3+ years of professional experience working in big data ecosystems, preference for Spark"
    },
    {
        "skill": "Data Pipeline Automation",
        "reference": "Automate and integrate new components into the data pipeline."
    },
    {
        "skill": "Adobe CDP Data Engineer",
        "reference": "Hi, please share Adobe CDP Data Engineer profiles"
    },
    {
        "skill": "Remote/C2C",
        "reference": "Remote/C2CJDeering and people management skills"
    },
    {
        "skill": "Motivated, collaborative, fast-paced environment",
        "reference": "A successful candidate will be a highly motivated, collaborative individual; motivated to achieve results in a fast-paced environment."
    },
    {
        "skill": "Data Engineering",
        "reference": "Your Skills 4+ years as a Data or Software Engineer (Semi Sr/Sr), Work experience with Apache Kafka, Strong experience with event-driven architecture and systems. Knowledge of Python, Familiarity with Software Engineering best practices and API Design."
    },
    {
        "skill": "Software Engineering",
        "reference": "Your Skills 4+ years as a Data or Software Engineer (Semi Sr/Sr), Work experience with Apache Kafka, Strong experience with event-driven architecture and systems. Knowledge of Python, Familiarity with Software Engineering best practices and API Design."
    },
    {
        "skill": "Python",
        "reference": "Your Skills 4+ years as a Data or Software Engineer (Semi Sr/Sr), Work experience with Apache Kafka, Strong experience with event-driven architecture and systems. Knowledge of Python, Familiarity with Software Engineering best practices and API Design."
    },
    {
        "skill": "Event-Driven Architecture",
        "reference": "4+ years as a Data or Software Engineer (Semi Sr/Sr), Work experience with Apache Kafka, Strong experience with event-driven architecture and systems. Knowledge of Python."
    },
    {
        "skill": "Apache Kafka",
        "reference": "4+ years as a Data or Software Engineer (Semi Sr/Sr), Work experience with Apache Kafka, Strong experience with event-driven architecture and systems. Knowledge of Python."
    },
    {
        "skill": "Robust Compliance Tools",
        "reference": "The company provides an end-to-end platform that simplifies secondary market transactions for private companies, their employees, and investors. Their cutting-edge technology and robust compliance tools provide unparalleled efficiency and security for all stakeholders involved in private market transactions."
    },
    {
        "skill": "Cutting-Edge Technology",
        "reference": "The company provides an end-to-end platform that simplifies secondary market transactions for private companies, their employees, and investors. Their cutting-edge technology and robust compliance tools provide unparalleled efficiency and security for all stakeholders involved in private market transactions."
    },
    {
        "skill": "Data Engineering",
        "reference": "Responsibilities: Designing and implementing large-scale, distributed data processing systems."
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Experience with big data processing frameworks like Apache Hadoop, Apache Spark, Apache Flink, or similar."
    },
    {
        "skill": "Data Infrastructure Management",
        "reference": "Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms."
    },
    {
        "skill": "Big Data Engineering, High-Velocity High-Volume Stream Processing, Real-time Data Processing, Data Ingestion, Spark Applications, Troubleshooting, Tuning",
        "reference": "6+ years of overall IT experience, 3+ years of experience with high-velocity high-volume stream processing: Apache Kafka and Spark Streaming Experience, Deep knowledge of troubleshooting and tuning Spark applications"
    },
    {
        "skill": "Data Ingestion from Message Queues, Big Data Tools/Technologies, Building & Optimizing 'Big Data' Pipelines, Architectures",
        "reference": "3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV"
    },
    {
        "skill": "Cloud Platforms, Database Solutions, NoSQL Databases, Experience in Azure/AWS Deployment",
        "reference": "3+ years of experience with Cloud platforms e.g. AWS, GCP, etc., 3+ years of experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or BigQuery, 2+ years of experience with NoSQL databases, including HBASE and/or Cassandra"
    },
    {
        "skill": "Data Engineering, Data Pipelines, SQL",
        "reference": "Design, build, and maintain scalable data systems for effective use of data."
    },
    {
        "skill": "Healthcare Data, Big Data Technologies",
        "reference": "Experience with healthcare data and big data technologies such as Hadoop, Spark, and NoSQL databases."
    },
    {
        "skill": "Data Modelling, Cloud Computing",
        "reference": "Strong understanding of data modeling, data lake, data warehousing, cloud computing platforms like AWS, and ETL tools."
    },
    {
        "skill": "Data Analysis and Architecture",
        "reference": "Must Have Skills Data Analysis and Architecture"
    },
    {
        "skill": "Spark & PySpark",
        "reference": "Experience using Big Data frameworks (e.g., Hadoop, Spark), databases for complex data assembly and transformation."
    },
    {
        "skill": "Python",
        "reference": "8+ years' of recent hands-on in an object-oriented language (Java, Scala, Python)."
    },
    {
        "skill": "Sqoop, Pig, Hive",
        "reference": "5+ years' of experience designing and building data pipelines and data-intensive applications."
    },
    {
        "skill": "No SQL Data Stores Object Store",
        "reference": "Experience using Big Data frameworks (e.g., Hadoop, Spark), databases for complex data assembly and transformation."
    },
    {
        "skill": "Design and Development of APIs, Kafka",
        "reference": "Hands-on experience working with Data Analysis and Architecture, Spark, PySpark, Python"
    },
    {
        "skill": "Lead Experience",
        "reference": "Possess expert knowledge in performance, large-scale data distributed system scalability, system architecture, and data engineering best practices."
    },
    {
        "skill": "Mentorship",
        "reference": "Ability to provide leadership, work collaboratively, be a mentor in a fantastic team."
    },
    {
        "skill": "Operational Excellence Evaluation",
        "reference": "Provide leadership, work collaboratively, be a mentor in a fantastic team. Ability to produce both detailed technical work and high-level architectural designs."
    },
    {
        "skill": "Data Engineering, Data Science",
        "reference": "Have at least 5-7 years of experience in a Data Engineering or Data Science role, with a focus on instrumenting data collection, building data pipelines and conducting data-intensive analysis"
    },
    {
        "skill": "Engineering background, SQL expertise",
        "reference": "Have a strong engineering background and are interested in data, Care deeply about the integrity of data, have a good nose for inconsistencies in data, and be able to pinpoint the issue to ensure that the team is not making decisions based on inaccurate or incomplete data"
    },
    {
        "skill": "Python, Amazon Web Services, Scientific computing",
        "reference": "Have extensive experience of a scientific computing language (e.g. Python) and SQL, Have experience building an Amazon Web Services-based system that processes data across multiple data stores and technologies, including MySQL, Redis, Elasticsearch"
    },
    {
        "skill": "SQL Data Warehousing",
        "reference": "In partnership with other business, platform, technology, and analytic teams across the enterprise, design, build and maintain well-engineered data solutions in a variety of environments, including traditional data warehouses, Big Data solutions, and cloud-oriented platforms."
    },
    {
        "skill": "Problem-Solving",
        "reference": "In partnership with other business, platform, technology, and analytic teams across the enterprise, design, build and maintain well-engineered data solutions in a variety of environments, including traditional data warehouses, Big Data solutions, and cloud-oriented platforms."
    },
    {
        "skill": "Data Engineering",
        "reference": "Work with internal and external platforms and systems to connect and align on data sourcing, flow, structure, and subject matter expertise. Work with business stakeholders and strategic partners to implement and support operational and analytic platforms."
    },
    {
        "skill": "Cloud Technologies",
        "reference": "Work with internal and external platforms and systems to connect and align on data sourcing, flow, structure, and subject matter expertise. Work with business stakeholders and strategic partners to implement and support operational and analytic platforms."
    },
    {
        "skill": "Communication Skills",
        "reference": "The incumbent is expected to partner with others throughout the organization (including other engineers, architects, analysts, data scientists, and non-technical audiences) in their daily work. The incumbent will work with cross-functional teams to deliver and maintain data products and capabilities that support and enable strategies at business unit and enterprise levels."
    },
    {
        "skill": "Analytical Skills",
        "reference": "The incumbent is expected to partner with others throughout the organization (including other engineers, architects, analysts, data scientists, and non-technical audiences) in their daily work. The incumbent will work with cross-functional teams to deliver and maintain data products and capabilities that support and enable strategies at business unit and enterprise levels."
    },
    {
        "skill": "Data Engineering, SQL Techniques, Big Data, Python",
        "reference": "As a Data Engineer, you will play a crucial role in designing and implementing business intelligence and ETL solutions."
    },
    {
        "skill": "ETL Processes, Database Performance, Mentoring, Code Reviews",
        "reference": "Key Responsibilities include mentoring team members in ETL processes and standards, participating in code reviews."
    },
    {
        "skill": "Advanced SQL Techniques, Automated Testing, Integration with other tech teams",
        "reference": "Develop and maintain ETL processes ensuring data accuracy and quality. Research and experiment with emerging Data Integration technologies and tools."
    },
    {
        "skill": "Talend, SQL, AWS, Healthcare (HEDIS), Big Data, Diagnosis, Data Manipulation, Data Mining, Cloud Infrastructure, Microsoft SQL Servers",
        "reference": "About the job"
    },
    {
        "skill": "Communication, Organization, Flexible Work Solutions, Competitive Pay, Paid Time Off, Health Insurance Coverage, 401(k), Stock Purchase Plans, Tuition Reimbursement, Best-in-Class Training",
        "reference": "Our Comprehensive Benefits Package"
    },
    {
        "skill": "Data Pipelines, Data Ingestion, Transformation, Validation / Quality, Optimization, Orchestration, DevSecOps, Talend Implementation, Standardized Data Management, Business Intelligence Analytics, Real-time Processing, Quality Assurance Testing",
        "reference": "Position Purpose and Duties"
    },
    {
        "skill": "Data Pipelines, Talend Implementation",
        "reference": "Develops and operationalizes data pipelines to make data available for consumption including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment."
    },
    {
        "skill": "Data Management Procedures",
        "reference": "Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)."
    },
    {
        "skill": "Business Intelligence Analytics",
        "reference": "Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analytics."
    },
    {
        "skill": "Data Engineering",
        "reference": "Responsibilities:Designing and implementing large-scale, distributed data processing systems using technologies"
    },
    {
        "skill": "Big Data Technologies",
        "reference": "Proven experience as a big data engineer or a similar role with deep understanding of big data technologies"
    },
    {
        "skill": "Data Analysis",
        "reference": "Collaborating with data scientists, data analysts, and other stakeholders to understand data requirements"
    },
    {
        "skill": "Data Integration",
        "reference": "As a data integration engineer, your role is hands-on coding, configuring, and maintaining data integration for our customers"
    },
    {
        "skill": "Communication & Empathy",
        "reference": "Excellent communication and presentation skills with various technical and non-technical audiences; must have strong empathy and emotional intelligence with diverse customers"
    },
    {
        "skill": "Technical Skills",
        "reference": "Integration Experience, JavaScript proficiency, WordPress/Shopify knowledge, Data flows, APIs, Web services understanding, HTML5/CSS3, Adobe Analytics/Google Analytics experience"
    },
    {
        "skill": "Data Engineering",
        "reference": "Lead Cross-Functional Team Engagement, Drive ETL Process Innovation, Architect Scalable Data Storage Solutions, Ensure Data Pipeline Integrity, Create Comprehensive Process Documentation, Champion Data Security and Compliance, Stay Ahead of Industry Trends, Mentor and Develop Team Talent"
    },
    {
        "skill": "Data Warehousing",
        "reference": "Experience in SQL and Data Modeling, Advanced Problem-Solving Skills, Effective Communication and Teamwork, Analytical and Critical Thinking, Commitment to Continuous Learning"
    },
    {
        "skill": "Databricks Expertise",
        "reference": "Mastery in Databricks Ecosystem, Cloud-Platform Wizardry, ETL Development Virtuoso, Version Control Guru, Big Data Technologies Connoisseur"
    },
    {
        "skill": "Snowflake, Coding",
        "reference": "Expert w/ Snowflake & Coding Ability"
    },
    {
        "skill": "Data Pipeline Development",
        "reference": "Experience building data pipelines from scratch and/or working with APIs"
    },
    {
        "skill": "Fivetran, DBT (Plus)",
        "reference": "Some Experience With: Fivetran and/or DBT"
    },
    {
        "skill": "Java",
        "reference": "Experience in programming language Java and understanding of the software development life cycle"
    },
    {
        "skill": "Full Stack Development",
        "reference": "For Java /Full Stack/Software Programmer"
    },
    {
        "skill": "Data Analysis",
        "reference": "Required Skills for data Science/Machine learning Positions"
    },
    {
        "skill": "Machine Learning",
        "reference": "Required Skills for data Science/Machine learning Positions"
    },
    {
        "skill": "Project Management",
        "reference": "Shortlisting and selection is totally based on clients discretion not ours"
    },
    {
        "skill": "Communication",
        "reference": "Highly motivated, self-learner, and technically inquisitive, Excellent written and verbal communication skills"
    },
    {
        "skill": "data engineering, leadership, mentoring",
        "reference": "Provide leadership to the engineering teams, own the complete solution life cycle and mentor junior team members."
    },
    {
        "skill": "data technologies, big data, scalable pipelines",
        "reference": "Experience with Relational and NoSQL databases, open data formats, programming languages like Python, Scala, frameworks, big data technologies such as Spark, Hadoop, MapReduce, building data pipelines, ETL processes."
    },
    {
        "skill": "collaboration, problem-solving, continuous improvement",
        "reference": "Collaborate with product managers, team members, customers and engineering teams to solve complex problems, mentor junior team members professionally, share best practices, and improve processes within and across teams."
    },
    {
        "skill": "Data Engineering",
        "reference": "Innovate and advise on the latest technologies and standard methodologies in Data Engineering."
    },
    {
        "skill": "Collaboration",
        "reference": "Collaborate across departments to make a wide variety of data accessible and relevant for researchers."
    },
    {
        "skill": "Cloud-based integration",
        "reference": "Design and develop infrastructure and processes for loading public and proprietary data from multiple source systems."
    },
    {
        "skill": "GCP Data Architect/Engineer",
        "reference": "Experience in building enterprise level solution on GCP cloud environment."
    },
    {
        "skill": "Learning, Independence, Decision Making",
        "reference": "Continuously learn, work independently, and make decisions with minimal supervision."
    },
    {
        "skill": "Technological Cloud Advocacy",
        "reference": "Be a technological cloud advocate to a wider audience inside and outside of the business."
    },
    {
        "skill": "Azure Data Engineering",
        "reference": "As an Azure Data Engineer at Techstra Solutions, you will design and implement data architecture on the Azure cloud platform."
    },
    {
        "skill": "Data Infrastructure Management",
        "reference": "Ensure data infrastructure meets business needs and adheres to industry best practices."
    },
    {
        "skill": "Strategic Thinking",
        "reference": "If you are a strategic thinker with strong technical expertise, this role offers an exciting opportunity to shape the data landscape."
    },
    {
        "skill": "Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Senior GCP Data Engineer",
        "reference": "Immediate Position with Known Ford Manager Need submittal. Senior GCP Data Engineer Fulltime permanent Immediate start 100% Remote Please note that this position is a hands-on coding experience."
    },
    {
        "skill": "Fulltime Permanent",
        "reference": "Senior GCP Data Engineer Fulltime permanent Immediate start 100% Remote Please note that this position is a hands-on coding experience."
    },
    {
        "skill": "Hands-on Coding Experience",
        "reference": "Please note that this position is a hands-on coding experience."
    },
    {
        "skill": "Data 10+ Years",
        "reference": "Overall experience in Data 10+ Years"
    },
    {
        "skill": "5 to 6 years Hands on experience in implementing GCP services in enterprise setup",
        "reference": ""
    },
    {
        "skill": "2-3 years of Python Development Experience",
        "reference": "2-3 years of python development experience"
    },
    {
        "skill": "Lead Complex's Big Data and Data Warehousing Projects",
        "reference": "Expert in airflow and Apache beam 3-4 years' experience in implementing IAC and CICD pipelines Provide technical and thought leadership to the Teams and drive the implementation"
    },
    {
        "skill": "Expert in Airflow and Apache Beam",
        "reference": ""
    },
    {
        "skill": "3-4 Years Experience Implementing IaC and CI/CD Pipelines",
        "reference": ""
    },
    {
        "skill": "Provide Technical Leadership to Teams",
        "reference": "Provide technical and thought leadership to the Teams and drive the implementation"
    },
    {
        "skill": "Experience with Tech Lead Interview Panel",
        "reference": "The interview round will be a fully technical with the tech leads as panel members (who are hands-on with the code and data)Below are the questions asked by the panel during the client round:"
    },
    {
        "skill": "Data Pipeline Setup for Batch Input and Streaming Input",
        "reference": "Q1. How to Set up Data Pipelines (for loading primarily batch input and streaming input)"
    },
    {
        "skill": "Experience with Airflow",
        "reference": "Q2. How to use Airflow"
    },
    {
        "skill": "BigQuery Experience for Loading Data, Stored Procedures, Analytics Consumption",
        "reference": "Q3. How to use BigQuery (for loading data, stored procedures, consuming from BigQuery for analytics)"
    },
    {
        "skill": "Handling Security and Exceptions",
        "reference": "Q4. How to handle security and exceptions"
    },
    {
        "skill": "GCP Best Practices",
        "reference": "Best practices with GCP elines"
    },
    {
        "skill": "InterSystems IRIS, Healthcare Experience, 15+ Years",
        "reference": "15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environment"
    },
    {
        "skill": "Cerner Integration, Data Mapping, Data Loss Minimization",
        "reference": "Evaluate and integrate data from multiple sources, which requires data mapping from one data source to another minimizing any data loss"
    },
    {
        "skill": "VistA Extraction, Ensemble Production, Validation",
        "reference": "Review Domain adds to Ensemble Production. Validate edits made to Domain record type, schema version, status, and payload size via the GUI Interface and Rule Builder. Validate Ensemble data flows built using VX130 ClassBuilder"
    },
    {
        "skill": "AWS Data Warehouse Architect/Engineer",
        "reference": "Role title - AWS Data Warehouse-Data Lake Architect/Engineer"
    },
    {
        "skill": "5+ years experience as data warehouse architect",
        "reference": "Experience as a data warehouse architect"
    },
    {
        "skill": "AWS Redshift, Glue, Athena, EC2",
        "reference": "Advanced AWS skills in S3, Lake Formation, Redshift, Glue, Athena, Data Pipeline and EC2"
    },
    {
        "skill": "Data pipelines development",
        "reference": "Experience developing data pipelines in AWS"
    },
    {
        "skill": "Data migration expertise from on-prem to cloud",
        "reference": "5+ years of experience as a data engineer and/or data integration developer Experience migrating from an on-prem data warehouse to AWS cloud data lake and Redshift environment"
    },
    {
        "skill": "Advanced SQL, Python development",
        "reference": "Advanced SQL programming skills Advanced Python development"
    },
    {
        "skill": "Informatica PowerCenter/IDMC knowledge",
        "reference": "Experience migrating Informatica PowerCenter mappings to Informatica IDMC Experience administering Informatica PowerCenter and IDMC"
    },
    {
        "skill": "Kafka data streams experience",
        "reference": "Experience with Kafka data streams"
    },
    {
        "skill": "AWS RDS administration skills",
        "reference": "Experience with AWS RDS administration, specifically SQL Server"
    },
    {
        "skill": "Governance process and access control establishment",
        "reference": "Experience establishing governance processes, development standards and access controls within the AWS data lake environment."
    },
    {
        "skill": "GCP Cloud Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Remote Location",
        "reference": "About the job"
    },
    {
        "skill": "6+ Month Duration",
        "reference": "About the job"
    },
    {
        "skill": "Infosys Implementation Partner",
        "reference": "Implementation Partner: Infosys End Client: To be disclosed"
    },
    {
        "skill": "End Client Disclosure",
        "reference": "Implementation Partner: Infosys End Client: To be disclosed"
    },
    {
        "skill": "Jd 6+ Years Experience",
        "reference": "Title: GCP Cloud Data Engineer"
    },
    {
        "skill": "Cloud Migration (GCP)",
        "reference": "Title: GCP Cloud Data Engineer"
    },
    {
        "skill": "Python, Pyspark, Hadoop, Big Query, GCP Services",
        "reference": ""
    },
    {
        "skill": "SQL Hand-on Experience",
        "reference": ""
    },
    {
        "skill": "Cloud Migration (GCP)",
        "reference": ""
    },
    {
        "skill": "Data Modeling",
        "reference": "Understanding of data modeling concepts, including Star-Schema Modeling, Schema Modeling, Fact and Dimension tables."
    },
    {
        "skill": "Database Knowledge",
        "reference": "Strong knowledge of both SQL and NoSQL databases."
    },
    {
        "skill": "ETL Development",
        "reference": "Developing automated ETL procedures to load data from various sources into a data warehouse."
    },
    {
        "skill": "ETL/ELT Development",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data Integration",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core Programming Languages",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Data Engineering",
        "reference": "Responsible for developing, maintaining, and optimizing cloud-based data infrastructure, designing efficient data extraction and ingestion processes."
    },
    {
        "skill": "Cloud Solutions",
        "reference": "Experience with implementing cloud solutions with Azure, AWS, or GCP."
    },
    {
        "skill": "Containerization",
        "reference": "2+ years with containerization with Docker or Podman experience."
    },
    {
        "skill": "Senior Azure Data Engineer",
        "reference": "At OmniData, we are searching for a remote Senior Azure Data Engineer that has hands on production experience in developing PySpark solutions in Synapse Analytics on data warehousing and analytics projects."
    },
    {
        "skill": "Exceptional reputation for mentoring less experienced teammates",
        "reference": "We need a team player who has a an exceptional reputation for mentoring less experienced teammates."
    },
    {
        "skill": "Strong technical aptitude with expertise in translating complex technical information",
        "reference": "We seek someone with a strong technical aptitude with expertise in translating complex technical information that appropriately meets the needs of the client while skillfully deploying the best strategies for client analytics goals."
    },
    {
        "skill": "Deep mentorship, great work/life balance and opportunity to be part of creating a consulting firm",
        "reference": "In return, we offer deep mentorship, a great work/life balance, and the opportunity to be part of creating a consulting firm that makes a difference for our clients!"
    },
    {
        "skill": "Experience in Big Data, Data Warehouse and Analytics projects",
        "reference": "You will work on various Big Data, Data Warehouse and Analytics projects for our world class customers."
    },
    {
        "skill": "Contribute collaboratively to team meetings using experience base",
        "reference": "In addressing complex client needs, you will be integrated into appropriately sized and skilled teams. This will give you the opportunity to analyze requirements, develop data and analytical solutions, and execute as part of the project team, all while working with the latest tools, such as Azure Synapse Analytics and related Microsoft technologies."
    },
    {
        "skill": "Experience in Python, DAX",
        "reference": "Nice for you to have - Experience with Python, Experience gathering requirements and working within various project delivery methodologies, Experienced working as a customer facing consultant, Exposure to DAX"
    },
    {
        "skill": "ETL/ELT Development, Implementation and Support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data Integration Concepts",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core Programming Languages - SQL, Python",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "ETL",
        "reference": "Data Engineer serves as an ETL developer and system administrator for the agency's multi-year Data Management Project to modernize the client's data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff."
    },
    {
        "skill": "System Admin",
        "reference": "Experience with System Administration of Linux-based Operating Systems and Servers (12 Years), Experience with System Administration of Windows-based Operating Systems and Servers (12 Years), Experience with System Administration of Microsoft SQL Server (12 Years), Experience in the maintenance and monitoring of system & network related activities associated with data security and controls (12 Years), Experience leveraging SQL relational databases to manage complex datasets and analytical reporting (12 Years), Experience with ETL processes (12 Years)."
    },
    {
        "skill": "Data Architecture Expertise",
        "reference": "11-15 years of experience needed to serve as the subject matter expert on data architecture for the client's multi-year Data Management Project."
    },
    {
        "skill": "Software Development",
        "reference": "Required Skills & Experience 5+ years of software development experience, 3+ years of experience with Map-Reduce, Hive, Spark, etc."
    },
    {
        "skill": "Map-Reduce",
        "reference": "Required Skills & Experience 5+ years of software development experience, 3+ years of experience with Map-Reduce, Hive, Spark, etc."
    },
    {
        "skill": "Hive",
        "reference": "Required Skills & Experience 5+ years of software development experience, 3+ years of experience with Map-Reduce, Hive, Spark, etc."
    },
    {
        "skill": "Spark",
        "reference": "Required Skills & Experience 5+ years of software development experience, 3+ years of experience with Map-Reduce, Hive, Spark, etc."
    },
    {
        "skill": "SQL",
        "reference": "Experience in UNIX shell-scripting, Bachelor\u2019s degree in Engineering or Computer Science or equivalent, etc."
    },
    {
        "skill": "UNIX Shell Scripting",
        "reference": "Experience in UNIX shell-scripting, Bachelor\u2019s degree in Engineering or Computer Science or equivalent, etc."
    },
    {
        "skill": "Engineering/Computer Science Degree",
        "reference": "Experience in UNIX shell-scripting, Bachelor\u2019s degree in Engineering or Computer Science or equivalent, etc."
    },
    {
        "skill": "Data Engineering",
        "reference": "Tech Breakdown 100% Data Engineering, Daily Responsibilities 100% Hands On"
    },
    {
        "skill": "Hands On Daily Responsibilities",
        "reference": "Tech Breakdown 100% Data Engineering, Daily Responsibilities 100% Hands On"
    },
    {
        "skill": "Big Data DevOps Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Data Engineering, Hadoop, Hive, HBase, Spark, Scala, Java, Jenkins, CI/CD, Kafka, AWS, Azure, GCP, Docker, Kubernetes",
        "reference": "Requirement and Proven experience as a Data Engineer"
    },
    {
        "skill": "Structured interviewing, candidate assessment, technical expertise evaluation, rubrics, client reporting",
        "reference": "Responsibilities of the role"
    },
    {
        "skill": "Data Engineering, DevOps",
        "reference": "Design, develop and maintain data infrastructure, build ETL pipelines, collaborate with cross-functional teams, optimize performance, manage CI/CD pipelines, implement IaC principles for efficient software delivery."
    },
    {
        "skill": "Biopharmaceutical experience, Regulatory compliance",
        "reference": "Minimum of 3 years in biopharma, knowledge in industry processes and data sources, familiarity with master data management, life sciences-specific data handling, and medical coding."
    },
    {
        "skill": "Azure Cloud, Data Management",
        "reference": "Proficiency in Azure services like ADF, Databricks, SQL DB, AKS, deployment experience, understanding of security and compliance practices, infrastructure-as-code tools for scalable resource management."
    },
    {
        "skill": "Data Engineering, ETL/ELT, SQL, Python",
        "reference": "Working with and utilizing standardized tools, technologies and processes"
    },
    {
        "skill": "Azure/AWS cloud services, Linux/Unix, Data Integration",
        "reference": "Understanding or knowledge of Azure/AWS cloud services \u2013 Azure Data Catalog, Azure Data Lake Store, Azure BLOB Store, Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management"
    },
    {
        "skill": "Data Pipeline Development, Process Features",
        "reference": "Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores. Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources."
    },
    {
        "skill": "Senior Data Engineer, Python, SQL, Leading Efforts",
        "reference": "As a Senior Data Engineer Consultant"
    },
    {
        "skill": "Backend services, data pipelines, scalable infrastructure",
        "reference": "You'll be leading efforts in building scalable backend services and data pipelines"
    },
    {
        "skill": "Microservices, Python, Scala, Ruby, AWS Lambda, Kinesis, SQS, RDS, Dynamo, Snowflake, Spark",
        "reference": "Work across multiple microservices written in Python, Scala, and Ruby using various technologies"
    },
    {
        "skill": "AWS data warehouse/lake experience",
        "reference": "5+ years of experience as a data warehouse architect, designing and implementing an AWS data lake and Redshift data warehouse"
    },
    {
        "skill": "Excellent communication skills",
        "reference": "Communication MUST be excellent"
    },
    {
        "skill": "Advanced SQL programming skills",
        "reference": ""
    },
    {
        "skill": "Advanced AWS skills",
        "reference": "Advanced AWS skills in S3, Lake Formation, Redshift, Glue, Athena, Data Pipeline and EC2 Experience developing data pipelines in AWS"
    },
    {
        "skill": "Experience with data migration",
        "reference": "5+ years of experience as a data engineer and/or data integration developer, Experience migrating from on-prem data warehouse to AWS cloud data lake and Redshift environment"
    },
    {
        "skill": "Advanced Python development",
        "reference": ""
    },
    {
        "skill": "Informatica PowerCenter/IDMC experience",
        "reference": "Experience migrating Informatica PowerCenter mappings to Informatica IDMC, Experience administering Informatica PowerCenter and IDMC"
    },
    {
        "skill": "Kafka data streams knowledge",
        "reference": ""
    },
    {
        "skill": "AWS RDS administration experience",
        "reference": "Experience with AWS RDS administration, specifically SQL Server"
    },
    {
        "skill": "Establish governance processes and access controls",
        "reference": "Experience establishing governance processes, development standards and access controls within the AWS data lake environment."
    },
    {
        "skill": "Talend development",
        "reference": "Develops and operationalizes data pipelines to make data available for consumption"
    },
    {
        "skill": "Data pipeline management",
        "reference": "Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Talend implementation of data flows."
    },
    {
        "skill": "Big Data, Data Processing experience",
        "reference": "Designs and maintains real-time processing applications and real-time data pipelines"
    },
    {
        "skill": "Data Engineering",
        "reference": "Design, build and maintain data pipelines, data warehouses, and other infrastructure"
    },
    {
        "skill": "Collaboration",
        "reference": "Work closely with data scientists, analysts, and stakeholders for business requirements and solutions"
    },
    {
        "skill": "Mentoring & Training",
        "reference": "Mentor and train team members to ensure high-quality deliverables and drive innovation"
    },
    {
        "skill": "Data Engineering",
        "reference": "The Senior Data Engineer will play a crucial role in the Stride Marketing Organization and will work closely with the VP of Marketing Technology and Marketing Technology and Analytics team to define, build, and maintain the Marketing Analytics data infrastructure utilizing data from multiple platforms."
    },
    {
        "skill": "Data Warehousing",
        "reference": "Assist with the creation of the data architecture to ensure data is available, actionable, trustworthy, and reusable for analytics and other purposes. Ability to build processes that support data transformation, workload management, data structures, dependency and metadata."
    },
    {
        "skill": "Data Processing",
        "reference": "Identify, design and help implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions."
    },
    {
        "skill": "Snowflake, Coding",
        "reference": "Expert w/ Snowflake & Coding Ability"
    },
    {
        "skill": "Data Pipeline, APIs",
        "reference": "Experience building data pipelines from scratch and/or working with APIs"
    },
    {
        "skill": "Technical Leadership",
        "reference": "Manage development velocity, team capacity, and backlogs Partner closely with the product team Take on key assignments and delegate as needed Act as the main technical point of contact for engineering Translate technical requirements to the rest of the engineering team"
    },
    {
        "skill": "Snowflake, Coding Ability",
        "reference": "Expert w/ Snowflake & Coding Ability"
    },
    {
        "skill": "Build Data Pipelines, APIs",
        "reference": "Experience building data pipelines from scratch and/or working with APIs"
    },
    {
        "skill": "Data Architecture, Modeling",
        "reference": "Data architecture via Snowflake, Data modeling"
    },
    {
        "skill": "Python",
        "reference": "Senior Data Engineer with at least 7 years of professional experience, expertise in Python, PySpark, EMR, Airflow, AWS and a STEM degree."
    },
    {
        "skill": "Pyspark",
        "reference": "Senior Data Engineer with at least 7 years of professional experience, expertise in Python, PySpark, EMR, Airflow, AWS and a STEM degree."
    },
    {
        "skill": "EMR",
        "reference": "Senior Data Engineer with at least 7 years of professional experience, expertise in Python, PySpark, EMR, Airflow, AWS and a STEM degree."
    },
    {
        "skill": "Airflow",
        "reference": "Senior Data Engineer with at least 7 years of professional experience, expertise in Python, PySpark, EMR, Airflow, AWS and a STEM degree."
    },
    {
        "skill": "AWS",
        "reference": "Senior Data Engineer with at least 7 years of professional experience, expertise in Python, PySpark, EMR, Airflow, AWS and a STEM degree."
    },
    {
        "skill": "STEM degree",
        "reference": "Senior Data Engineer with at least 7 years of professional experience, expertise in Python, PySpark, EMR, Airflow, AWS and a STEM degree."
    },
    {
        "skill": "5+ Years of Experience",
        "reference": "Basic Qualifications (Required Skills & Experience) - 5+ years of experience, 8+ experience preferred Python Pyspark Spark AWS Airflow EMR ETL work SQL."
    },
    {
        "skill": "Python",
        "reference": "Basic Qualifications (Required Skills & Experience) - 5+ years of experience, 8+ experience preferred Python Pyspark Spark AWS Airflow EMR ETL work SQL."
    },
    {
        "skill": "Pyspark",
        "reference": "Basic Qualifications (Required Skills & Experience) - 5+ years of experience, 8+ experience preferred Python Pyspark Spark AWS Airflow EMR ETL work SQL."
    },
    {
        "skill": "Fully Remote",
        "reference": "You will receive the following benefits: Medical Insurance Dental Benefits Vision Benefits 401(k) Fully Remote Equity and Bonuses involved."
    },
    {
        "skill": "Equity",
        "reference": "You will receive the following benefits: Medical Insurance Dental Benefits Vision Benefits 401(k) Fully Remote Equity and Bonuses involved."
    },
    {
        "skill": "Bonuses",
        "reference": "You will receive the following benefits: Medical Insurance Dental Benefits Vision Benefits 401(k) Fully Remote Equity and Bonuses involved."
    },
    {
        "skill": "Data Engineering, Blockchain, Excellent Communication",
        "reference": "About the job"
    },
    {
        "skill": "Cross-functional Collaboration, Data Models, ETL Pipelines",
        "reference": "The Role You'll work with a cross-functional team"
    },
    {
        "skill": "Complex Problem Solving, Big Data Processes, Ambiguous Environment",
        "reference": "Work on complex problems ranging from mathematical proofs to big data processes"
    },
    {
        "skill": "Finance, Gaming/Crypto Gaming, Blockchain Experience",
        "reference": "Preferred experience with finance, gaming/crypto gaming, blockchain, or crypto protocols."
    },
    {
        "skill": "Advanced Python, Pandas Package, Data Analysis",
        "reference": "Required Experience: Advanced skills with python (or similar language) for data collection, analysis, and communication"
    },
    {
        "skill": "Web3, Smart Contract Development, Blockchain Data",
        "reference": "Experience in Web3 including smart contract development, blockchain data, and DeFi protocols such as Uniswap."
    },
    {
        "skill": "AWS Cloud Services, Serverless Infrastructure, Data Engineering",
        "reference": "Hands-on experience in AWS Cloud services including building and implementing server and serverless data engineering"
    },
    {
        "skill": "Data Warehousing, Pipelines Design, ETL Experience",
        "reference": "Good knowledge of ETL, data warehousing, and pipelines design."
    },
    {
        "skill": "Test-Driven Development, Data Science, Modeling",
        "reference": "Understanding of Test-Driven Development. Good knowledge of ETL, data warehousing, and pipelines design."
    },
    {
        "skill": "Data Focused Role, ETL Experience, Querying SQL Data",
        "reference": "Preferred 5+ years of experience in a data focused role including ETL, querying data with SQL, data modeling, data collection, data analysis, and data science."
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Collaborate, design, build and maintain data systems",
        "reference": "Responsibilities"
    },
    {
        "skill": "Data warehouse lifecycle management",
        "reference": "Responsibilities"
    },
    {
        "skill": "Technology platforms, design approaches, techniques",
        "reference": "Responsibilities"
    },
    {
        "skill": "Integrated quality solutions",
        "reference": "Responsibilities"
    },
    {
        "skill": "Cloud engineering, Google Cloud Platform",
        "reference": "Qualifications"
    },
    {
        "skill": "End-to-end project management",
        "reference": "Qualifications"
    },
    {
        "skill": "Data modelling, ETL development, Data Warehousing",
        "reference": "Qualifications"
    },
    {
        "skill": "Agile methodologies, software engineering best practices",
        "reference": "Familiarity with software engineering best practices such as Agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations."
    },
    {
        "skill": "ETL/ELT development, implementation, and support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data integration concepts understanding",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core programming languages - SQL, Python",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "AWS, Python, Data Engineering",
        "reference": "Looking for a mid-level data engineer with AWS background in designing, building, and maintaining data pipelines and infrastructure using Amazon Web Services (AWS) technologies such as GLUE, pyspark and other data services."
    },
    {
        "skill": "ETL, Data Management",
        "reference": "Data Engineer who has background in python script language used for ETL, data management process, ensuring that data is collected, processed, stored."
    },
    {
        "skill": "SQL, Databases",
        "reference": "Excellent understanding of SQL programming and databases."
    },
    {
        "skill": "Data Engineering",
        "reference": "Develops systems to manage data flow throughout Signify Health\u2019s infrastructure."
    },
    {
        "skill": "SQL Queries",
        "reference": "Analyze and manipulate data by writing and running SQL queries."
    },
    {
        "skill": "Log Analysis",
        "reference": "Analyze logs to identify and prevent potential issues from occurring."
    },
    {
        "skill": "IRIS Experience",
        "reference": "Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS."
    },
    {
        "skill": "InterSystems experience",
        "reference": "Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS."
    },
    {
        "skill": "SQL Experience",
        "reference": "Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS."
    },
    {
        "skill": "Healthcare Environment",
        "reference": "Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS. Title: Senior Health Data Migration Engineer."
    },
    {
        "skill": "15+ Years of Professional Work Experience",
        "reference": "Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS. Title: Senior Health Data Migration Engineer."
    },
    {
        "skill": "Veterans Affairs",
        "reference": "ActivitiesSupport the Department of Veterans Affairs (VA) Electronic Health Record Modernization Integration Office (EHRM-IO) as a Senior Health Data Migration Engineer."
    },
    {
        "skill": "Bachelor's Degree in Computer Science",
        "reference": "ActivitiesSupport the Department of Veterans Affairs (VA) Electronic Health Record Modernization Integration Office (EHRM-IO) as a Senior Health Data Migration Engineer."
    },
    {
        "skill": "ETL/ELT development, implementation, and support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Core Programming Languages (SQL, Python)",
        "reference": "Understanding or knowledge of data integration concepts, 3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Data integration technologies (Azure Data Catalog, Azure Data Lake Store, etc.)",
        "reference": "Understanding or knowledge of Azure cloud services \u2013 Azure Data Catalog, Azure Data Lake Store, Azure BLOB Store"
    },
    {
        "skill": "Complex tasks and critical programs",
        "reference": "Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach"
    },
    {
        "skill": "Quality control (code, process, data assets)",
        "reference": "Ensure quality of code, processes, and data assets"
    },
    {
        "skill": "Sr Data Engineer",
        "reference": "Job Title - Sr Data Engineer (strong Azure Data Bricks)"
    },
    {
        "skill": "12 Months Remote Duration",
        "reference": ""
    },
    {
        "skill": "Deliver and Maintain Data Engineering Components",
        "reference": "Primary Responsibilities:"
    },
    {
        "skill": "Design Enterprise Data Warehouse",
        "reference": "Design enterprise data warehouse components through partnerships with Business Stakeholders, Business Analysts, Data Engineers and Developers."
    },
    {
        "skill": "Data Integration & Transformation Solutions",
        "reference": "Develop data integration and transformation solutions to meet the input needs of the models."
    },
    {
        "skill": "Batch Jobs Development",
        "reference": "Develop and support batch jobs."
    },
    {
        "skill": "Unit & Regression Testing",
        "reference": "Perform unit and regression testing."
    },
    {
        "skill": "Code/Peer Reviews",
        "reference": "Perform code/peer reviews to ensure adherence to established design and development standards."
    },
    {
        "skill": "Deployment Scripts, Checklists & Runbooks",
        "reference": "Produce deployment scripts, checklists, playbook and operations runbook in accordance with SDLC & change management requirements."
    },
    {
        "skill": "Monitor Scheduled Jobs & Platform Performance",
        "reference": "Monitor the scheduled to jobs and performance of the platforms to ensure smooth operation."
    },
    {
        "skill": "Troubleshoot and Fix Issues",
        "reference": "Troubleshoot and fix issues that arise with data and/or processes."
    },
    {
        "skill": "Experience with Software Development",
        "reference": "Required Experience: 10+ years of software development experience"
    },
    {
        "skill": "Microsoft BI Tools Experience",
        "reference": "5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS"
    },
    {
        "skill": "Azure Platform Experience",
        "reference": "3+ years of experience in Azure using Data Factory, Databricks & ADLS (MUST HAVE)"
    },
    {
        "skill": "RDBMS Design and Development",
        "reference": "5+ years of experience in RDBMS design and development"
    },
    {
        "skill": "Visual Studio Development Environment",
        "reference": "Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques"
    },
    {
        "skill": "Professional Software Engineering Best Practices",
        "reference": "Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing and operations"
    },
    {
        "skill": "Bachelor's Degree in Related Field",
        "reference": "Bachelor's degree in related field (prefer CS major)"
    },
    {
        "skill": "Snowflake, Coding",
        "reference": "Expert with Snowflake & Coding Ability"
    },
    {
        "skill": "Building Data Pipelines, Snowflake",
        "reference": "Building data pipelines from scratch and/or working with APIs"
    },
    {
        "skill": "Data Architecture, Modeling",
        "reference": "Data architecture via Snowflake, Data modeling"
    },
    {
        "skill": "ETL/ELT development, implementation and support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data integration concepts understanding",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core programming languages proficiency",
        "reference": "Demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Data Quality",
        "reference": "Design, develop, implement, and sustain data governance policies and procedures"
    },
    {
        "skill": "Data Management",
        "reference": "Identify and collaborate with data stewards; Create and maintain metadata; Collaborate with infrastructure team on data architecture"
    },
    {
        "skill": "Communication",
        "reference": "Communicate across varying data roles to solve cross-domain data governance issues"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Data Architecture",
        "reference": "Design, construct, install, and maintain large-scale data processing pipelines and other infrastructure."
    },
    {
        "skill": "Database Solutions",
        "reference": "Set the data strategy by selecting, architecting, and integrating cutting-edge database solutions."
    },
    {
        "skill": "Data Integration, Fivetran Specialist, Problem-solving",
        "reference": "As a Data Integration Engineer/ Fivetran Specialist, you will be at the forefront of our organization's data integration efforts."
    },
    {
        "skill": "Technical Background, Cross-Functional Teamwork, Internal Support",
        "reference": "This role requires a strong technical background, excellent problem-solving skills, and the ability to work effectively in a cross-functional team."
    },
    {
        "skill": "Snowflake, Coding",
        "reference": "Expert with Snowflake & Coding Ability"
    },
    {
        "skill": "Data Pipeline, API Experience",
        "reference": "Experience building data pipelines from scratch and/or working with APIs"
    },
    {
        "skill": "Fivetran, DBT Experience",
        "reference": "Some experience with - Fivetran and/or DBT"
    },
    {
        "skill": "Data Engineering, Data Integration, Design, Development",
        "reference": "Overjet is hiring a talented Staff Data Engineer to design, develop, and implement data integration solutions."
    },
    {
        "skill": "Cross-functional Collaboration, Data Modelling, Database Design",
        "reference": "Collaborate with cross-functional teams, including Machine Learning engineers and product managers"
    },
    {
        "skill": "Product Development, Data Resources Management",
        "reference": "Guide the development of data resources, support new product launches and improve product runtime performance"
    },
    {
        "skill": "Data Engineering Experience",
        "reference": "Title: Data Engineer Terms: W2 Contract/ 2 yrs +"
    },
    {
        "skill": "2 Years Contract",
        "reference": "Title: Data Engineer Terms: W2 Contract/ 2 yrs +"
    },
    {
        "skill": "Remote Work in USA",
        "reference": "Location: Remote in USA/Must be able to work PST Hours"
    },
    {
        "skill": "PST Hours",
        "reference": "Location: Remote in USA/Must be able to work PST Hours"
    },
    {
        "skill": "Oracle Graph Database Experience",
        "reference": "Qualifications 5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
    },
    {
        "skill": "1+ Year",
        "reference": "Qualifications 5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
    },
    {
        "skill": "ETL/ELT development",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Core Programming Languages",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Data integration concepts",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Data Engineering",
        "reference": "Title: Data Engineer Terms"
    },
    {
        "skill": "Oracle Graph Database",
        "reference": "1+ yr of experience working with Oracle Graph Database"
    },
    {
        "skill": "ETL",
        "reference": "Must have ETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD."
    },
    {
        "skill": "SQL",
        "reference": "Must have ETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD."
    },
    {
        "skill": "Postgres",
        "reference": "Must have ETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD."
    },
    {
        "skill": "Cloud (Snowflake, AWS)",
        "reference": "Must have ETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD."
    },
    {
        "skill": "Automation (Python or Java)",
        "reference": "Must have ETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD."
    },
    {
        "skill": "CI/CD",
        "reference": "Must have ETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD."
    },
    {
        "skill": "ETL",
        "reference": "Good to have API."
    },
    {
        "skill": "API",
        "reference": "Good to have API."
    },
    {
        "skill": "10+ years of experience, hands-on development, end-to-end solutions",
        "reference": "10+ years of experience of hands-on development experience; create and design and architect solutions at the same time. Providing end-to-end solutions."
    },
    {
        "skill": "Teaching best practices, Python and PySpark, Apache Spark, Salesforce understanding, AWS services",
        "reference": "Ability to teach or mentor best practices. Building data pipelines using Python and PySpark. Strong programming skills in Python and PySpark. Experience with Apache Spark required Salesforce understanding and knowledge is preferred Proficiency in AWS services, specifically S3, Glue, Athena, and Lambda."
    },
    {
        "skill": "Hadoop ecosystem, HDFS, MapReduce, Hive, real-time data management",
        "reference": "Experience with Hadoop ecosystem, including HDFS, MapReduce, and Hive. Hands-on experience with Apache Hudi for real-time data management."
    },
    {
        "skill": "SQL and NoSQL databases, data governance, security best practices, workflow scheduling tools",
        "reference": "Familiarity with SQL and NoSQL databases. Knowledge of data governance and data security best practices. Experience with workflow scheduling tools like AWS Step Functions or Control M."
    },
    {
        "skill": "Big Data Engineering",
        "reference": "Designing and implementing large-scale, distributed data processing systems"
    },
    {
        "skill": "Data Infrastructure Development",
        "reference": "Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms."
    },
    {
        "skill": "Data Security & Privacy",
        "reference": "Implementing data security and privacy measures to protect sensitive information throughout the data lifecycle"
    },
    {
        "skill": "Data Engineering",
        "reference": "The Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data."
    },
    {
        "skill": "ETL/ELT Development",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Programming Languages",
        "reference": "Developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Java",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Scala",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Hadoop",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Spark",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Data mining",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Stream processing",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Kafka",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Spark Streaming",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Akka Streams",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Data quality",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Best practices",
        "reference": "Running big data analytics, building large scale data infrastructure and detecting meaningful data patterns."
    },
    {
        "skill": "Java",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Scala",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Hadoop",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Spark",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Kafka",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Spark Streaming",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Akka Streams",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Data mining",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Stream processing",
        "reference": "Responsibilities Running big data analytics, and building large scale data infrastructure."
    },
    {
        "skill": "Communication Skills",
        "reference": "Strong communication skills, knowledge of Unix-based operating systems, experience with JVM build systems (SBT, Maven, Gradle), AWS experience, and ability to quickly learn new tools and technologies."
    },
    {
        "skill": "Unix-based operating systems",
        "reference": "Strong communication skills, knowledge of Unix-based operating systems, experience with JVM build systems (SBT, Maven, Gradle), AWS experience, and ability to quickly learn new tools and technologies."
    },
    {
        "skill": "JVM build systems",
        "reference": "Strong communication skills, knowledge of Unix-based operating systems, experience with JVM build systems (SBT, Maven, Gradle), AWS experience, and ability to quickly learn new tools and technologies."
    },
    {
        "skill": "AWS experience",
        "reference": "Strong communication skills, knowledge of Unix-based operating systems, experience with JVM build systems (SBT, Maven, Gradle), AWS experience, and ability to quickly learn new tools and technologies."
    },
    {
        "skill": "Problem solving skills",
        "reference": "Strong communication skills, knowledge of Unix-based operating systems, experience with JVM build systems (SBT, Maven, Gradle), AWS experience, and ability to quickly learn new tools and technologies."
    },
    {
        "skill": "Big Data",
        "reference": "Working with bleeding-edge big data technologies to develop a high-performance data analytics platform, which handles petabytes of data."
    },
    {
        "skill": "Data analytics",
        "reference": "Working with bleeding-edge big data technologies to develop a high-performance data analytics platform, which handles petabytes of data."
    },
    {
        "skill": "High performance data analytics platform",
        "reference": "Working with bleeding-edge big data technologies to develop a high-performance data analytics platform, which handles petabytes of data."
    },
    {
        "skill": "Azure Data Engineering, MDM Solutions (Profisee), EDA Experience",
        "reference": "The ideal candidate will have a track record of building data sources leveraging MDM solutions such as Profisee and has worked with Event Driven Architecture."
    },
    {
        "skill": "Data Solution Design & Development in Azure Cloud Environment, Process Automation",
        "reference": "Designs and develops processes based on the requirements of the company. This includes collecting data, analyzing data, designing algorithms, drawing flowcharts, and implementing code."
    },
    {
        "skill": "Collaboration & Project Management",
        "reference": "Collaborates effectively with the development groups to deliver projects to the satisfaction of the client in a timely fashion."
    },
    {
        "skill": "Data Pipeline Design",
        "reference": "Designing, building, and maintaining data pipelines, databases, and cloud platforms"
    },
    {
        "skill": "Database Administration",
        "reference": "Managing on-premises, cloud, and hybrid research databases and database platforms"
    },
    {
        "skill": "ETL Techniques",
        "reference": "Integrating and transforming health-related data into analyzable formats for research"
    },
    {
        "skill": "Data Engineering",
        "reference": "Design, development and maintenance of data processes and pipelines supporting critical Strategic Business Unit (SBU) Data initiatives"
    },
    {
        "skill": "Cloud Computing",
        "reference": "Work with AWS/Azure Cloud platforms and related services and Terraform, Gitlab and similar CI/CD tools. Experience with Databricks Platform."
    },
    {
        "skill": "Software Development",
        "reference": "Experience building data pipelines leveraging tools like Spark, Python, PySpark & SQL"
    },
    {
        "skill": "Data Analysis",
        "reference": "Demonstrated engineering experience in system integration and design, data pipeline development, or software/service development and deployment."
    },
    {
        "skill": "Professional Development",
        "reference": "Re-Empower Program. Experience with Micro Services Architectures. AWS/Azure certifications."
    },
    {
        "skill": "Data Engineer",
        "reference": "Data Engineer with data pipeline expertise for a full-time, permanent position."
    },
    {
        "skill": "Active TS/SCI Clearance",
        "reference": "Candidates must hold an active TS/SCI clearance."
    },
    {
        "skill": "Database administration and development experience",
        "reference": "Database administration and development experience will be a plus for consideration."
    },
    {
        "skill": "Data Engineering",
        "reference": "4+ years of proven experience as a Data Engineer, with a focus on building robust data pipelines and maintaining data infrastructure. Strong programming skills in languages like Python, Scala or Javascript."
    },
    {
        "skill": "Data Analysis",
        "reference": "Expert level writing of SQL for data manipulation, transformation and for analytics."
    },
    {
        "skill": "Dashboard Development",
        "reference": "Build and maintain interactive and intuitive dashboards using tools like Looker."
    },
    {
        "skill": "ETL/ELT development, implementation and support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data integration concepts knowledge",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core programming languages proficiency - SQL, Python",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Data Engineering",
        "reference": "Ingestion, transformation and integration of data to provide a platform"
    },
    {
        "skill": "ETL/ELT Development",
        "reference": "Development, implementation, support for 3+ years"
    },
    {
        "skill": "Core Programming Languages",
        "reference": "Experience with SQL, Python"
    },
    {
        "skill": "Data Platform Engineer II",
        "reference": "The Data Platform Engineer II responsible for building out and optimizing Aware SaaS production infrastructure, scale and monitor cloud services, and develop solutions to extend the platform with automation."
    },
    {
        "skill": "Cloud Experience",
        "reference": "3+ years of experience in hands-on coding and deploying solutions on cloud platforms (E.g., Azure, AWS, GCP), 3+ years of experience with code deployment and production configuration management."
    },
    {
        "skill": "DevOps & Automation",
        "reference": "50% time in coding project works - building tools, solutions, and reusable script templates for the cloud service(s). Helping developers with DevOps and automation requests."
    },
    {
        "skill": "ETL/ELT development",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data integration concepts",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core programming languages",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Azure cloud services",
        "reference": "Understanding or knowledge of Azure cloud services \u2013 Azure Data Catalog, Azure Data Lake Store, Azure BLOB Store"
    },
    {
        "skill": "AWS cloud services",
        "reference": "Understanding or knowledge of AWS cloud services \u2013 S3, EC2, Data Pipelines"
    },
    {
        "skill": "Relational databases",
        "reference": "3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)"
    },
    {
        "skill": "Linux/Unix commands",
        "reference": "Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management"
    },
    {
        "skill": "Data Engineering",
        "reference": "About the job"
    },
    {
        "skill": "ETL/ELT Development",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Core Programming Languages",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Cloud Services Knowledge",
        "reference": "Understanding or knowledge of Azure cloud services \u2013 Azure Data Catalog, Azure Data Lake Store, Azure BLOB Store"
    },
    {
        "skill": "SQL Experience",
        "reference": "3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)"
    },
    {
        "skill": "Unix/Linux Commands",
        "reference": "Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management"
    },
    {
        "skill": "ETL/ELT",
        "reference": "Data Engineer role spans a broad range of experience."
    },
    {
        "skill": "Data Engineering",
        "reference": "Data Engineer role spans a broad range of experience."
    },
    {
        "skill": "Core Programming Languages",
        "reference": "Development, implementation, and support in ETL/ELT and data integration."
    },
    {
        "skill": "Data Integration",
        "reference": "Development, implementation, and support in ETL/ELT and data integration."
    },
    {
        "skill": "Azure cloud services",
        "reference": "Understanding or knowledge of Azure and AWS cloud services."
    },
    {
        "skill": "AWS cloud services",
        "reference": "Understanding or knowledge of Azure and AWS cloud services."
    },
    {
        "skill": "SQL",
        "reference": "Demonstrated success in developing with Core Programming Languages, SQL, Python."
    },
    {
        "skill": "Python",
        "reference": "Demonstrated success in developing with Core Programming Languages, SQL, Python."
    },
    {
        "skill": "Relational databases",
        "reference": "Demonstrated success in developing with Core Programming Languages, SQL, Python."
    },
    {
        "skill": "Linux/Unix commands",
        "reference": "Demonstrated success in developing with Core Programming Languages, SQL, Python."
    },
    {
        "skill": "Data Pipelines",
        "reference": "Ensure quality of code and data assets in the process of building ETL/ELT pipelines for various initiatives."
    },
    {
        "skill": "Critical projects",
        "reference": "Work on complex tasks in critical programs with a focus on determining technical approach."
    },
    {
        "skill": "Technical approach",
        "reference": "Work on complex tasks in critical programs with a focus on determining technical approach."
    },
    {
        "skill": "Data Engineering",
        "reference": "have an active role designing and implementing the Intelligent Data Platform's information architecture and data engineering"
    },
    {
        "skill": "Cloud Migration",
        "reference": "Mercys move towards becoming a more data-driven learning and decisioning organization, and ventures toward a large-scale cloud migration and digital transformation"
    },
    {
        "skill": "Data Management",
        "reference": "effectively manage data in compliance with data governance and security standards, improve data quality, enhance decisioning, foster innovation and continuously improve the efficiency of business and health service offerings"
    },
    {
        "skill": "Data Engineering",
        "reference": "4+ years of proven experience as a Data Engineer"
    },
    {
        "skill": "SQL and non-SQL DBs",
        "reference": "Expert level writing of SQL for data manipulation, transformation and analytics"
    },
    {
        "skill": "Cloud-based data platforms",
        "reference": "Hands-on Experience building or maintaining data pipelines with a modern orchestration tools like Airflow/Prefect/Dagster. Strong expertise with SQL and non-SQL DBs, cloud-based data platforms (e.g., AWS, GCP) and their related services (S3, BigQuery, etc.)"
    },
    {
        "skill": "Data analysis and reporting",
        "reference": "Experience building and maintaining interactive and intuitive dashboards using tools like Looker."
    },
    {
        "skill": "ETL/ELT Development, Implementation, and Support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data Integration Concepts",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core Programming Languages - SQL, Python",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Data Engineering, Data Architecture, Quality Engineering",
        "reference": "The Senior Data Engineer will focus on quality engineering best practices to meet and exceed internal and external client expectations."
    },
    {
        "skill": "Cloud Platforms, Data Modeling, Performance Tuning",
        "reference": "You will analyze, design, develop, test and document solutions supporting data integration, performance tuning, and data modeling to drive organization growth objectives."
    },
    {
        "skill": "Agile Methodologies, Software Development Life Cycle (SDLC), Collaboration",
        "reference": "Work collaboratively on creative solutions with engineers, product managers, and analysts in an agile like environment. Perform, design, code reviews."
    },
    {
        "skill": "Data Engineer",
        "reference": "Job Details: Data Engineer with SQL/Python/ETL, Azure/AWS skills, contract-to-perm, remote/local for occasional meetings, preferably BS, and Spark skills as a plus"
    },
    {
        "skill": "SQL",
        "reference": "Job Details: Data Engineer with SQL/Python/ETL, Azure/AWS skills, contract-to-perm, remote/local for occasional meetings, preferably BS, and Spark skills as a plus"
    },
    {
        "skill": "Python",
        "reference": "Job Details: Data Engineer with SQL/Python/ETL, Azure/AWS skills, contract-to-perm, remote/local for occasional meetings, preferably BS, and Spark skills as a plus"
    },
    {
        "skill": "ETL",
        "reference": "Job Details: Data Engineer with SQL/Python/ETL, Azure/AWS skills, contract-to-perm, remote/local for occasional meetings, preferably BS, and Spark skills as a plus"
    },
    {
        "skill": "Azure/AWS",
        "reference": "Job Details: Data Engineer with SQL/Python/ETL, Azure/AWS skills, contract-to-perm, remote/local for occasional meetings, preferably BS, and Spark skills as a plus"
    },
    {
        "skill": "BS",
        "reference": "Job Details: Data Engineer with SQL/Python/ETL, Azure/AWS skills, contract-to-perm, remote/local for occasional meetings, preferably BS, and Spark skills as a plus"
    },
    {
        "skill": "Associate",
        "reference": "#associate#mid-senior"
    },
    {
        "skill": "Mid-senior",
        "reference": "#associate#mid-senior"
    },
    {
        "skill": "ETL/ELT development, implementation, support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data integration concepts",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core programming languages: SQL, Python",
        "reference": "3+ years of demonstrated success developing with Core Programming Languages - SQL, Python"
    },
    {
        "skill": "ETL/ELT development",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Core Programming Languages (SQL, Python)",
        "reference": "Demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "Data integration concepts",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Data Architecture, Data Modeling, MDM",
        "reference": "7+ years of experience working in data architecture, data modeling, master data management, metadata management"
    },
    {
        "skill": "Scaling & Optimizing Schemas, ETL Pipelines",
        "reference": "Recent accomplishments working with relational methods and approaches (logging, columnar, star and snowflake, dimensional modeling)"
    },
    {
        "skill": "Airflow, SQL, Python, Data Governance, SDLC",
        "reference": "Proficiency in coding through the use of with Python and SQL, Airflow or any workflow management platform for data engineering pipelines orchestration. Familiar with data governance frameworks, SDLC, and Agile methodology"
    },
    {
        "skill": "AWS, Spark SQL, Apache Pinot, NoSQL Data Stores, Real-Time Processing",
        "reference": "Bonus Points: Hands-on experience with Spark SQL, AWS (S3, EMR), Apache Pinot, Big Data & Hadoop. Proficient in coding through the use of Scala, Experience working with cloud technologies such as AWS, GCP or Azure. Understanding of NoSQL Data Stores, Experience around Batch & Real-time processing"
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Python, PySpark, Airflow, AWS, ETL",
        "reference": "Basic Qualifications (Required Skills & Experience) 6-8 years of experience Python Pyspark Spark AWS Airflow EMR ETL work SQL"
    },
    {
        "skill": "Remote Work with Benefits",
        "reference": "Other Qualifications & Desired Competencies Fully Remote!!! Equity and Bonuses involved You Will Receive The Following Benefits Medical Insurance Dental Benefits Vision Benefits 401(k)"
    },
    {
        "skill": "Collaboration",
        "reference": "About the job"
    },
    {
        "skill": "Data Integration",
        "reference": "About the job"
    },
    {
        "skill": "Programming",
        "reference": "About the job"
    },
    {
        "skill": "Agile Development",
        "reference": "Your Impact"
    },
    {
        "skill": "Data Management",
        "reference": "Your Impact"
    },
    {
        "skill": "Project Management",
        "reference": "Your Impact"
    },
    {
        "skill": "Database Technologies",
        "reference": "About You"
    },
    {
        "skill": "API Tools",
        "reference": "About You"
    },
    {
        "skill": "REST APIs",
        "reference": "About You"
    },
    {
        "skill": "ETL/ELT development, implementation and support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Data integration concepts knowledge",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Core programming languages experience",
        "reference": "Demonstrated success developing with Core Programming Languages \u2013 SQL, Python"
    },
    {
        "skill": "SQL, Azure Data Factory, DBT, ETL work",
        "reference": "Top Skills"
    },
    {
        "skill": "Timely projects, Progress measurement, Software engineering fundamentals",
        "reference": "Top Skills"
    },
    {
        "skill": ".NET or Java, SQL constructs, Frontend client applications (Angular), Git, Cloud-based systems, Big data design, Data normalization patterns, Queuing technologies, Metrics and alerting tools",
        "reference": "Job Description"
    },
    {
        "skill": "Communication, RESTful APIs, HL7 V2.x / FHIR, System deployment tasks, CI/CD Pipeline, Kubernetes, Terraform",
        "reference": "Project/Day To Day Communicate"
    },
    {
        "skill": "Data analysis and manipulation (SQL), Data source consumption (flat files, streaming systems, RESTful APIs)",
        "reference": "Project/Day To Day Communicate"
    },
    {
        "skill": "Interact with Electronic Health Records",
        "reference": "Interface with Electronic Health Records"
    },
    {
        "skill": "Scalable, reliable and performant systems engineering, Collaboration across teams",
        "reference": "Engineer scalable, reliable, and performant systems to manage data / Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization"
    },
    {
        "skill": "Build quality systems while expanding offerings",
        "reference": "Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization / Build quality systems while expanding offerings to dependent teams"
    },
    {
        "skill": "Comfortable in multiple roles (Design, Development, Deployment, Monitoring)",
        "reference": "Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
    },
    {
        "skill": "Big Data, Python, Spark/Pyspark, Hadoop",
        "reference": "At least six (6) years of experience in working with Big data tools and technology. Experience with Hadoop, Python, Spark/Pyspark, Hive and Tableau/Heavy."
    },
    {
        "skill": "Data Engineering, ETL Processing",
        "reference": "As a Big Data engineer, you will be expected to work closely with our Business and ETL team to implement all Data processing procedures for all new projects and maintain effective awareness of all production activities according to required standards and provide support to all existing applications."
    },
    {
        "skill": "Data Architecture, Design",
        "reference": "Work with the Data Architect(s) to help drive architecture and design approaches that result in implemented business solutions."
    },
    {
        "skill": "Graph Database",
        "reference": "Must have Graph Database and Java Strong experience in large scale/high volume data pipeline infrastructure"
    },
    {
        "skill": "Java",
        "reference": "Must have Graph Database and Java Strong experience in large scale/high volume data pipeline infrastructure"
    },
    {
        "skill": "Experience",
        "reference": "Must have Graph Database and Java Strong experience in large scale/high volume data pipeline infrastructure"
    },
    {
        "skill": "NoSQL databases",
        "reference": "Experience with NoSQL databases (HBase, Cassandra, MongoDB)Experience with data stream technologies (Flink, Kafka, Confluent)"
    },
    {
        "skill": "Data stream technologies",
        "reference": "Experience with NoSQL databases (HBase, Cassandra, MongoDB)Experience with data stream technologies (Flink, Kafka, Confluent)"
    },
    {
        "skill": "Linux operating systems support",
        "reference": "3+ years supporting open-source Linux operating systems (CentOS, SuSE, Red Hat)Ability to read, understand, and write java code Engineer Strong knowledge of scripting and automation tools"
    },
    {
        "skill": "Engineering skills",
        "reference": "3+ years supporting open-source Linux operating systems (CentOS, SuSE, Red Hat)Ability to read, understand, and write java code Engineer Strong knowledge of scripting and automation tools"
    },
    {
        "skill": "ETL/ELT development, implementation, and support",
        "reference": "3+ years of ETL/ELT development, implementation, and support"
    },
    {
        "skill": "Core programming languages\u2014SQL, Python",
        "reference": "Understanding or knowledge of data integration concepts"
    },
    {
        "skill": "Azure cloud services",
        "reference": "Understanding or knowledge of Azure cloud services \u2013 Azure Data Catalog, Azure Data Lake Store, Azure BLOB Store"
    },
    {
        "skill": "AWS cloud services",
        "reference": "Understanding or knowledge of AWS cloud services \u2013 S3, EC2, Data Pipelines"
    },
    {
        "skill": "SQL for relational databases",
        "reference": "Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management"
    },
    {
        "skill": "SQL",
        "reference": "Top Skills"
    },
    {
        "skill": "Azure Data Factory",
        "reference": "Top Skills"
    },
    {
        "skill": "DBT ETL work",
        "reference": "Top Skills"
    },
    {
        "skill": "Proven project completion",
        "reference": ""
    },
    {
        "skill": "Timely manner progress measurement",
        "reference": ""
    },
    {
        "skill": "Software engineering fundamentals",
        "reference": "Strong software engineering skills"
    },
    {
        "skill": ".NET or Java",
        "reference": "Strong software engineering skills"
    },
    {
        "skill": "SQL constructs knowledge",
        "reference": "Strong software engineering skills"
    },
    {
        "skill": "Frontend client apps",
        "reference": "Experience in frontend and Git"
    },
    {
        "skill": "Angular",
        "reference": "Experience in frontend and Git"
    },
    {
        "skill": "Revision control experience",
        "reference": "Experience in frontend and Git"
    },
    {
        "skill": "Cloud-based systems",
        "reference": ""
    },
    {
        "skill": "Big data design understanding",
        "reference": ""
    },
    {
        "skill": "Queuing technologies knowledge",
        "reference": "Demonstrated experience in Queueing technologies and metrics"
    },
    {
        "skill": "Metrics, Logging, Monitoring & Alerting tools experience",
        "reference": "Demonstrated experience in Queueing technologies and metrics"
    },
    {
        "skill": "Strong communication skills",
        "reference": ""
    },
    {
        "skill": "RESTful APIs",
        "reference": ""
    },
    {
        "skill": "HL7 V2.x / FHIR interface messages understanding",
        "reference": ""
    },
    {
        "skill": "System deployment",
        "reference": ""
    },
    {
        "skill": "CI/CD Pipeline, K8s, Terraform experience",
        "reference": ""
    },
    {
        "skill": "Senior Data Engineer",
        "reference": "As a Senior Data Engineer"
    },
    {
        "skill": "Integration development",
        "reference": "Join our product development team focused on point-of-sale (POS) integrations."
    },
    {
        "skill": "Data pipeline design",
        "reference": "Design data pipeline solutions, analyze customer data, and identify data patterns."
    },
    {
        "skill": "Data Integration",
        "reference": "Build data integration (ETL) pipelines using SQL, EMR, Python and Spark"
    },
    {
        "skill": "Technical Knowledge & Leadership",
        "reference": "Technical Knowledge and leadership Collaborate with Software Solution team members"
    },
    {
        "skill": "Data Warehouse Experience",
        "reference": "9+ years building data pipelines and implementing feeds for data warehouse"
    },
    {
        "skill": "Scala and Spark",
        "reference": "Deep knowledge of Scala and Spark; Experience with Databricks is preferred."
    },
    {
        "skill": "Databricks",
        "reference": "Deep knowledge of Scala and Spark; Experience with Databricks is preferred."
    },
    {
        "skill": "Modern orchestration frameworks",
        "reference": "Deep knowledge of modern orchestration frameworks such as Apache airflow; Experience working with SQL and NoSQL database systems."
    },
    {
        "skill": "SQL and NoSQL database systems",
        "reference": "Deep knowledge of modern orchestration frameworks such as Apache airflow; Experience working with SQL and NoSQL database systems."
    },
    {
        "skill": "Cloud environments",
        "reference": "Experience with cloud environments (Azure Preferred); Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management."
    },
    {
        "skill": "Healthcare industry experience",
        "reference": "Experience with cloud environments (Azure Preferred); Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management."
    },
    {
        "skill": "Azure Services",
        "reference": "Advanced skills in Azure Defender, OpenAI, Azure Machine Learning, Cognitive Services, and Purview"
    },
    {
        "skill": "Data Engineering",
        "reference": "Apply data engineering principles to develop reusable workflows including ingestion, quality, transformation, and optimization. Build scalable data pipelines using exact, transform, and load tools."
    },
    {
        "skill": "Cloud Architecture",
        "reference": "Utilization of IaaS/PaaS/SaaS services to build reference architectures and Agile SAFe methodology to allow some Data Analytics staff to shadow and/or co-create"
    },
    {
        "skill": "Data Engineering",
        "reference": "As a Senior Data Engineer, you'll play a crucial role in developing, deploying, and supporting data systems, pipelines, lakes, and lakehouses."
    },
    {
        "skill": "Data Platform Development",
        "reference": "You'll work on collaborating with stakeholders to make effective use of core data assets, loading both streaming and batched data using Spark and Pyspark libraries, engineering lakehouse models, building scalable data pipelines, and working within an IT managed AWS account."
    },
    {
        "skill": "Data Platform Maintenance",
        "reference": "You'll be responsible for documenting data pipelines, cloud infrastructure, and standard operating procedures, express data platform cloud infrastructure as code, automate various tasks like testing, scaling, and performance tuning of the data platform, monitoring operations, ensuring security and privacy, and managing continuous upgrades."
    },
    {
        "skill": "Data Engineering",
        "reference": "5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
    },
    {
        "skill": "Experience",
        "reference": "5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
    },
    {
        "skill": "Oracle Graph Database",
        "reference": "5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
    },
    {
        "skill": "Working Hours",
        "reference": "Location: Remote in USA/Must be able to work PST Hours"
    },
    {
        "skill": "Location",
        "reference": "Location: Remote in USA/Must be able to work PST Hours"
    },
    {
        "skill": "Remote",
        "reference": "Location: Remote in USA/Must be able to work PST Hours"
    },
    {
        "skill": "PST",
        "reference": "Location: Remote in USA/Must be able to work PST Hours"
    },
    {
        "skill": "Custom SQL Server Datawarehouse",
        "reference": "Hands-on experience in designing, coding, enhancing, testing and production support of custom SQL Server Datawarehouse to meet business process requirements"
    },
    {
        "skill": "Excel VBCode Apps",
        "reference": "Proficient with Excel and creating Apps in Excel using VBCode"
    },
    {
        "skill": "SQL Server SSMS Advanced TSQL, Stored Procedures, best practices RDBMS",
        "reference": "Confident in Microsoft SQL Server (using SSMS - Advanced TSQL, Stored Procedures, best practices for writing clean effective code and balancing Declarative customizations with Programmatic customizations"
    },
    {
        "skill": "Oracle PLSQL TOAD knowledge",
        "reference": "Knowledge/experience in Oracle (PLSQL using TOAD)"
    },
    {
        "skill": "Salesforce knowledge and experience",
        "reference": "Salesforce knowledge and experience"
    },
    {
        "skill": "SnapLogic knowledge and experience",
        "reference": "SnapLogic knowledge and experience"
    },
    {
        "skill": "Tableau knowledge and experience",
        "reference": "Tableau knowledge and experience"
    },
    {
        "skill": "Strong communication skills, business/development requirements clarification",
        "reference": "Must-Haves (Soft Skills) - Strong written and verbal communication skills and ability to clarify business and development requirements"
    },
    {
        "skill": "Project management methodologies and sound development practices",
        "reference": "Nice-To-Haves (Soft Skills) - Ability to handle numerous projects/priorities using proven project management methodologies and sound development practices"
    },
    {
        "skill": "Programming efforts mentoring",
        "reference": "Coordinates, guides and mentors programming efforts performed by in-house programmers or outside consultants to ensure that all programming is completed according to the project plan"
    },
    {
        "skill": "Extensive knowledge of information systems design principles",
        "reference": "Applicant must have 5 years of relevant experience with the following: Extensive knowledge of information systems design principles and new systems design techniques."
    },
    {
        "skill": "New systems design techniques",
        "reference": "Applicant must have 5 years of relevant experience with the following: Extensive knowledge of information systems design principles and new systems design techniques."
    },
    {
        "skill": "Data warehouse development",
        "reference": "A very strong understanding of current industry standards and best practices used in data and data warehouse development."
    },
    {
        "skill": "Industry standards & best practices",
        "reference": "A very strong understanding of current industry standards and best practices used in data and data warehouse development."
    },
    {
        "skill": "Business data applications",
        "reference": "Extensive knowledge of system and business data applications. Sizable experience is setting up other data warehouse environments."
    },
    {
        "skill": "Sizable experience setting up data warehouse environments",
        "reference": "Extensive knowledge of system and business data applications. Sizable experience is setting up other data warehouse environments."
    },
    {
        "skill": "Evaluate business needs & objectives",
        "reference": "Ability to evaluate the business needs and objectives. Extensive knowledge of policies, standards, procedures, and techniques used for data and application development"
    },
    {
        "skill": "Data analysis & development techniques",
        "reference": "Ability to evaluate the business needs and objectives. Extensive knowledge of policies, standards, procedures, and techniques used for data and application development"
    },
    {
        "skill": "Problem-solving & analytical analysis",
        "reference": "Ability to perform problem solving and analytical analysis on complex issues."
    },
    {
        "skill": "Complex issues",
        "reference": "Ability to perform problem solving and analytical analysis on complex issues."
    },
    {
        "skill": "Technical competency",
        "reference": "High technical competency level of all phases of data analysis and developing activities"
    },
    {
        "skill": "All phases of data analysis and developing activities",
        "reference": "High technical competency level of all phases of data analysis and developing activities"
    },
    {
        "skill": "Training & education",
        "reference": "Willingness to train/educate data team members, DBAs, infrastructure support and management in effective data warehouse designs/approaches."
    },
    {
        "skill": "Data warehouse designs/approaches",
        "reference": "Willingness to train/educate data team members, DBAs, infrastructure support and management in effective data warehouse designs/approaches."
    },
    {
        "skill": "Project planning",
        "reference": "Proficient in developing project plans including discovery, development, and implementation."
    },
    {
        "skill": "Discovery, development, implementation",
        "reference": "Proficient in developing project plans including discovery, development, and implementation."
    },
    {
        "skill": "ETL tools - Azure Synapse & Azure Data Factory",
        "reference": "Experience in using the ETL tools - Azure Synapse and Azure Data Factory"
    },
    {
        "skill": "Managing assignments",
        "reference": "Ability to manage multiple data development assignments and priorities."
    },
    {
        "skill": "Priorities & data development",
        "reference": "Ability to manage multiple data development assignments and priorities."
    },
    {
        "skill": "Communication skills",
        "reference": "Ability to communicate effectively, both orally and in writing. Working knowledge in the above skills must be within the last 6 months."
    },
    {
        "skill": "Both oral and written",
        "reference": "Ability to communicate effectively, both orally and in writing. Working knowledge in the above skills must be within the last 6 months."
    },
    {
        "skill": "Data Engineer",
        "reference": "About the job"
    },
    {
        "skill": "Cybersecurity Experience",
        "reference": "This Cybersecurity company is looking for a Data Engineer"
    },
    {
        "skill": "ETL pipelines, Python",
        "reference": "What You Will Be Doing ETL pipelines using Python Utilizing cutting edge technology"
    }
]